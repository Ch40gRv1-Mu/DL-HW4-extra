{"cells":[{"cell_type":"markdown","metadata":{"id":"Gj0wTk9rucAM"},"source":["# 10-714 Homework 4\n","\n","In this homework, you will leverage all of the components built in the last three homeworks to solve some modern problems with high performing network structures. We will start by adding a few new ops leveraging our new CPU/CUDA backends. Then, you will implement convolution, and a convolutional neural network to train a classifier on the CIFAR-10 image classification dataset. Then, you will implement recurrent and long-short term memory (LSTM) neural networks, and do word-level prediction language modeling on the Penn Treebank dataset.\n","\n","As always, we will start by copying this notebook and getting the starting code.\n","Reminder: __you must save a copy in drive__."]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"usqNjQTQucAO","executionInfo":{"status":"ok","timestamp":1763015252409,"user_tz":300,"elapsed":8318,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"d2220b98-9a57-4693-aa79-131d67166dfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/MyDrive\n","/content/drive/MyDrive/dl-test/hw4\n","Collecting git+https://github.com/dlsys10714/mugrade.git\n","  Cloning https://github.com/dlsys10714/mugrade.git to /tmp/pip-req-build-u2r284_t\n","  Running command git clone --filter=blob:none --quiet https://github.com/dlsys10714/mugrade.git /tmp/pip-req-build-u2r284_t\n","  Resolved https://github.com/dlsys10714/mugrade.git to commit ac73f725eb2ce0e2c6a38fa540035ee970b8b873\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: pybind11 in /usr/local/lib/python3.12/dist-packages (3.0.1)\n"]}],"source":["from google.colab import drive\n","\n","drive.mount('/content/drive')\n","\n","%cd /content/drive/MyDrive/\n","%cd /content/drive/MyDrive/dl-test/hw4\n","\n","!pip3 install --upgrade --no-deps git+https://github.com/dlsys10714/mugrade.git\n","!pip3 install pybind11"]},{"cell_type":"code","source":["!python -c \"import torch; print(torch.version.cuda)\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iURia9MZ2Jo3","executionInfo":{"status":"ok","timestamp":1763015256831,"user_tz":300,"elapsed":4421,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"97107802-950a-45e0-824e-7db6626add81"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["12.6\n"]}]},{"cell_type":"code","execution_count":16,"metadata":{"id":"raU5egHBucAO","executionInfo":{"status":"ok","timestamp":1763015256846,"user_tz":300,"elapsed":6,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["# REQUIRED FOR MUGRADE\n","MY_API_KEY = \"<FILL YOUR API KEY HERE>\"\n","HW4_NAME = \"hw4\""]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"evVgfcOMucAO","outputId":"8b4e01c4-3e51-4265-e5fc-e3c6ecda1dd2","executionInfo":{"status":"ok","timestamp":1763015282487,"user_tz":300,"elapsed":25640,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/dl-test/hw4\n","\u001b[0mCMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n","  Compatibility with CMake < 3.10 will be removed from a future version of\n","  CMake.\n","\n","  Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n","  to tell CMake that the project requires at least <min> but has been updated\n","  to work with policies introduced by <max> or earlier.\n","\n","\u001b[0m\n","-- Found pybind11: /usr/local/lib/python3.12/dist-packages/pybind11/include (found version \"3.0.1\")\n","-- Found cuda, building cuda backend\n","-- Configuring done (0.5s)\n","-- Generating done (0.6s)\n","-- Build files have been written to: /content/drive/MyDrive/dl-test/hw4/build\n","make[1]: Entering directory '/content/drive/MyDrive/dl-test/hw4/build'\n","make[2]: Entering directory '/content/drive/MyDrive/dl-test/hw4/build'\n","make[3]: Entering directory '/content/drive/MyDrive/dl-test/hw4/build'\n","make[3]: Leaving directory '/content/drive/MyDrive/dl-test/hw4/build'\n","[  0%] Built target ndarray_backend_cpu\n","make[3]: Entering directory '/content/drive/MyDrive/dl-test/hw4/build'\n","[ 25%] \u001b[34m\u001b[1mBuilding NVCC (Device) object CMakeFiles/ndarray_backend_cuda.dir/src/ndarray_backend_cuda_generated_ndarray_backend_cuda.cu.o\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/dl-test/hw4/build'\n","make[3]: Entering directory '/content/drive/MyDrive/dl-test/hw4/build'\n","[ 50%] \u001b[32m\u001b[1mLinking CXX shared module /content/drive/MyDrive/dl-test/hw4/python/needle/backend_ndarray/ndarray_backend_cuda.cpython-312-x86_64-linux-gnu.so\u001b[0m\n","make[3]: Leaving directory '/content/drive/MyDrive/dl-test/hw4/build'\n","[ 50%] Built target ndarray_backend_cuda\n","make[2]: Leaving directory '/content/drive/MyDrive/dl-test/hw4/build'\n","make[1]: Leaving directory '/content/drive/MyDrive/dl-test/hw4/build'\n"]}],"source":["# REQUIRED FOR MUGRADE\n","MY_API_KEY = \"npnvWYJh0QPS6kFO40SK\"\n","HW4_NAME = \"hw4\"\n","# %!rm -rf build\n","\n","# # 3. Recreate build and re-run cmake from scratch\n","# %!mkdir build\n","%cd /content/drive/MyDrive/dl-test/hw4\n","!make"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z_cP7subucAO","executionInfo":{"status":"ok","timestamp":1763015282496,"user_tz":300,"elapsed":8,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"3ac2e75e-e9fa-43ab-b7e8-45cb6726ef6c"},"outputs":[{"output_type":"stream","name":"stdout","text":["env: PYTHONPATH=./python\n","env: NEEDLE_BACKEND=nd\n"]}],"source":["%set_env PYTHONPATH ./python\n","%set_env NEEDLE_BACKEND nd"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"7ZB5yDfPucAO","executionInfo":{"status":"ok","timestamp":1763015282505,"user_tz":300,"elapsed":8,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["import sys\n","sys.path.append('./python')"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"miKVHth8ucAP","executionInfo":{"status":"ok","timestamp":1763015282815,"user_tz":300,"elapsed":310,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["# Download the datasets you will be using for this assignment\n","\n","import urllib.request\n","import os\n","\n","!mkdir -p './data/ptb'\n","# Download Penn Treebank dataset\n","ptb_data = \"https://raw.githubusercontent.com/wojzaremba/lstm/master/data/ptb.\"\n","for f in ['train.txt', 'test.txt', 'valid.txt']:\n","    if not os.path.exists(os.path.join('./data/ptb', f)):\n","        urllib.request.urlretrieve(ptb_data + f, os.path.join('./data/ptb', f))\n","\n","# Download CIFAR-10 dataset\n","if not os.path.isdir(\"./data/cifar-10-batches-py\"):\n","    urllib.request.urlretrieve(\"https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\", \"./data/cifar-10-python.tar.gz\")\n","    !tar -xvzf './data/cifar-10-python.tar.gz' -C './data'"]},{"cell_type":"markdown","metadata":{"id":"DBLQSwoLucAP"},"source":["To finish setting up the assignment, go ahead and fill in all the code in `python/needle/autograd.py` using your solution code from the previous homework. Also copy the solutions in `src/ndarray_backend_cpu.cc` and `src/ndarray_backend_cuda.cu` from homework 3.\n","\n","**Note**: Be careful not to accidentally delete or modify any new imports and function declarations when copying over code from previous assignments."]},{"cell_type":"markdown","metadata":{"id":"TZjd8V5HucAP"},"source":["## Part 1: ND Backend [10 pts]\n","\n","Recall that in homework 2, the `array_api` was imported as `numpy`. In this part, the goal is to write the necessary operations with `array_api` imported from the needle backend `NDArray` in `python/needle/backend_ndarray/ndarray.py`. Make sure to copy the solutions for `reshape`, `permute`, `broadcast_to` and `__getitem__` from homework 3.\n","\n","Fill in the following classes in `python/needle/ops/ops_logarithmic.py` and `python/needle/ops/ops_mathematic.py`:\n","\n","- `PowerScalar`\n","- `EWiseDiv`\n","- `DivScalar`\n","- `Transpose`\n","- `Reshape`\n","- `BroadcastTo`\n","- `Summation`\n","- `MatMul`\n","- `Negate`\n","- `Log`\n","- `Exp`\n","- `ReLU`\n","- `LogSumExp`\n","- `Tanh` (new)\n","- `Stack` (new)\n","- `Split` (new)\n","\n","Note that for most of these, you already wrote the solutions in the previous homework and you should not change most part of your previous solution, if issues arise, please check if the `array_api` function used is supported in the needle backend.\n","\n","The `Tanh`, `Stack`, and `Split` operators are newly added. `Stack` concatenates same-sized tensors along a new axis, and `Split` undoes this operation. The gradients of the two operations can be written in terms of each other. We do not directly test `Split`, and only test the backward pass of `Stack` (for which we assume you used `Split`).\n","\n","**Note:** You may want to make your Summation op support sums over multiple axes; you will likely need it for the backward pass of the BroadcastTo op if yours supports broadcasting over multiple axes at a time. However, this is more about ease of use than necessity, and we leave this decision up to you (there are no corresponding tests).\n","\n","**Note:** Depending on your implementations, you may want to ensure that you call `.compact()` before reshaping arrays. (If this is necessary, you will run into corresponding error messages later in the assignment.)\n","\n","**Note**: Be careful not to accidentally delete or modify any new imports and function declarations when copying over code from previous assignments."]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c1JEWPAaucAP","executionInfo":{"status":"ok","timestamp":1763015302113,"user_tz":300,"elapsed":19293,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"891ec052-a1cb-4d11-ea3b-fa4801a818fe"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1685 deselected / 118 selected                          \u001b[0m\n","\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  0%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  1%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[32m    [  2%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[32m  [  3%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m   [  4%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m   [  5%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_ewise_fn[cuda-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [  6%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-divide] \u001b[32mPASSED\u001b[0m\u001b[31m   [  7%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape0-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m [  8%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-divide] \u001b[32mPASSED\u001b[0m\u001b[31m   [  9%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cpu-shape1-subtract] \u001b[32mPASSED\u001b[0m\u001b[31m [ 10%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-divide] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 11%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape0-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-divide] \u001b[31mFAILED\u001b[0m\u001b[31m  [ 12%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_scalar_fn[cuda-shape1-subtract] \u001b[31mFAILED\u001b[0m\u001b[31m [ 13%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-16] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 14%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-8-8-8] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 15%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-1-2-3] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 16%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-3-4-5] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 16%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-5-4-3] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 17%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-16-16-32] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 18%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-64-64-64] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 19%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-72-72-72] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 20%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-72-73-74] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 21%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-74-73-72] \u001b[32mPASSED\u001b[0m\u001b[31m           [ 22%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cpu-128-128-128] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 22%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-16] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 23%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-8-8-8] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 24%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-1-2-3] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 25%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-3-4-5] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 26%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-5-4-3] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 27%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-16-16-32] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 27%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-64-64-64] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 28%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-72-72-72] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 29%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-72-73-74] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 30%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-74-73-72] \u001b[31mFAILED\u001b[0m\u001b[31m          [ 31%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_matmul[cuda-128-128-128] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 32%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_power[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 33%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_power[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 33%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_power[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 34%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_power[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m             [ 35%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_log[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 36%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_log[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 37%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_log[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 38%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_log[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 38%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_exp[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 39%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_exp[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m                [ 40%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_exp[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 41%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_exp[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m               [ 42%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_relu[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 43%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_relu[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 44%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_relu[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m              [ 44%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_relu[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 45%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 46%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m               [ 47%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh[cuda-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 48%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m              [ 49%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh_backward[cpu-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m      [ 50%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 51%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_tanh_backward[cuda-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 52%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack[cpu-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 53%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack[cpu-shape1-0-2] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 54%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack[cpu-shape2-2-5] \u001b[32mPASSED\u001b[0m\u001b[31m          [ 55%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack[cuda-shape0-0-1] \u001b[32mPASSED\u001b[0m\u001b[31m         [ 55%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 56%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m         [ 57%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 59%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack_backward[cpu-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape0-0-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape1-0-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_stack_backward[cuda-shape2-2-5] \u001b[31mFAILED\u001b[0m\u001b[31m [ 62%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m     [ 63%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 64%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 65%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 66%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 66%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 67%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 68%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 69%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m [ 72%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 74%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 75%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_summation_backward[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_broadcast_to[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m [ 77%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_broadcast_to[cuda-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_reshape[cpu-shape0-shape_to0] \u001b[32mPASSED\u001b[0m\u001b[31m  [ 80%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_reshape[cpu-shape1-shape_to1] \u001b[32mPASSED\u001b[0m\u001b[31m  [ 81%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_reshape[cuda-shape0-shape_to0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_reshape[cuda-shape1-shape_to1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 83%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 83%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cpu-axes0-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 84%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 85%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cpu-axes1-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m    [ 86%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape0] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 87%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cpu-None-shape1] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 88%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 88%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cuda-axes0-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 89%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 90%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cuda-axes1-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m   [ 91%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape0] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 92%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_transpose[cuda-None-shape1] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 93%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape0-None] \u001b[32mPASSED\u001b[0m\u001b[31m     [ 94%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape1-0] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 94%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape2-1] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 95%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cpu-shape3-2] \u001b[32mPASSED\u001b[0m\u001b[31m        [ 96%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape0-None] \u001b[31mFAILED\u001b[0m\u001b[31m    [ 97%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape1-0] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 98%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape2-1] \u001b[31mFAILED\u001b[0m\u001b[31m       [ 99%]\u001b[0m\n","tests/hw4/test_nd_backend.py::test_logsumexp[cuda-shape3-2] \u001b[31mFAILED\u001b[0m\u001b[31m       [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape0-divide] _______________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137d7232aca0>, shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","B          = needle.Tensor([[[0.]]])\n","_A         = array([[[-0.34836832]]], dtype=float32)\n","_B         = array([[[-1.8110192]]], dtype=float32)\n","device     = cuda()\n","fn         = <function <lambda> at 0x137d7232aca0>\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196a020>, array([[[0.19236037]]], dtype=float32), array([[[-1.8110192]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 2.0033796\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 1.1062167\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[0.19236]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-1.811019]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196a020>, array([[[0.19236037]]], dtype=float32), array([[[-1.8110192]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape0-subtract] ______________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137c91e4ea20>, shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","B          = needle.Tensor([[[0.]]])\n","_A         = array([[[0.9578964]]], dtype=float32)\n","_B         = array([[[1.2263213]]], dtype=float32)\n","device     = cuda()\n","fn         = <function <lambda> at 0x137c91e4ea20>\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196a5c0>, array([[[-0.26842493]]], dtype=float32), array([[[0.]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.26842493\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.268425]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196a5c0>, array([[[-0.26842493]]], dtype=float32), array([[[0.]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_ewise_fn[cuda-shape1-divide] _______________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137d7232aca0>, shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","B          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[-1.4833167 ,  0.13791604,  1.1932526 , -1.0757235 ,\n","          1.7676828 , -0.34903246],\n","        [-1.0753901 ,...838],\n","        [ 0.11024354,  0.57954335, -1.7123466 ,  0.42849943,\n","          0.9961082 ,  0.33900097]]], dtype=float32)\n","_B         = array([[[ 0.14525874, -0.9536581 ,  0.6435548 ,  0.20046894,\n","          0.00906379, -0.56320494],\n","        [ 0.85617113,...21 ],\n","        [-1.350194  ,  1.6009326 ,  0.2970727 , -0.2077485 ,\n","         -1.8044882 ,  1.087597  ]]], dtype=float32)\n","device     = cuda()\n","fn         = <function <lambda> at 0x137d7232aca0>\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196aca0>, array([[[-1.02115488e+01, -1.44617915e-01,  1.85415840...1 ],\n","        [-1.350194  ,  1.6009326 ,  0.2970727 , -0.2077485 ,\n","         -1.8044882 ,  1.087597  ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 195.0179\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 21516.162\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.021155e+01, -1.446179e-01,  1.854158e+00, -5.366036e+00,\u001b[0m\n","\u001b[1m\u001b[31mE                     1.950270e+02,  6.197255e-01],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-1.256046e+00,  1.099804e+00,  7.588176e-01, -7.283399e-01,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[ 0.145259, -0.953658,  0.643555,  0.200469,  0.009064,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.563205],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.856171,  1.247184,  0.388146,  0.896749,  1.326023,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196aca0>, array([[[-1.02115488e+01, -1.44617915e-01,  1.85415840...1 ],\n","        [-1.350194  ,  1.6009326 ,  0.2970727 , -0.2077485 ,\n","         -1.8044882 ,  1.087597  ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_ewise_fn[cuda-shape1-subtract] ______________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137c91e4ea20>, shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, EWISE_OP_FNS, ids=EWISE_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_ewise_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","B          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[-0.96954954, -0.5111908 ,  0.5459043 , -1.0255798 ,\n","         -0.9868334 , -1.1512803 ],\n","        [-0.11136018,...78 ],\n","        [ 0.07969699,  0.35053274, -0.38154238, -0.9728293 ,\n","          0.06547206,  0.1765085 ]]], dtype=float32)\n","_B         = array([[[-0.6627614 ,  1.8296949 , -1.6159147 , -0.8153447 ,\n","         -0.6713712 ,  1.2912054 ],\n","        [-0.2565905 ,...04 ],\n","        [ 0.6192242 , -0.14435019, -0.21653645,  0.7103066 ,\n","         -0.47707722, -1.3997524 ]]], dtype=float32)\n","device     = cuda()\n","fn         = <function <lambda> at 0x137c91e4ea20>\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:56: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196a0c0>, array([[[-3.0678815e-01, -2.3408856e+00,  2.1618190e+0...        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.4669023\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-3.067881e-01, -2.340886e+00,  2.161819e+00, -2.102351e-01,\u001b[0m\n","\u001b[1m\u001b[31mE                    -3.154622e-01, -2.442486e+00],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 1.452303e-01,  9.134747e-01,  1.288497e+00,  1.171898e+00,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0., 0., 0., 0., 0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0., 0., 0., 0., 0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0., 0., 0., 0., 0., 0.],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196a0c0>, array([[[-3.0678815e-01, -2.3408856e+00,  2.1618190e+0...        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape0-divide] ______________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137c91e4eb60>, shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[-0.21264285]]], dtype=float32)\n","_B         = -0.6414637565612793\n","device     = cuda()\n","fn         = <function <lambda> at 0x137c91e4eb60>\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196b1a0>, array([[[0.33149627]]], dtype=float32), array([[[0.]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.33149627\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[0.331496]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196b1a0>, array([[[0.33149627]]], dtype=float32), array([[[0.]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_scalar_fn[cuda-shape0-subtract] _____________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137c91e4ec00>, shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[0.7919234]]], dtype=float32)\n","_B         = -0.7252244353294373\n","device     = cuda()\n","fn         = <function <lambda> at 0x137c91e4ec00>\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196b600>, array([[[1.5171478]]], dtype=float32), array([[[0.7919234]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.7252244\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 0.9157759\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[1.517148]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.791923]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196b600>, array([[[1.5171478]]], dtype=float32), array([[[0.7919234]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_scalar_fn[cuda-shape1-divide] ______________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137c91e4eb60>, shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[ 0.17852722, -0.48561108,  0.4178631 ,  0.6601184 ,\n","         -0.26862556,  0.04397374],\n","        [ 0.11434969,...873],\n","        [ 0.95702404,  0.3420432 , -1.1888571 , -1.3259709 ,\n","         -1.4491868 ,  0.3708898 ]]], dtype=float32)\n","_B         = -0.7234068512916565\n","device     = cuda()\n","fn         = <function <lambda> at 0x137c91e4eb60>\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196b6a0>, array([[[-0.24678674,  0.6712835 , -0.5776322 , -0.912...73],\n","        [ 0.95702404,  0.3420432 , -1.1888571 , -1.3259709 ,\n","         -1.4491868 ,  0.3708898 ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 7.26929\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 2.382348\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.246787,  0.671283, -0.577632, -0.912513,  0.371334,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.060787],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.158071, -0.984129,  0.540839,  0.034679,  0.681248,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[ 0.178527, -0.485611,  0.417863,  0.660118, -0.268626,\u001b[0m\n","\u001b[1m\u001b[31mE                     0.043974],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.11435 ,  0.711925, -0.391246, -0.025087, -0.492819,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196b6a0>, array([[[-0.24678674,  0.6712835 , -0.5776322 , -0.912...73],\n","        [ 0.95702404,  0.3420432 , -1.1888571 , -1.3259709 ,\n","         -1.4491868 ,  0.3708898 ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_scalar_fn[cuda-shape1-subtract] _____________________\u001b[0m\n","\n","fn = <function <lambda> at 0x137c91e4ec00>, shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mfn\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SCALAR_OP_FNS, ids=SCALAR_OP_NAMES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_scalar_fn\u001b[39;49;00m(fn, shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(\u001b[94m1\u001b[39;49;00m).astype(np.float32).item()\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(fn(_A, _B), fn(A, _B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[-0.37743163, -0.0939057 ,  1.2136433 ,  2.1777341 ,\n","         -0.12863235,  1.989526  ],\n","        [ 0.5402597 ,...386],\n","        [-1.2373476 ,  0.6722726 , -1.3963248 ,  2.26083   ,\n","         -0.72201365, -0.01691202]]], dtype=float32)\n","_B         = 1.3843916654586792\n","device     = cuda()\n","fn         = <function <lambda> at 0x137c91e4ec00>\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:72: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d104a0>, array([[[-1.7618233 , -1.4782974 , -0.17074835,  0.793...86],\n","        [-1.2373476 ,  0.6722726 , -1.3963248 ,  2.26083   ,\n","         -0.72201365, -0.01691202]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 1.3843918\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 290.43176\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.761823, -1.478297, -0.170748,  0.793342, -1.513024,\u001b[0m\n","\u001b[1m\u001b[31mE                     0.605134],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.844132, -0.182809, -1.673583, -2.639068, -0.90489 ,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.377432, -0.093906,  1.213643,  2.177734, -0.128632,\u001b[0m\n","\u001b[1m\u001b[31mE                     1.989526],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.54026 ,  1.201583, -0.289191, -1.254677,  0.479501,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d104a0>, array([[[-1.7618233 , -1.4782974 , -0.17074835,  0.793...86],\n","        [-1.2373476 ,  0.6722726 , -1.3963248 ,  2.26083   ,\n","         -0.72201365, -0.01691202]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m__________________________ test_matmul[cuda-16-16-16] __________________________\u001b[0m\n","\n","m = 16, n = 16, p = 16, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [...0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","B          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [...0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","_A         = array([[-4.8341087e-01,  1.8093601e+00,  1.6722593e+00,  8.0163997e-01,\n","         1.5130348e+00,  5.7170558e-01, -6.382...793e+00, -1.2557693e-01,\n","         1.1033653e+00,  9.9985802e-01,  1.3731679e+00, -6.6210198e-01]],\n","      dtype=float32)\n","_B         = array([[-0.6730201 , -1.2317069 ,  0.6105233 , -1.0086207 ,  0.971255  ,\n","        -0.98045826, -0.87343013, -0.50210905...690245 ,\n","         1.0893643 , -1.0063945 , -0.397469  , -1.5228411 ,  0.27464268,\n","         0.8954157 ]], dtype=float32)\n","device     = cuda()\n","m          = 16\n","n          = 16\n","p          = 16\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d11260>, array([[-6.59989119e-02, -9.52318573e+00,  3.32090902e...90245 ,\n","         1.0893643 , -1.0063945 , -0.397469  , -1.5228411 ,  0.27464268,\n","         0.8954157 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 256 / 256 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 12.264388\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 1038.112\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[-6.599891e-02, -9.523186e+00,  3.320909e+00,  1.446190e+00,\u001b[0m\n","\u001b[1m\u001b[31mE                    4.255707e+00,  7.807609e-01, -1.431365e+00, -7.581457e+00,\u001b[0m\n","\u001b[1m\u001b[31mE                    6.824069e-01, -1.274879e+00,  1.872034e+00, -4.985985e+00,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[-0.67302 , -1.231707,  0.610523, -1.008621,  0.971255, -0.980458,\u001b[0m\n","\u001b[1m\u001b[31mE                   -0.87343 , -0.502109,  0.238857, -0.544474,  0.264026, -0.031052,\u001b[0m\n","\u001b[1m\u001b[31mE                    0.188314,  0.602501, -1.628968, -0.519979],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d11260>, array([[-6.59989119e-02, -9.52318573e+00,  3.32090902e...90245 ,\n","         1.0893643 , -1.0063945 , -0.397469  , -1.5228411 ,  0.27464268,\n","         0.8954157 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________________ test_matmul[cuda-8-8-8] ____________________________\u001b[0m\n","\n","m = 8, n = 8, p = 8, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0.]])\n","B          = needle.Tensor([[-0.6730201  -1.2317069   0.6105233  -1.0086207   0.971255   -0.98045826\n","  -0.87343013 -0.50210905]\n"," [ ...5987 -1.5124943 ]\n"," [-0.07404925 -0.72915363  0.3502874  -1.3699573   0.04136498  1.7261171\n","   0.3988833  -0.19048904]])\n","_A         = array([[-1.4774635 , -1.6267474 ,  0.49602756,  0.31081805,  1.2617579 ,\n","         0.89151007, -0.14909866, -1.3068166 ...33,  1.380418  , -1.8872807 ,  0.7998044 , -1.1721367 ,\n","        -0.11657883,  1.6036352 , -1.1328816 ]], dtype=float32)\n","_B         = array([[ 1.0537726 , -0.19507441,  0.6123048 , -1.2122457 ,  0.46485847,\n","         0.58945036,  2.0647578 ,  1.6958039 ...4 ,  1.4154483 , -1.5804107 , -0.13925153,  0.88675374,\n","         1.7259799 , -0.5968434 ,  2.8232105 ]], dtype=float32)\n","device     = cuda()\n","m          = 8\n","n          = 8\n","p          = 8\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196a840>, array([[-4.703101  , -2.8492887 , -2.6986284 , -0.5457... , -0.5026022 ,  0.575385  ,  0.19826922,  0.3358586 ,\n","         0.7329904 ,  1.9778988 ,  0.7161346 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 64 / 64 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 6.7707376\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 147.70091\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[-4.703101, -2.849289, -2.698628, -0.545737, -1.740224, -3.609611,\u001b[0m\n","\u001b[1m\u001b[31mE                   -2.623294, -4.137527],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 2.726994,  2.989327, -1.855997,  1.854532, -1.147853, -2.167189,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 1.418315, -0.607672, -0.849557, -2.280574,  0.091745,  0.693373,\u001b[0m\n","\u001b[1m\u001b[31mE                   -0.233741,  0.359968],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.969087, -0.283946,  0.332936, -0.428272,  0.931943,  0.159707,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196a840>, array([[-4.703101  , -2.8492887 , -2.6986284 , -0.5457... , -0.5026022 ,  0.575385  ,  0.19826922,  0.3358586 ,\n","         0.7329904 ,  1.9778988 ,  0.7161346 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________________ test_matmul[cuda-1-2-3] ____________________________\u001b[0m\n","\n","m = 1, n = 2, p = 3, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[ 1.4183148 -0.6076717]])\n","B          = needle.Tensor([[-0.37743163 -0.0939057   1.2136433 ]\n"," [ 2.1777341  -0.12863235  1.989526  ]])\n","_A         = array([[-0.0460759, -0.299862 ]], dtype=float32)\n","_B         = array([[ 0.5341751 ,  1.3190286 ,  1.9854463 ],\n","       [-0.8265349 ,  0.05200673, -2.2706025 ]], dtype=float32)\n","device     = cuda()\n","m          = 1\n","n          = 2\n","p          = 3\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196b920>, array([[ 0.22323382, -0.07637027,  0.58938617]], dtype=float32), array([[0.5341751, 1.3190286, 1.9854463]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 3 / 3 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 1.3960602\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 1.0578989\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 0.223234, -0.07637 ,  0.589386]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[0.534175, 1.319029, 1.985446]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196b920>, array([[ 0.22323382, -0.07637027,  0.58938617]], dtype=float32), array([[0.5341751, 1.3190286, 1.9854463]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________________ test_matmul[cuda-3-4-5] ____________________________\u001b[0m\n","\n","m = 3, n = 4, p = 5, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]])\n","B          = needle.Tensor([[0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]])\n","_A         = array([[ 0.654816  ,  0.72850287, -0.2106692 , -0.17719436],\n","       [ 0.85219604,  0.45814183, -0.09138661, -1.1754771 ],\n","       [-0.23682164,  1.7254593 ,  0.7422256 , -0.0074945 ]],\n","      dtype=float32)\n","_B         = array([[-0.3631913 , -0.8601258 ,  0.3114279 ,  0.5743038 ,  0.24980997],\n","       [ 0.6041445 ,  1.287218  , -0.0696068...1749268 , -1.5551547 ],\n","       [ 0.62649083,  0.12255095, -1.0435718 ,  0.24988598,  0.3234629 ]],\n","      dtype=float32)\n","device     = cuda()\n","m          = 3\n","n          = 4\n","p          = 5\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c91969ee0>, array([[ 0.1366801 ,  0.710473  ,  0.23295565,  1.0570...557981 ,  0.5887635 ],\n","       [-0.21547143, -1.6977823 ,  0.49925917, -1.1749268 , -1.5551547 ]],\n","      dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 15 / 15 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 2.861465\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 21.32237\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 0.13668 ,  0.710473,  0.232956,  1.057057,  0.862803],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-0.749461, -0.132169,  1.414577,  0.603505,  0.244521],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.963815,  1.163683,  0.184527,  0.121612, -0.199973]],\u001b[0m\n","\u001b[1m\u001b[31mE                 dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[-0.363191, -0.860126,  0.311428,  0.574304,  0.24981 ],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.604145,  1.287218, -0.069607,  0.655798,  0.588763],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-0.215471, -1.697782,  0.499259, -1.174927, -1.555155]],\u001b[0m\n","\u001b[1m\u001b[31mE                 dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c91969ee0>, array([[ 0.1366801 ,  0.710473  ,  0.23295565,  1.0570...557981 ,  0.5887635 ],\n","       [-0.21547143, -1.6977823 ,  0.49925917, -1.1749268 , -1.5551547 ]],\n","      dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________________ test_matmul[cuda-5-4-3] ____________________________\u001b[0m\n","\n","m = 5, n = 4, p = 3, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]\n"," [0. 0. 0. 0.]])\n","B          = needle.Tensor([[-1.3892012  -1.4697326   1.0070543 ]\n"," [ 0.34883013 -2.0843477  -0.24267545]\n"," [ 0.11248029  1.987426   -0.49136162]\n"," [ 0.93339074 -1.0682596  -1.36612   ]])\n","_A         = array([[-1.3892012 , -1.4697326 ,  1.0070543 ,  0.34883013],\n","       [-2.0843477 , -0.24267545,  0.11248029,  1.987426 ...0366048 , -1.0864305 ,  0.47813606],\n","       [-0.03617776,  0.03262429, -1.8234262 , -0.15546963]],\n","      dtype=float32)\n","_B         = array([[ 0.7341658 ,  0.8313993 ,  0.08911297],\n","       [-0.10103321,  0.25325453,  0.22029683],\n","       [-1.2978022 , -0.55815214,  1.5079324 ],\n","       [ 0.19379255, -0.96344835, -0.30928165]], dtype=float32)\n","device     = cuda()\n","m          = 5\n","n          = 4\n","p          = 3\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d116c0>, array([[-2.1107688, -2.4253666,  0.9631099],\n","       [-...324 ],\n","       [ 0.19379255, -0.96344835, -0.30928165],\n","       [ 0.        ,  0.        ,  0.        ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 15 / 15 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.025202\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 15.893898\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[-2.110769, -2.425367,  0.96311 ],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-1.266567, -3.771947, -0.684265],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.666601,  1.740305, -1.026511],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 0.734166,  0.831399,  0.089113],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-0.101033,  0.253255,  0.220297],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-1.297802, -0.558152,  1.507932],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d116c0>, array([[-2.1107688, -2.4253666,  0.9631099],\n","       [-...324 ],\n","       [ 0.19379255, -0.96344835, -0.30928165],\n","       [ 0.        ,  0.        ,  0.        ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m__________________________ test_matmul[cuda-16-16-32] __________________________\u001b[0m\n","\n","m = 16, n = 16, p = 32, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [...0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","B          = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0... 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n","  0. 0. 0. 0. 0. 0. 0. 0.]])\n","_A         = array([[-5.92951536e-01, -8.68785977e-01,  1.28109825e+00,\n","        -2.13154539e-01,  1.37239945e+00, -3.68608564e-01,\n","..., -2.11009383e+00,\n","        -2.48701707e-01, -7.93531001e-01,  2.41460130e-01,\n","        -4.81520534e-01]], dtype=float32)\n","_B         = array([[-7.37000585e-01,  1.07231128e+00, -1.86253166e+00,\n","        -8.23480904e-01,  1.96635008e-01, -1.68239748e+00,\n","...,\n","        -1.90087244e-01, -8.86591554e-01,  1.40486643e-01,\n","        -8.94032776e-01,  2.30349619e-02]], dtype=float32)\n","device     = cuda()\n","m          = 16\n","n          = 16\n","p          = 32\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d11d00>, array([[ 5.79341412e+00,  4.84327936e+00,  2.86099672e...\n","        -1.90087244e-01, -8.86591554e-01,  1.40486643e-01,\n","        -8.94032776e-01,  2.30349619e-02]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 512 / 512 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 14.351611\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 2131.447\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 5.793414e+00,  4.843279e+00,  2.860997e+00,  4.788017e+00,\u001b[0m\n","\u001b[1m\u001b[31mE                    3.125614e+00,  4.703459e+00,  3.884049e+00, -4.495328e+00,\u001b[0m\n","\u001b[1m\u001b[31mE                   -5.414935e+00, -5.445869e+00, -1.181538e+00,  5.360313e+00,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[-7.370006e-01,  1.072311e+00, -1.862532e+00, -8.234809e-01,\u001b[0m\n","\u001b[1m\u001b[31mE                    1.966350e-01, -1.682397e+00, -6.327389e-01,  8.508228e-01,\u001b[0m\n","\u001b[1m\u001b[31mE                    3.926935e-01,  1.064973e+00, -2.998181e-01,  3.319875e-02,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d11d00>, array([[ 5.79341412e+00,  4.84327936e+00,  2.86099672e...\n","        -1.90087244e-01, -8.86591554e-01,  1.40486643e-01,\n","        -8.94032776e-01,  2.30349619e-02]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m__________________________ test_matmul[cuda-64-64-64] __________________________\u001b[0m\n","\n","m = 64, n = 64, p = 64, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","B          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","_A         = array([[-1.082387  , -0.31285337,  0.37914652, ..., -0.41516542,\n","         1.9055135 , -1.2175403 ],\n","       [-0.4006369...9],\n","       [-0.6071943 ,  0.6906254 , -0.8257664 , ..., -0.33885276,\n","         0.4185762 , -1.6399068 ]], dtype=float32)\n","_B         = array([[ 0.41004443,  1.1745555 ,  0.80214846, ..., -0.5857207 ,\n","         0.7658256 ,  0.8750583 ],\n","       [ 1.1478539... ],\n","       [ 0.11291716,  0.47895536, -1.1013322 , ...,  0.8595541 ,\n","        -1.5817857 ,  0.8475244 ]], dtype=float32)\n","device     = cuda()\n","m          = 64\n","n          = 64\n","p          = 64\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196b1a0>, array([[-15.203465  ,   0.10620427,  -6.083422  , ...,...],\n","       [ 0.11291716,  0.47895536, -1.1013322 , ...,  0.8595541 ,\n","        -1.5817857 ,  0.8475244 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 4096 / 4096 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 29.894709\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 28886.213\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[-15.203465,   0.106204,  -6.083422, ...,  -5.599523,  11.295276,\u001b[0m\n","\u001b[1m\u001b[31mE                    -2.460725],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ -3.898699,  -4.082157,   2.295377, ...,  -4.146647,  16.239416,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 0.410044,  1.174556,  0.802148, ..., -0.585721,  0.765826,\u001b[0m\n","\u001b[1m\u001b[31mE                    0.875058],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.147854, -0.258742, -0.01186 , ...,  0.462715, -1.834535,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196b1a0>, array([[-15.203465  ,   0.10620427,  -6.083422  , ...,...],\n","       [ 0.11291716,  0.47895536, -1.1013322 , ...,  0.8595541 ,\n","        -1.5817857 ,  0.8475244 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m__________________________ test_matmul[cuda-72-72-72] __________________________\u001b[0m\n","\n","m = 72, n = 72, p = 72, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","B          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","_A         = array([[-0.14028323,  0.60604614,  0.04811767, ...,  0.51525635,\n","         0.45924923,  1.9541698 ],\n","       [-1.1255426... ],\n","       [-0.13650139,  0.41142735, -1.6502085 , ..., -2.5761743 ,\n","         1.4478314 ,  2.2190225 ]], dtype=float32)\n","_B         = array([[ 0.24280068,  0.02310721,  1.9812624 , ..., -1.1015782 ,\n","        -0.3129008 ,  0.5849371 ],\n","       [ 1.4762851... ],\n","       [ 0.76414275,  0.25707817, -2.0578747 , ...,  0.6905931 ,\n","        -1.3811514 ,  0.6022044 ]], dtype=float32)\n","device     = cuda()\n","m          = 72\n","n          = 72\n","p          = 72\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196bce0>, array([[ 5.02835894e+00,  1.02126102e+01, -1.17517967e...],\n","       [ 0.76414275,  0.25707817, -2.0578747 , ...,  0.6905931 ,\n","        -1.3811514 ,  0.6022044 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 5184 / 5184 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 30.583199\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 17006.895\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 5.028359e+00,  1.021261e+01, -1.175180e+01, ..., -1.100217e+01,\u001b[0m\n","\u001b[1m\u001b[31mE                   -1.480679e+01, -8.878044e+00],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-1.147231e+01, -1.470090e+01, -1.303887e+01, ..., -9.544926e+00,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 0.242801,  0.023107,  1.981262, ..., -1.101578, -0.312901,\u001b[0m\n","\u001b[1m\u001b[31mE                    0.584937],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.476285,  1.900729,  0.006579, ...,  0.07687 , -0.130841,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196bce0>, array([[ 5.02835894e+00,  1.02126102e+01, -1.17517967e...],\n","       [ 0.76414275,  0.25707817, -2.0578747 , ...,  0.6905931 ,\n","        -1.3811514 ,  0.6022044 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m__________________________ test_matmul[cuda-72-73-74] __________________________\u001b[0m\n","\n","m = 72, n = 73, p = 74, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[ 0.4977691   0.0534094   0.25114632 ... -0.2682148   0.50150937\n","   1.470783  ]\n"," [ 0.11335862 -0.533734....         ...  0.          0.\n","   0.        ]\n"," [ 0.          0.          0.         ...  0.          0.\n","   0.        ]])\n","B          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","_A         = array([[-2.2869356 ,  2.4410696 , -0.06609856, ..., -0.6628585 ,\n","         1.2714901 , -1.6431442 ],\n","       [-0.6135641... ],\n","       [ 0.37932992,  0.2737079 ,  1.8354076 , ...,  0.7942683 ,\n","         2.1143708 ,  0.5757829 ]], dtype=float32)\n","_B         = array([[-0.81970483, -0.62832344,  0.58655894, ..., -0.7188211 ,\n","        -0.16015641, -0.38464722],\n","       [-0.9847221... ],\n","       [-0.8729308 , -0.37538484,  0.90310943, ...,  0.34306654,\n","        -0.10175896,  1.9507737 ]], dtype=float32)\n","device     = cuda()\n","m          = 72\n","n          = 73\n","p          = 74\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196a840>, array([[ -8.380574  ,  11.193316  ,  -4.323647  , ...,...],\n","       [ 2.1143708 ,  0.5757829 , -0.0918901 , ...,  1.4180171 ,\n","        -2.0698159 ,  0.2830565 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 5328 / 5328 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 37.21401\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 65906.4\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ -8.380574,  11.193316,  -4.323647, ...,   3.078425,   7.615777,\u001b[0m\n","\u001b[1m\u001b[31mE                    -6.272015],\u001b[0m\n","\u001b[1m\u001b[31mE                  [  8.949349,  17.359081,  11.323651, ...,  24.486877,   0.444648,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[-2.286936,  2.44107 , -0.066099, ...,  1.27149 , -1.643144,\u001b[0m\n","\u001b[1m\u001b[31mE                   -0.613564],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-2.632752,  0.229816, -1.111817, ...,  0.457476,  0.003578,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196a840>, array([[ -8.380574  ,  11.193316  ,  -4.323647  , ...,...],\n","       [ 2.1143708 ,  0.5757829 , -0.0918901 , ...,  1.4180171 ,\n","        -2.0698159 ,  0.2830565 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m__________________________ test_matmul[cuda-74-73-72] __________________________\u001b[0m\n","\n","m = 74, n = 73, p = 72, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","B          = needle.Tensor([[ 2.5433717   2.202355   -1.7542894  ... -0.6226457   0.4746837\n","   0.291169  ]\n"," [ 0.632095    0.5023340... 0.84277093 -0.4595621\n","  -0.20413521]\n"," [-0.45796904  1.9853209  -0.21903189 ...  1.3095028   0.9791505\n","   0.29373983]])\n","_A         = array([[ 2.5433717 ,  2.202355  , -1.7542894 , ...,  0.4746837 ,\n","         0.291169  ,  0.632095  ],\n","       [ 0.5023340...8],\n","       [ 0.3124429 ,  0.67045975, -0.2541804 , ...,  0.8254428 ,\n","        -0.9933159 , -1.3417903 ]], dtype=float32)\n","_B         = array([[ 0.86880785, -0.70741963, -0.14541002, ...,  0.33905905,\n","         0.03319501, -0.30614465],\n","       [ 0.3715162...3],\n","       [ 1.4325385 ,  0.01491717, -0.4534368 , ..., -1.5561616 ,\n","        -0.32822824, -0.85069335]], dtype=float32)\n","device     = cuda()\n","m          = 74\n","n          = 73\n","p          = 72\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d12700>, array([[ 10.933304  ,   2.5179043 , -12.230467  , ...,...],\n","       [-0.0918901 ,  0.0556215 , -1.0586392 , ...,  1.4180171 ,\n","        -2.0698159 ,  0.2830565 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 5328 / 5328 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 30.389038\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 24054.484\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 10.933304,   2.517904, -12.230467, ...,  14.394853,  -4.630666,\u001b[0m\n","\u001b[1m\u001b[31mE                     0.137446],\u001b[0m\n","\u001b[1m\u001b[31mE                  [  8.108294,  -9.103022,   0.123742, ...,  16.499578,   9.668077,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 0.868808, -0.70742 , -0.14541 , ...,  0.339059,  0.033195,\u001b[0m\n","\u001b[1m\u001b[31mE                   -0.306145],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.371516,  2.018187,  0.146865, ..., -0.384173, -0.83849 ,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d12700>, array([[ 10.933304  ,   2.5179043 , -12.230467  , ...,...],\n","       [-0.0918901 ,  0.0556215 , -1.0586392 , ...,  1.4180171 ,\n","        -2.0698159 ,  0.2830565 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m________________________ test_matmul[cuda-128-128-128] _________________________\u001b[0m\n","\n","m = 128, n = 128, p = 128, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mm,n,p\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, MATMUL_DIMS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_matmul\u001b[39;49;00m(m, n, p, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(m, n).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randn(n, p).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        B = ndl.Tensor(nd.array(_B), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A @ _B, (A @ B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","B          = needle.Tensor([[0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]])\n","_A         = array([[ 1.9537501 ,  0.05903736, -1.2711045 , ...,  0.22699478,\n","        -0.10282926, -0.18651716],\n","       [-0.2311179...2],\n","       [ 0.06558133, -1.1310216 , -1.9421903 , ..., -1.8011059 ,\n","         1.3072039 ,  0.5935067 ]], dtype=float32)\n","_B         = array([[ 0.9572396 , -0.46439824, -1.6827878 , ..., -0.11662703,\n","        -1.5686997 ,  0.01031934],\n","       [-0.1855606...7],\n","       [-0.8687678 , -1.4076406 ,  2.1396523 , ...,  1.6590946 ,\n","        -0.4116172 , -0.4557549 ]], dtype=float32)\n","device     = cuda()\n","m          = 128\n","n          = 128\n","p          = 128\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:93: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d12980>, array([[ -5.17393   ,  -5.553888  , -13.896679  , ...,...],\n","       [-0.8687678 , -1.4076406 ,  2.1396523 , ...,  1.6590946 ,\n","        -0.4116172 , -0.4557549 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 16384 / 16384 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 48.849308\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 41544.805\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ -5.17393 ,  -5.553888, -13.896679, ...,   2.394024,   6.857246,\u001b[0m\n","\u001b[1m\u001b[31mE                    -1.200997],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-19.615627, -11.097386,   8.644199, ...,  18.860813, -13.985773,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 0.95724 , -0.464398, -1.682788, ..., -0.116627, -1.5687  ,\u001b[0m\n","\u001b[1m\u001b[31mE                    0.010319],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-0.185561,  0.324603, -0.471762, ..., -0.409862,  0.11312 ,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d12980>, array([[ -5.17393   ,  -5.553888  , -13.896679  , ...,...],\n","       [-0.8687678 , -1.4076406 ,  2.1396523 , ...,  1.6590946 ,\n","        -0.4116172 , -0.4557549 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________________ test_power[cuda-shape0] ____________________________\u001b[0m\n","\n","shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[1.171145]]], dtype=float32)\n","_B         = 0\n","device     = cuda()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d109a0>, array([[[1.]]], dtype=float32), array([[[1.171145]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.17114496\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 0.14613473\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[1.]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[1.171145]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d109a0>, array([[[1.]]], dtype=float32), array([[[1.171145]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________________ test_power[cuda-shape1] ____________________________\u001b[0m\n","\n","shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_power\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _B = np.random.randint(\u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(_A**_B, (A**_B).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[-0.6280028 , -0.43987197,  2.9139524 ,  0.05009432,\n","         -0.73577106, -0.47775924],\n","        [-1.7431105 ,...16 ],\n","        [-0.8517918 , -1.9387237 ,  0.42015556, -0.24831858,\n","         -1.0194412 ,  0.38210192]]], dtype=float32)\n","_B         = 0\n","device     = cuda()\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:102: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d114e0>, array([[[1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1.,...6 ],\n","        [-0.8517918 , -1.9387237 ,  0.42015556, -0.24831858,\n","         -1.0194412 ,  0.38210192]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 3.9324012\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 232.39035\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[1., 1., 1., 1., 1., 1.],\u001b[0m\n","\u001b[1m\u001b[31mE                   [1., 1., 1., 1., 1., 1.],\u001b[0m\n","\u001b[1m\u001b[31mE                   [1., 1., 1., 1., 1., 1.],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.628003, -0.439872,  2.913952,  0.050094, -0.735771,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.477759],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-1.743111, -1.975679, -0.356641, -1.985647, -2.264599,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d114e0>, array([[[1., 1., 1., 1., 1., 1.],\n","        [1., 1., 1.,...6 ],\n","        [-0.8517918 , -1.9387237 ,  0.42015556, -0.24831858,\n","         -1.0194412 ,  0.38210192]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_log[cuda-shape0] _____________________________\u001b[0m\n","\n","shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_log\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32) + \u001b[94m5.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.log(_A), ndl.log(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[6.5344124]]], dtype=float32)\n","device     = cuda()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:110: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d13240>, array([[[1.8770825]]], dtype=float32), array([[[0.7341658]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 1.1429167\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 1.5567555\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[1.877082]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.734166]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d13240>, array([[[1.8770825]]], dtype=float32), array([[[0.7341658]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_log[cuda-shape1] _____________________________\u001b[0m\n","\n","shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_log\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32) + \u001b[94m5.\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.log(_A), ndl.log(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[ 6.5344124e+00 -4.3987197e-01  2.9139524e+00  5.0094321e-02\n","   -7.3577106e-01 -4.7775924e-01]\n","  [-1.7...-01  1.2529916e+00]\n","  [-8.5179180e-01 -1.9387237e+00  4.2015556e-01 -2.4831858e-01\n","   -1.0194412e+00  3.8210192e-01]]])\n","_A         = array([[[3.516722 , 4.4941716, 3.9078727, 5.770175 , 4.984753 ,\n","         5.790717 ],\n","        [2.6877737, 4.846334 , 5....      5.082136 ],\n","        [6.005202 , 4.3703594, 5.3648767, 4.616481 , 5.4491186,\n","         5.858667 ]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:110: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196ae80>, array([[[1.2575293 , 1.5027814 , 1.3629931 , 1.7527025...     5.082136 ],\n","        [6.005202 , 4.3703594, 5.3648767, 4.616481 , 5.4491186,\n","         5.858667 ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 6.2029753\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 0.74538314\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[1.257529, 1.502781, 1.362993, 1.752702, 1.606384, 1.756256],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0.988713, 1.578223, 1.681039, 1.730417, 1.463987, 1.698015],\u001b[0m\n","\u001b[1m\u001b[31mE                   [1.195163, 1.586676, 1.540191, 1.471272, 1.7891  , 1.550994],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[3.516722, 4.494172, 3.907873, 5.770175, 4.984753, 5.790717],\u001b[0m\n","\u001b[1m\u001b[31mE                   [2.687774, 4.846334, 5.371132, 5.643005, 4.323163, 5.463094],\u001b[0m\n","\u001b[1m\u001b[31mE                   [3.304096, 4.887478, 4.665481, 4.354769, 5.984065, 4.716154],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196ae80>, array([[[1.2575293 , 1.5027814 , 1.3629931 , 1.7527025...     5.082136 ],\n","        [6.005202 , 4.3703594, 5.3648767, 4.616481 , 5.4491186,\n","         5.858667 ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_exp[cuda-shape0] _____________________________\u001b[0m\n","\n","shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_exp\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.exp(_A), ndl.exp(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[0.14809337]]], dtype=float32)\n","device     = cuda()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:118: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d12de0>, array([[[1.1596211]]], dtype=float32), array([[[0.]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 1.1596211\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[1.159621]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d12de0>, array([[[1.1596211]]], dtype=float32), array([[[0.]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_exp[cuda-shape1] _____________________________\u001b[0m\n","\n","shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_exp\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.exp(_A), ndl.exp(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[ 2.4905684 , -0.6954556 , -1.8243234 , -0.31614092,\n","          0.08606078, -0.74876255],\n","        [ 1.3998232 ,...898],\n","        [ 0.1688959 ,  0.02774058, -1.563431  ,  0.17357937,\n","          0.47140115,  1.1728562 ]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:118: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d13560>, array([[[12.068133  ,  0.49884713,  0.16132677,  0.728...98],\n","        [ 0.1688959 ,  0.02774058, -1.563431  ,  0.17357937,\n","          0.47140115,  1.1728562 ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 120 / 120 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 22.924566\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 71.43084\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[12.068133,  0.498847,  0.161327,  0.728957,  1.089873,\u001b[0m\n","\u001b[1m\u001b[31mE                     0.472951],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 4.054483,  0.333672,  0.752667,  0.707268,  2.469876,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[ 2.490568, -0.695456, -1.824323, -0.316141,  0.086061,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.748763],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 1.399823, -1.097597, -0.284132, -0.346346,  0.904168,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d13560>, array([[[12.068133  ,  0.49884713,  0.16132677,  0.728...98],\n","        [ 0.1688959 ,  0.02774058, -1.563431  ,  0.17357937,\n","          0.47140115,  1.1728562 ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_relu[cuda-shape1] ____________________________\u001b[0m\n","\n","shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_relu\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.maximum(_A, \u001b[94m0\u001b[39;49;00m), ndl.relu(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[ 0.86880785 -0.70741963 -0.14541002  2.4232674  -1.0968686\n","    0.73122996]\n","  [-1.9972199  -0.1773353 ...-1.0444434  -0.9494368\n","    1.6016346 ]\n","  [-2.4813304   0.9830089  -1.6712784   0.1171381   1.9512353\n","    0.00520304]]])\n","_A         = array([[[-0.05377588,  0.20338777,  0.16131176,  0.04317565,\n","         -1.4132142 ,  0.14171205],\n","        [-0.5501977 ,...069],\n","        [-0.79452735,  0.3885921 ,  0.13083026,  0.47227657,\n","         -2.1078956 ,  0.21374775]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:126: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d11d00>, array([[[0.        , 0.20338777, 0.16131176, 0.0431756...69],\n","        [-0.79452735,  0.3885921 ,  0.13083026,  0.47227657,\n","         -2.1078956 ,  0.21374775]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 49 / 120 (40.8%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 2.36583\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 1.\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[0.      , 0.203388, 0.161312, 0.043176, 0.      , 0.141712],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0.      , 0.      , 0.168161, 0.      , 1.794351, 0.      ],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0.200031, 1.195579, 0.      , 0.      , 1.196289, 0.      ],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.053776,  0.203388,  0.161312,  0.043176, -1.413214,\u001b[0m\n","\u001b[1m\u001b[31mE                     0.141712],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.550198, -1.156469,  0.168161, -0.797816,  1.794351,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d11d00>, array([[[0.        , 0.20338777, 0.16131176, 0.0431756...69],\n","        [-0.79452735,  0.3885921 ,  0.13083026,  0.47227657,\n","         -2.1078956 ,  0.21374775]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape0] ____________________________\u001b[0m\n","\n","shape = (1, 1, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[-0.32874075]]])\n","_A         = array([[[-2.3530958]]], dtype=float32)\n","device     = cuda()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d13ce0>, array([[[-0.9820837]]], dtype=float32), array([[[-2.3530958]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 1.3710121\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 0.58264184\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.982084]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-2.353096]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d13ce0>, array([[[-0.9820837]]], dtype=float32), array([[[-2.3530958]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m____________________________ test_tanh[cuda-shape1] ____________________________\u001b[0m\n","\n","shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_tanh\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.tanh(_A), ndl.tanh(A).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[-0.3843144  -0.75144523 -1.770937   -0.02603321 -0.41861278\n","   -0.5232033 ]\n","  [-2.3042443  -0.5429495... 0.52975696 -1.398209\n","   -1.5902878 ]\n","  [ 0.3040398  -0.7475126  -0.5147766   2.4157639  -0.12393533\n","    0.5282001 ]]])\n","_A         = array([[[-0.5291037 , -1.1316243 ,  1.2829843 ,  0.6779355 ,\n","         -0.32166547, -0.51734763],\n","        [ 0.48438653,...48 ],\n","        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n","          0.99664706, -1.344323  ]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:134: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919dc2c0>, array([[[-0.48469567, -0.81157446,  0.857278  ,  0.590...8 ],\n","        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n","          0.99664706, -1.344323  ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 117 / 120 (97.5%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 1.0358227\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 0.5179531\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.484696, -0.811574,  0.857278,  0.590176, -0.311012,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.47565 ],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.44975 , -0.825315,  0.907951,  0.793254,  0.806773,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.529104, -1.131624,  1.282984,  0.677935, -0.321665,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.517348],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.484387, -1.173263,  1.51573 ,  1.080149,  1.117716,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919dc2c0>, array([[[-0.48469567, -0.81157446,  0.857278  ,  0.590...8 ],\n","        [-0.8919766 , -0.5311779 ,  1.3061328 ,  0.70647854,\n","          0.99664706, -1.344323  ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_______________________ test_tanh_backward[cuda-shape1] ________________________\u001b[0m\n","\n","shape = (4, 5, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, GENERAL_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_tanh_backward\u001b[39;49;00m(shape, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.tanh, A)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[ 0.86880785 -0.70741963 -0.14541002  2.4232674  -1.0968686\n","    0.73122996]\n","  [-1.9972199  -0.1773353 ...-1.0444434  -0.9494368\n","    1.6016346 ]\n","  [-2.4813304   0.9830089  -1.6712784   0.1171381   1.9512353\n","    0.00520304]]])\n","_A         = array([[[ 0.6276671 ,  1.7709465 ,  2.5802875 ,  1.0994289 ,\n","         -0.25607443, -0.30351138],\n","        [-1.2755342 ,...12 ],\n","        [-0.8847228 , -1.9263881 , -1.6778411 ,  0.455592  ,\n","          0.29220578,  1.1388148 ]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:142: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function tanh at 0x137d74d556c0>\n","args = (needle.Tensor([[[ 0.86880785 -0.70741963 -0.14541002  2.4232674  -1.0968686\n","    0.73122996]\n","  [-1.9972199  -0.1773353....0444434  -0.9494368\n","    1.6016346 ]\n","  [-2.4813304   0.9830089  -1.6712784   0.1171381   1.9512353\n","    0.00520304]]]),)\n","kwargs = {}, eps = 1e-05\n","out = needle.Tensor([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01  6.2845510e-01]\n","  [-4.9...+00 -2.4180122e-01]\n","  [-2.0745020e+00 -3.3635896e-01  3.5263869e-01 -6.9581836e-01\n","   -1.7307004e+00  1.0185822e+00]]])\n","c = array([[[-0.95230234, -0.63597503, -0.19050878,  1.35308429,\n","         -2.45410143, -0.07941442],\n","        [ 0.10208082,...30038, -0.55089105],\n","        [ 0.04892026,  2.01325595,  0.27848269,  0.11210785,\n","         -0.23827138,  1.09982379]]])\n","num_args = 1, i = 0, j = 119, f1 = np.float64(-12.102848049822386)\n","f2 = np.float64(-12.102848049822386), error = np.float64(10.769484291355438)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m4.2e-1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(10.769484291355438) < 0.42\u001b[0m\n","\n","args       = (needle.Tensor([[[ 0.86880785 -0.70741963 -0.14541002  2.4232674  -1.0968686\n","    0.73122996]\n","  [-1.9972199  -0.1773353....0444434  -0.9494368\n","    1.6016346 ]\n","  [-2.4813304   0.9830089  -1.6712784   0.1171381   1.9512353\n","    0.00520304]]]),)\n","backward_grad = (needle.Tensor([[[-0.11904067 -1.5495101   0.08427918  0.9816303   1.1716882\n","    0.49106193]\n","  [-0.03200253  1.7679057...35365704 -0.2695059\n","   -0.8694885 ]\n","  [ 0.17268623 -0.10636393 -1.1813806   0.2179534  -0.73230654\n","   -0.6999966 ]]]),)\n","c          = array([[[-0.95230234, -0.63597503, -0.19050878,  1.35308429,\n","         -2.45410143, -0.07941442],\n","        [ 0.10208082,...30038, -0.55089105],\n","        [ 0.04892026,  2.01325595,  0.27848269,  0.11210785,\n","         -0.23827138,  1.09982379]]])\n","eps        = 1e-05\n","error      = np.float64(10.769484291355438)\n","f          = <function tanh at 0x137d74d556c0>\n","f1         = np.float64(-12.102848049822386)\n","f2         = np.float64(-12.102848049822386)\n","i          = 0\n","j          = 119\n","kwargs     = {}\n","num_args   = 1\n","numerical_grad = [array([[[0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0...., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.]]])]\n","out        = needle.Tensor([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01  6.2845510e-01]\n","  [-4.9...+00 -2.4180122e-01]\n","  [-2.0745020e+00 -3.3635896e-01  3.5263869e-01 -6.9581836e-01\n","   -1.7307004e+00  1.0185822e+00]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:33: AssertionError\n","\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape1-0-2] __________________________\u001b[0m\n","\n","shape = (5, 5), axis = 0, l = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n","        out_t = torch.stack(A_t, dim=axis)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(out_t.numpy(), out.numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[ 6.6157240e-01 -8.8291633e-01  9.8894596e-01  2.3833154e-01\n","  -1.0510052e+00]\n"," [ 5.6525648e-01  1.101...9917  1.5769796  -2.5110445   2.2139328  -1.0675079 ]\n"," [-0.8125158   0.34947073  0.476301    0.33545637 -1.122424  ]])]\n","A_t        = [tensor([[-0.4484,  1.0540,  0.0305, -0.4769,  1.5635],\n","        [-1.0746,  0.9731, -1.0777, -1.1209, -0.4109],\n","       ....5013],\n","        [ 0.7467,  0.2934,  0.6407,  0.2455, -0.8840],\n","        [ 2.7281,  3.3226, -0.2855,  0.3290,  0.2979]])]\n","_A         = [array([[-0.4484034 ,  1.0540462 ,  0.03045906, -0.4768823 ,  1.5634812 ],\n","       [-1.0746397 ,  0.9731278 , -1.077711...4550702, -0.8840323 ],\n","       [ 2.7280824 ,  3.322596  , -0.28550744,  0.32897174,  0.29785007]],\n","      dtype=float32)]\n","axis       = 0\n","device     = cuda()\n","l          = 2\n","out        = needle.Tensor([[[-1.0035697   1.4039029  -0.58463264 -1.2771463  -0.9480996 ]\n","  [ 0.8200676   1.0044218   0.35027558  ...43  -0.8436964   0.94281155  0.0096883   0.28348145]\n","  [ 2.9316173  -1.4829825   0.5405916   0.80236626 -1.2171426 ]]])\n","out_t      = tensor([[[-0.4484,  1.0540,  0.0305, -0.4769,  1.5635],\n","         [-1.0746,  0.9731, -1.0777, -1.1209, -0.4109],\n","      ...013],\n","         [ 0.7467,  0.2934,  0.6407,  0.2455, -0.8840],\n","         [ 2.7281,  3.3226, -0.2855,  0.3290,  0.2979]]])\n","shape      = (5, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:156: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d11940>, array([[[-0.4484034 ,  1.0540462 ,  0.03045906, -0.476...     0.28348145],\n","        [ 2.9316173 , -1.4829825 ,  0.5405916 ,  0.80236626,\n","         -1.2171426 ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 50 / 50 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.8055787\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 24.340576\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.448403,  1.054046,  0.030459, -0.476882,  1.563481],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-1.07464 ,  0.973128, -1.077712, -1.120895, -0.41088 ],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.183382,  1.060166,  0.652786,  0.597207,  0.969281],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-1.00357 ,  1.403903, -0.584633, -1.277146, -0.9481  ],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.820068,  1.004422,  0.350276,  1.327755,  0.220132],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 1.267629,  0.538849, -0.446186,  1.996467,  1.501295],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d11940>, array([[[-0.4484034 ,  1.0540462 ,  0.03045906, -0.476...     0.28348145],\n","        [ 2.9316173 , -1.4829825 ,  0.5405916 ,  0.80236626,\n","         -1.2171426 ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_________________________ test_stack[cuda-shape2-2-5] __________________________\u001b[0m\n","\n","shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        out = ndl.stack(A, axis=axis)\u001b[90m\u001b[39;49;00m\n","        out_t = torch.stack(A_t, dim=axis)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(out_t.numpy(), out.numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[[ 0.7154689   0.12812123 -0.4501938  -0.15363154 -0.7921522\n","   -0.9739061   1.2552259 ]\n","  [ 1.1897821... -0.16565436 -1.3301085 ]\n","  [ 1.1904292   2.1352863   1.0796812  -1.6482835   1.4176877\n","   -2.033359    0.9832383 ]]])]\n","A_t        = [tensor([[[ 0.3833,  0.1805,  0.1305,  0.8502,  0.6713,  1.1949, -0.3069],\n","         [-0.4117, -0.2897,  0.5881,  1.263...1644,  0.5059,  0.1374, -0.3010,  0.6344],\n","         [-0.0400, -0.7133,  1.1940,  0.3981, -1.5425, -1.0406, -1.0655]]])]\n","_A         = [array([[[ 0.38333625,  0.18051435,  0.1305337 ,  0.8501626 ,\n","          0.6712771 ,  1.1948769 , -0.30687174],\n","       ...[-0.04002846, -0.7133349 ,  1.1940389 ,  0.39805946,\n","         -1.54246   , -1.0406394 , -1.0654556 ]]], dtype=float32)]\n","axis       = 2\n","device     = cuda()\n","l          = 5\n","out        = needle.Tensor([[[[ 1.3927809  -1.5096966  -0.9114776  -0.37042677  0.7603414\n","     0.3224579   1.332678  ]\n","   [ 0.30677...0.7239937   1.4136596 ]\n","   [-1.5136725   0.22765186 -0.2193532  -0.90291476  1.4750397\n","    -0.3753992  -0.35506016]]]])\n","out_t      = tensor([[[[ 0.3833,  0.1805,  0.1305,  0.8502,  0.6713,  1.1949, -0.3069],\n","          [ 0.9905,  0.3983,  0.5823, -1.12...544,  1.0476, -1.5948,  1.2582, -1.8862],\n","          [-0.0400, -0.7133,  1.1940,  0.3981, -1.5425, -1.0406, -1.0655]]]])\n","shape      = (1, 5, 7)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:156: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919dd120>, array([[[[ 0.38333625,  0.18051435,  0.1305337 ,  0.85...1.5136725 ,  0.22765186, -0.2193532 , -0.90291476,\n","           1.4750397 , -0.3753992 , -0.35506016]]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 175 / 175 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.3943143\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 146.73409\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[[ 0.383336,  0.180514,  0.130534,  0.850163,  0.671277,\u001b[0m\n","\u001b[1m\u001b[31mE                      1.194877, -0.306872],\u001b[0m\n","\u001b[1m\u001b[31mE                    [ 0.990475,  0.398295,  0.58231 , -1.122874,  0.513346,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[[ 1.392781, -1.509697, -0.911478, -0.370427,  0.760341,\u001b[0m\n","\u001b[1m\u001b[31mE                      0.322458,  1.332678],\u001b[0m\n","\u001b[1m\u001b[31mE                    [ 0.306772,  0.707401,  3.842211, -1.198045, -0.091165,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919dd120>, array([[[[ 0.38333625,  0.18051435,  0.1305337 ,  0.85...1.5136725 ,  0.22765186, -0.2193532 , -0.90291476,\n","           1.4750397 , -0.3753992 , -0.35506016]]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape0-0-1] ______________________\u001b[0m\n","\n","shape = (5, 5), axis = 0, l = 1, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n","            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n"," [-0.19517688  1.2884692   1.625865   -1...0993  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n"," [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]])]\n","A_t        = [tensor([[-0.2649, -2.1368, -0.2813,  0.5420, -0.8280],\n","        [-0.1952,  1.2885,  1.6259, -1.3193,  0.6103],\n","       ...3294,  0.6092,  1.7408, -0.1585,  0.4973],\n","        [ 0.7943,  0.8823, -1.2204, -0.5734, -0.6012]], requires_grad=True)]\n","_A         = [array([[-0.26491052, -2.136809  , -0.28127787,  0.54197794, -0.8279596 ],\n","       [-0.19517688,  1.2884692 ,  1.625865...5845154,  0.4973323 ],\n","       [ 0.7943122 ,  0.8822589 , -1.2203852 , -0.5734197 , -0.6012362 ]],\n","      dtype=float32)]\n","axis       = 0\n","device     = cpu()\n","i          = 0\n","l          = 1\n","shape      = (5, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n","  [-0.19517688  1.2884692   1.625865   -...993  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n","  [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n","  [-0.19517688  1.2884692   1.625865   ...3  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n","  [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d0f20>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n","  [-0.19517688  1.2884692   1.625865   ...3  0.60915947  1.7407578  -0.15845154  0.4973323 ]\n","  [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919d0f20>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d22a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d22a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919d0f20>\n","a = NDArray([[[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n","  [-0.19517688  1.2884692   1.625865   -1.3193...7  1.7407578  -0.15845154  0.4973323 ]\n","  [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[-0.26491052 -2.136809   -0.28127787  0.54197794 -0.8279596 ]\n","  [-0.19517688  1.2884692   1.625865   -1.3193...7  1.7407578  -0.15845154  0.4973323 ]\n","  [ 0.7943122   0.8822589  -1.2203852  -0.5734197  -0.6012362 ]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d0f20>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape1-0-2] ______________________\u001b[0m\n","\n","shape = (5, 5), axis = 0, l = 2, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n","            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n"," [-0.73380625 -1.1747231  -0.3217507   0...209   0.0315434   0.55054754 -0.9497181  -0.03779113]\n"," [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]])]\n","A_t        = [tensor([[-0.2230,  0.0861,  0.1369,  1.5095, -0.7127],\n","        [-0.7338, -1.1747, -0.3218,  0.8628, -1.1587],\n","       ...7436,  0.0315,  0.5505, -0.9497, -0.0378],\n","        [-1.6837, -1.3373,  1.0488,  2.2584,  0.9400]], requires_grad=True)]\n","_A         = [array([[-0.2229816 ,  0.08610313,  0.13694113,  1.5095038 , -0.7126971 ],\n","       [-0.73380625, -1.1747231 , -0.321750...497181 , -0.03779113],\n","       [-1.6837281 , -1.3372594 ,  1.0487753 ,  2.258363  ,  0.94000864]],\n","      dtype=float32)]\n","axis       = 0\n","device     = cpu()\n","i          = 1\n","l          = 2\n","shape      = (5, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n","  [-0.73380625 -1.1747231  -0.3217507   ...09   0.0315434   0.55054754 -0.9497181  -0.03779113]\n","  [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n","  [-0.73380625 -1.1747231  -0.3217507  ...   0.0315434   0.55054754 -0.9497181  -0.03779113]\n","  [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d3c50>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n","  [-0.73380625 -1.1747231  -0.3217507  ...   0.0315434   0.55054754 -0.9497181  -0.03779113]\n","  [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919d3c50>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d3e60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d3e60>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919d3c50>\n","a = NDArray([[[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n","  [-0.73380625 -1.1747231  -0.3217507   0.8627...   0.55054754 -0.9497181  -0.03779113]\n","  [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[-0.2229816   0.08610313  0.13694113  1.5095038  -0.7126971 ]\n","  [-0.73380625 -1.1747231  -0.3217507   0.8627...   0.55054754 -0.9497181  -0.03779113]\n","  [-1.6837281  -1.3372594   1.0487753   2.258363    0.94000864]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d3c50>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_____________________ test_stack_backward[cpu-shape2-2-5] ______________________\u001b[0m\n","\n","shape = (1, 5, 7), axis = 2, l = 5, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n","            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n","   -1.0202792  -0.12881304]\n","  [-0.1319975... 0.8690877   0.2656894 ]\n","  [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n","   -1.0931357   0.24931994]]])]\n","A_t        = [tensor([[[ 1.4049,  2.0662,  1.9755,  0.7066, -1.1856, -1.0203, -0.1288],\n","         [-0.1320, -0.5643,  0.2627, -0.298...8691,  0.2657],\n","         [ 1.7977,  0.2424, -0.9547, -0.7499,  0.0561, -1.0931,  0.2493]]],\n","       requires_grad=True)]\n","_A         = [array([[[ 1.4048624 ,  2.0662467 ,  1.975455  ,  0.70656043,\n","         -1.1856185 , -1.0202792 , -0.12881304],\n","       ...[ 1.7976619 ,  0.24236043, -0.95474315, -0.74990946,\n","          0.05611507, -1.0931357 ,  0.24931994]]], dtype=float32)]\n","axis       = 2\n","device     = cpu()\n","i          = 4\n","l          = 5\n","shape      = (1, 5, 7)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n","    -1.0202792  -0.12881304]\n","   [ 0.41845....7191777   1.2221696 ]\n","   [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n","    -1.0931357   0.24931994]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n","    -1.0202792  -0.12881304]\n","   [ 0.4184...191777   1.2221696 ]\n","   [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n","    -1.0931357   0.24931994]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c50>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n","    -1.0202792  -0.12881304]\n","   [ 0.4184...191777   1.2221696 ]\n","   [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n","    -1.0931357   0.24931994]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c50>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919f2c90>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919f2c90>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c50>\n","a = NDArray([[[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n","    -1.0202792  -0.12881304]\n","   [ 0.4184599   0...221696 ]\n","   [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n","    -1.0931357   0.24931994]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.4048624   2.0662467   1.975455    0.70656043 -1.1856185\n","    -1.0202792  -0.12881304]\n","   [ 0.4184599   0...221696 ]\n","   [ 1.7976619   0.24236043 -0.95474315 -0.74990946  0.05611507\n","    -1.0931357   0.24931994]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c50>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape0-0-1] _____________________\u001b[0m\n","\n","shape = (5, 5), axis = 0, l = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n","            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[ 0.86880785 -0.70741963 -0.14541002  2.4232674  -1.0968686 ]\n"," [ 0.73122996 -1.9972199  -0.1773353  -0...3016  1.9181329   1.3844903   0.24238469 -0.1497871 ]\n"," [-1.2934906  -0.41482008 -0.23039535 -0.41272727  1.0557821 ]])]\n","A_t        = [tensor([[-1.1226,  1.3219, -0.1445, -0.9866,  0.2616],\n","        [ 0.9347, -0.3751,  0.2546,  0.2845,  0.7044],\n","       ...3590,  0.8249,  0.5144, -0.1400, -0.2062],\n","        [-0.2305, -2.1563, -1.2539,  1.0906, -0.5155]], requires_grad=True)]\n","_A         = [array([[-1.122568  ,  1.3219295 , -0.14445436, -0.9865747 ,  0.2616311 ],\n","       [ 0.93468   , -0.37505797,  0.254606...4003062, -0.20621192],\n","       [-0.23054962, -2.1563048 , -1.2538761 ,  1.090643  , -0.51554435]],\n","      dtype=float32)]\n","axis       = 0\n","device     = cuda()\n","i          = 0\n","l          = 1\n","shape      = (5, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01]\n","  [ 6.2845510e-01 -4.9...  1.0023479e+00\n","    4.0598720e-01]\n","  [ 6.5826502e-04 -8.5298610e-01 -8.3761197e-01  1.8905207e+00\n","   -2.5722811e-01]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01]\n","  [ 6.2845510e-01 -4....1.0023479e+00\n","    4.0598720e-01]\n","  [ 6.5826502e-04 -8.5298610e-01 -8.3761197e-01  1.8905207e+00\n","   -2.5722811e-01]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d1550>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01]\n","  [ 6.2845510e-01 -4....1.0023479e+00\n","    4.0598720e-01]\n","  [ 6.5826502e-04 -8.5298610e-01 -8.3761197e-01  1.8905207e+00\n","   -2.5722811e-01]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919d1550>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d19d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d19d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919d1550>\n","a = NDArray([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01]\n","  [ 6.2845510e-01 -4.9721878...\n","    4.0598720e-01]\n","  [ 6.5826502e-04 -8.5298610e-01 -8.3761197e-01  1.8905207e+00\n","   -2.5722811e-01]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[-3.2874075e-01 -7.3191094e-01 -6.7036712e-01 -1.0646241e-01\n","    4.3553030e-01]\n","  [ 6.2845510e-01 -4.9721878...\n","    4.0598720e-01]\n","  [ 6.5826502e-04 -8.5298610e-01 -8.3761197e-01  1.8905207e+00\n","   -2.5722811e-01]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d1550>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape1-0-2] _____________________\u001b[0m\n","\n","shape = (5, 5), axis = 0, l = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n","            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[ 6.5344124e+00 -4.3987197e-01  2.9139524e+00  5.0094321e-02\n","  -7.3577106e-01]\n"," [-4.7775924e-01 -1.743...4202e+00]]), needle.Tensor([[0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]])]\n","A_t        = [tensor([[-1.0568, -0.3027,  0.4282, -0.6883,  1.6516],\n","        [ 0.7430, -0.2764,  0.7661, -0.8939,  1.2894],\n","       ...6141,  0.6806,  0.5635,  1.0086,  0.7597],\n","        [ 0.3135, -2.0439, -0.0910,  0.1101, -0.2324]], requires_grad=True)]\n","_A         = [array([[-1.0567564 , -0.30265552,  0.4281822 , -0.68831074,  1.6516227 ],\n","       [ 0.74296576, -0.27638575,  0.766072...085729 ,  0.75971615],\n","       [ 0.31350297, -2.0439312 , -0.09097444,  0.11011965, -0.23238643]],\n","      dtype=float32)]\n","axis       = 0\n","device     = cuda()\n","i          = 1\n","l          = 2\n","shape      = (5, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d1af0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919d1af0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d1e20>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d1e20>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919d1af0>\n","a = NDArray([[[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0.]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d1af0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_____________________ test_stack_backward[cuda-shape2-2-5] _____________________\u001b[0m\n","\n","shape = (1, 5, 7), axis = 2, l = 5, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axis, l\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, STACK_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_stack_backward\u001b[39;49;00m(shape, axis, l, device):\u001b[90m\u001b[39;49;00m\n","        _A = [np.random.randn(*shape).astype(np.float32) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A = [ndl.Tensor(nd.array(_A[i]), device=device) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        A_t = [torch.Tensor(_A[i]) \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l)]\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(l):\u001b[90m\u001b[39;49;00m\n","            A_t[i].requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       ndl.stack(A, axis=axis).sum().backward()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = [needle.Tensor([[[-0.4857571  -0.34173542  1.428068   -1.4228437  -1.5166509\n","   -1.6472323  -0.27967188]\n","  [-0.5221117...01]\n","  [ 2.1807544e-01  1.0986426e+00 -7.4709648e-01 -1.4509449e+00\n","   -1.5804800e-01 -8.0194801e-01 -2.2773589e-01]]])]\n","A_t        = [tensor([[[-1.2567, -0.0045,  0.0332,  0.4080, -0.1314, -1.1836,  0.1577],\n","         [ 0.4062, -0.9746, -0.8885, -0.661...9483,  0.2169],\n","         [ 0.4364, -0.4950,  1.4167, -0.2632,  1.5032, -1.2881, -0.2428]]],\n","       requires_grad=True)]\n","_A         = [array([[[-1.2567368 , -0.00447594,  0.03316377,  0.4079615 ,\n","         -0.13135351, -1.1835566 ,  0.15766405],\n","       ...[ 0.43637753, -0.49504337,  1.4166641 , -0.2632424 ,\n","          1.5031598 , -1.2881242 , -0.24279231]]], dtype=float32)]\n","axis       = 2\n","device     = cuda()\n","i          = 4\n","l          = 5\n","shape      = (1, 5, 7)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:167: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 8.24896812e-01  6.52850270e-01 -4.06272173e-01  1.41356230e-01\n","     3.85968059e-01 -1.23175561e+00 ...1.21913922e+00  1.92275971e-01  2.26324070e-02 -1.15882170e+00\n","     1.82729661e-01 -1.45694956e-01  1.81304570e-03]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 8.24896812e-01  6.52850270e-01 -4.06272173e-01  1.41356230e-01\n","     3.85968059e-01 -1.23175561e+00...21913922e+00  1.92275971e-01  2.26324070e-02 -1.15882170e+00\n","     1.82729661e-01 -1.45694956e-01  1.81304570e-03]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d2c30>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 8.24896812e-01  6.52850270e-01 -4.06272173e-01  1.41356230e-01\n","     3.85968059e-01 -1.23175561e+00...21913922e+00  1.92275971e-01  2.26324070e-02 -1.15882170e+00\n","     1.82729661e-01 -1.45694956e-01  1.81304570e-03]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919d2c30>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d25a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919d25a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919d2c30>\n","a = NDArray([[[[ 8.24896812e-01  6.52850270e-01 -4.06272173e-01  1.41356230e-01\n","     3.85968059e-01 -1.23175561e+00  5.472... 1.92275971e-01  2.26324070e-02 -1.15882170e+00\n","     1.82729661e-01 -1.45694956e-01  1.81304570e-03]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 8.24896812e-01  6.52850270e-01 -4.06272173e-01  1.41356230e-01\n","     3.85968059e-01 -1.23175561e+00  5.472... 1.92275971e-01  2.26324070e-02 -1.15882170e+00\n","     1.82729661e-01 -1.45694956e-01  1.81304570e-03]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919d2c30>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_______________________ test_summation[cpu-shape0-None] ________________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = None, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.5133875]]])\n","_A         = array([[[0.5133875]]], dtype=float32)\n","axes       = None\n","device     = cpu()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:268: in summation\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        a          = needle.Tensor([[[0.5133875]]])\n","        axes       = None\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0.5133875]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c20>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[0.5133875]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c20>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919f3650>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919f3650>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c20>\n","a = NDArray([[[0.5133875]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[0.5133875]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919f3c20>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_______________________ test_summation[cuda-shape0-None] _______________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = None, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.86880785]]])\n","_A         = array([[[-0.25639066]]], dtype=float32)\n","axes       = None\n","device     = cuda()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:268: in summation\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        a          = needle.Tensor([[[0.86880785]]])\n","        axes       = None\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0.86880785]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919bbf80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[0.86880785]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919bbf80>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919b90d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919b90d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919bbf80>\n","a = NDArray([[[0.86880785]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[0.86880785]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919bbf80>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m________________________ test_summation[cuda-shape1-0] _________________________\u001b[0m\n","\n","shape = (5, 3), axes = 0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[-0.25639066  0.91954184 -0.5470395 ]\n"," [-1.6141661  -1.9141308   0.2576824 ]\n"," [ 0.87655175  0.2108965  -0.60347354]\n"," [ 0.40885907  1.5238507   1.8298346 ]\n"," [-0.45180562 -0.16761442  0.9780772 ]])\n","_A         = array([[ 0.5157999 , -0.44359782,  0.83550507],\n","       [-0.7541298 , -1.3492199 ,  0.28219447],\n","       [ 0.2446352 ,  ...5233 ],\n","       [ 0.54226357, -0.22101124,  0.429469  ],\n","       [-0.2687997 ,  0.08625331, -0.6898737 ]], dtype=float32)\n","axes       = 0\n","device     = cuda()\n","shape      = (5, 3)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919de3e0>, array([ 0.27976915, -1.563567  ,  1.4928181 ], dtype=float32), array([ 6.5344124 , -0.43987197,  2.9139524 ], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 3 / 3 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 6.2546434\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 2.554596\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([ 0.279769, -1.563567,  1.492818], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([ 6.534412, -0.439872,  2.913952], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919de3e0>, array([ 0.27976915, -1.563567  ,  1.4928181 ], dtype=float32), array([ 6.5344124 , -0.43987197,  2.9139524 ], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m________________________ test_summation[cuda-shape2-1] _________________________\u001b[0m\n","\n","shape = (8, 3, 2), axes = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[ 6.5344124e+00 -4.3987197e-01]\n","  [ 2.9139524e+00  5.0094321e-02]\n","  [-7.3577106e-01 -4.7775924e-01]]\n","\n","...5456827e-01]]\n","\n"," [[-2.8461671e-01 -1.2881047e+00]\n","  [-1.3250664e+00 -5.2173384e-03]\n","  [-5.5496162e-01 -1.0558120e+00]]])\n","_A         = array([[[ 2.1244366e-01, -1.0691541e+00],\n","        [-1.2854066e+00,  1.0652852e+00],\n","        [ 1.8692191e+00, -3.832957...,  5.0015819e-01],\n","        [-4.7015163e-01, -4.7376055e-01],\n","        [ 1.4888886e-02, -2.3518357e+00]]], dtype=float32)\n","axes       = 1\n","device     = cuda()\n","shape      = (8, 3, 2)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919ddd00>, array([[ 0.7962562 , -0.3871647 ],\n","       [ 1.751513  ...\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 16 / 16 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.569602\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 0.796256, -0.387165],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.751513,  0.018881],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-3.196006, -0.649324],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                  [0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                  [0., 0.],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919ddd00>, array([[ 0.7962562 , -0.3871647 ],\n","       [ 1.751513  ...\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.],\n","       [0., 0.]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m________________________ test_summation[cuda-shape3-2] _________________________\u001b[0m\n","\n","shape = (8, 3, 2), axes = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.sum(_A, axes), ndl.summation(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0....\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]])\n","_A         = array([[[ 0.939183  ,  1.4393458 ],\n","        [ 0.26261216,  0.9305667 ],\n","        [-1.3698933 ,  1.1468899 ]],\n","\n","       [...  [[ 1.6697443 , -1.404448  ],\n","        [-1.4388192 , -0.02291146],\n","        [ 0.04162927, -1.2710459 ]]], dtype=float32)\n","axes       = 2\n","device     = cuda()\n","shape      = (8, 3, 2)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:183: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919dc9a0>, array([[ 2.3785288 ,  1.1931789 , -0.22300339],\n","      ...     [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 24 / 24 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 2.5200722\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 2.378529,  1.193179, -0.223003],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.047955,  0.899812, -2.520072],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.331658,  0.779079,  0.571527],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[0., 0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                  [0., 0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                  [0., 0., 0.],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919dc9a0>, array([[ 2.3785288 ,  1.1931789 , -0.22300339],\n","      ...     [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________ test_summation_backward[cpu-shape0-None] ___________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = None, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.00565341]]])\n","_A         = array([[[0.00565341]]], dtype=float32)\n","axes       = None\n","device     = cpu()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n","    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","          ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0.00565341]]]),)\n","        eps        = 1e-05\n","        f          = <function summation at 0x137d74d549a0>\n","        kwargs     = {'axes': None}\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:268: in summation\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        a          = needle.Tensor([[[0.00565341]]])\n","        axes       = None\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0.00565341]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919ba3c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[0.00565341]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919ba3c0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919b9190>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919b9190>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919ba3c0>\n","a = NDArray([[[0.00565341]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[0.00565341]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919ba3c0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m__________________ test_summation_backward[cuda-shape0-None] ___________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = None, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[-0.14738184]]], dtype=float32)\n","axes       = None\n","device     = cuda()\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:16: in backward_check\n","    \u001b[0mout = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","          ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0.]]]),)\n","        eps        = 1e-05\n","        f          = <function summation at 0x137d74d549a0>\n","        kwargs     = {'axes': None}\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:268: in summation\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Summation(axes)(a)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        a          = needle.Tensor([[[0.]]])\n","        axes       = None\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[0.]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x137c919b9160>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[0.]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x137c919b9160>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919ba4b0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x137c919ba4b0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x137c919b9160>\n","a = NDArray([[[0.]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[0.]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x137c919b9160>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape1-0] ____________________\u001b[0m\n","\n","shape = (5, 3), axes = 0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[ 0.86880785 -0.70741963 -0.14541002]\n"," [ 2.4232674  -1.0968686   0.73122996]\n"," [-1.9972199  -0.1773353  -0.52167463]\n"," [ 0.23671676  0.09227231  0.64150995]\n"," [ 1.8896871  -1.2521687   0.0424663 ]])\n","_A         = array([[-0.24800439, -1.5495273 , -0.1203487 ],\n","       [-1.5250406 , -0.10352791,  1.8576714 ],\n","       [-0.23217203,  ...7145 ],\n","       [ 0.7708414 , -0.61790377,  0.14626923],\n","       [ 0.7474238 , -0.01467989,  0.7175262 ]], dtype=float32)\n","axes       = 0\n","device     = cuda()\n","shape      = (5, 3)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function summation at 0x137d74d549a0>\n","args = (needle.Tensor([[ 0.86880785 -0.70741963 -0.14541002]\n"," [ 2.4232674  -1.0968686   0.73122996]\n"," [-1.9972199  -0.1773353  -0.52167463]\n"," [ 0.23671676  0.09227231  0.64150995]\n"," [ 1.8896871  -1.2521687   0.0424663 ]]),)\n","kwargs = {'axes': 0}, eps = 1e-05\n","out = needle.Tensor([-0.24800439 -1.5495273  -0.1203487 ])\n","c = array([-1.1380909 ,  0.45943186,  0.86492562]), num_args = 1, i = 0, j = 14\n","f1 = np.float64(-1.1447898001722778), f2 = np.float64(-1.1447898001722778)\n","error = np.float64(3.5555653551622064)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m4.2e-1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(3.5555653551622064) < 0.42\u001b[0m\n","\n","args       = (needle.Tensor([[ 0.86880785 -0.70741963 -0.14541002]\n"," [ 2.4232674  -1.0968686   0.73122996]\n"," [-1.9972199  -0.1773353  -0.52167463]\n"," [ 0.23671676  0.09227231  0.64150995]\n"," [ 1.8896871  -1.2521687   0.0424663 ]]),)\n","backward_grad = (needle.Tensor([[ 0.7154689   0.12812123 -0.4501938 ]\n"," [-0.15363154 -0.7921522  -0.9739061 ]\n"," [ 1.2552259   1.1897821  -1.2562194 ]\n"," [-0.42185605  2.1962426   0.60885316]\n"," [ 0.31095642  0.4648094   0.2393958 ]]),)\n","c          = array([-1.1380909 ,  0.45943186,  0.86492562])\n","eps        = 1e-05\n","error      = np.float64(3.5555653551622064)\n","f          = <function summation at 0x137d74d549a0>\n","f1         = np.float64(-1.1447898001722778)\n","f2         = np.float64(-1.1447898001722778)\n","i          = 0\n","j          = 14\n","kwargs     = {'axes': 0}\n","num_args   = 1\n","numerical_grad = [array([[0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.],\n","       [0., 0., 0.]])]\n","out        = needle.Tensor([-0.24800439 -1.5495273  -0.1203487 ])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:33: AssertionError\n","\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape2-1] ____________________\u001b[0m\n","\n","shape = (8, 3, 2), axes = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[-0.24596018 -0.79486454]\n","  [ 0.58626044 -0.6478184 ]\n","  [ 1.3432454  -0.47048303]]\n","\n"," [[-0.992502    1....6]\n","  [-0.6842725   0.00833943]]\n","\n"," [[ 0.8831561   0.46175793]\n","  [-0.37671268  2.3502855 ]\n","  [ 1.3729278  -2.401563  ]]])\n","_A         = array([[[-0.5685719 , -0.13818243],\n","        [ 0.02258228,  0.7159469 ],\n","        [ 1.4280046 ,  0.04938466]],\n","\n","       [...  [[-0.16331597,  1.5508778 ],\n","        [-0.7730272 ,  0.2436368 ],\n","        [ 0.6098334 ,  0.57733613]]], dtype=float32)\n","axes       = 1\n","device     = cuda()\n","shape      = (8, 3, 2)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function summation at 0x137d74d549a0>\n","args = (needle.Tensor([[[-0.24596018 -0.79486454]\n","  [ 0.58626044 -0.6478184 ]\n","  [ 1.3432454  -0.47048303]]\n","\n"," [[-0.992502    1...\n","  [-0.6842725   0.00833943]]\n","\n"," [[ 0.8831561   0.46175793]\n","  [-0.37671268  2.3502855 ]\n","  [ 1.3729278  -2.401563  ]]]),)\n","kwargs = {'axes': 1}, eps = 1e-05\n","out = needle.Tensor([[ 1.1175141  -0.96641815]\n"," [-1.0879033   0.09836815]\n"," [ 0.20992573  0.05783941]\n"," [-0.36809775  0.14016663]\n"," [-1.4519229   0.02877928]\n"," [ 0.8902226   0.5894598 ]\n"," [-0.03376669  0.6756418 ]\n"," [-1.2712289  -1.448107  ]])\n","c = array([[ 0.24750309,  1.73478823],\n","       [ 1.97808523, -2.10939137],\n","       [ 0.0146967 ,  0.07289008],\n","       [ 0.33...-0.21903647],\n","       [ 0.09909772, -0.76121954],\n","       [ 0.73192888,  0.65431253],\n","       [ 1.69228165, -0.85450624]])\n","num_args = 1, i = 0, j = 47, f1 = np.float64(-2.223771212903516)\n","f2 = np.float64(-2.223771212903516), error = np.float64(6.742708198354612)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m4.2e-1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(6.742708198354612) < 0.42\u001b[0m\n","\n","args       = (needle.Tensor([[[-0.24596018 -0.79486454]\n","  [ 0.58626044 -0.6478184 ]\n","  [ 1.3432454  -0.47048303]]\n","\n"," [[-0.992502    1...\n","  [-0.6842725   0.00833943]]\n","\n"," [[ 0.8831561   0.46175793]\n","  [-0.37671268  2.3502855 ]\n","  [ 1.3729278  -2.401563  ]]]),)\n","backward_grad = (needle.Tensor([[[ 0.7717825   0.4729359 ]\n","  [ 0.6874767   1.4893833 ]\n","  [-0.03366714  2.0345984 ]]\n","\n"," [[-0.22133623  0...\n","  [ 0.7463318  -0.34591737]]\n","\n"," [[-0.10830966  1.0025363 ]\n","  [ 0.977909    0.5141969 ]\n","  [-0.6395886  -0.08031812]]]),)\n","c          = array([[ 0.24750309,  1.73478823],\n","       [ 1.97808523, -2.10939137],\n","       [ 0.0146967 ,  0.07289008],\n","       [ 0.33...-0.21903647],\n","       [ 0.09909772, -0.76121954],\n","       [ 0.73192888,  0.65431253],\n","       [ 1.69228165, -0.85450624]])\n","eps        = 1e-05\n","error      = np.float64(6.742708198354612)\n","f          = <function summation at 0x137d74d549a0>\n","f1         = np.float64(-2.223771212903516)\n","f2         = np.float64(-2.223771212903516)\n","i          = 0\n","j          = 47\n","kwargs     = {'axes': 1}\n","num_args   = 1\n","numerical_grad = [array([[[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","     ...0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]]])]\n","out        = needle.Tensor([[ 1.1175141  -0.96641815]\n"," [-1.0879033   0.09836815]\n"," [ 0.20992573  0.05783941]\n"," [-0.36809775  0.14016663]\n"," [-1.4519229   0.02877928]\n"," [ 0.8902226   0.5894598 ]\n"," [-0.03376669  0.6756418 ]\n"," [-1.2712289  -1.448107  ]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:33: AssertionError\n","\u001b[31m\u001b[1m____________________ test_summation_backward[cuda-shape3-2] ____________________\u001b[0m\n","\n","shape = (8, 3, 2), axes = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_summation_backward\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.summation, A, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[-0.04577059 -1.3129978 ]\n","  [-0.61812884 -1.7109851 ]\n","  [-0.22723491  2.0234768 ]]\n","\n"," [[-0.7743705   1.... ]\n","  [-1.5136725   0.22765186]]\n","\n"," [[-0.2193532  -0.90291476]\n","  [ 1.4750397  -0.3753992 ]\n","  [-0.35506016 -0.09440131]]])\n","_A         = array([[[-0.35047   , -0.64244956],\n","        [-0.3165104 , -0.55685735],\n","        [ 0.17516482, -0.59294105]],\n","\n","       [...  [[ 1.0682702 ,  0.87369984],\n","        [-1.4355507 ,  0.1892495 ],\n","        [-1.1416229 , -0.20496817]]], dtype=float32)\n","axes       = 2\n","device     = cuda()\n","shape      = (8, 3, 2)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:191: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function summation at 0x137d74d549a0>\n","args = (needle.Tensor([[[-0.04577059 -1.3129978 ]\n","  [-0.61812884 -1.7109851 ]\n","  [-0.22723491  2.0234768 ]]\n","\n"," [[-0.7743705   1...\n","  [-1.5136725   0.22765186]]\n","\n"," [[-0.2193532  -0.90291476]\n","  [ 1.4750397  -0.3753992 ]\n","  [-0.35506016 -0.09440131]]]),)\n","kwargs = {'axes': 2}, eps = 1e-05\n","out = needle.Tensor([[-0.35047    -0.64244956 -0.3165104 ]\n"," [-0.55685735  0.17516482 -0.59294105]\n"," [ 0.16579469 -1.4521552  ...\n"," [ 0.5937512   0.12590845 -0.9233482 ]\n"," [ 1.4924948   0.16379414  0.6218202 ]\n"," [-1.6380898   2.717582    0.16186446]])\n","c = array([[-0.13278429, -0.44021094,  0.8286478 ],\n","       [ 0.04491751,  1.97075877, -0.63587435],\n","       [-1.30128528, -...3061577,  0.74715982],\n","       [-1.95579318,  0.79195079, -0.46331693],\n","       [ 0.78626659, -1.07966071,  0.55414091]])\n","num_args = 1, i = 0, j = 47, f1 = np.float64(-1.3917866703253678)\n","f2 = np.float64(-1.3917866703253678), error = np.float64(7.508318165991799)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m4.2e-1\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(7.508318165991799) < 0.42\u001b[0m\n","\n","args       = (needle.Tensor([[[-0.04577059 -1.3129978 ]\n","  [-0.61812884 -1.7109851 ]\n","  [-0.22723491  2.0234768 ]]\n","\n"," [[-0.7743705   1...\n","  [-1.5136725   0.22765186]]\n","\n"," [[-0.2193532  -0.90291476]\n","  [ 1.4750397  -0.3753992 ]\n","  [-0.35506016 -0.09440131]]]),)\n","backward_grad = (needle.Tensor([[[ 0.36562374 -1.0697118 ]\n","  [-1.9621731   0.6403759 ]\n","  [ 0.12356576 -0.0983316 ]]\n","\n"," [[ 0.3857213  -3...\n","  [ 2.2296534   0.00701912]]\n","\n"," [[ 0.0350826  -0.24187759]\n","  [ 0.52747124  0.3501474 ]\n","  [-0.8503036   0.05526282]]]),)\n","c          = array([[-0.13278429, -0.44021094,  0.8286478 ],\n","       [ 0.04491751,  1.97075877, -0.63587435],\n","       [-1.30128528, -...3061577,  0.74715982],\n","       [-1.95579318,  0.79195079, -0.46331693],\n","       [ 0.78626659, -1.07966071,  0.55414091]])\n","eps        = 1e-05\n","error      = np.float64(7.508318165991799)\n","f          = <function summation at 0x137d74d549a0>\n","f1         = np.float64(-1.3917866703253678)\n","f2         = np.float64(-1.3917866703253678)\n","i          = 0\n","j          = 47\n","kwargs     = {'axes': 2}\n","num_args   = 1\n","numerical_grad = [array([[[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","     ...0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]],\n","\n","       [[0., 0.],\n","        [0., 0.],\n","        [0., 0.]]])]\n","out        = needle.Tensor([[-0.35047    -0.64244956 -0.3165104 ]\n"," [-0.55685735  0.17516482 -0.59294105]\n"," [ 0.16579469 -1.4521552  ...\n"," [ 0.5937512   0.12590845 -0.9233482 ]\n"," [ 1.4924948   0.16379414  0.6218202 ]\n"," [-1.6380898   2.717582    0.16186446]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:33: AssertionError\n","\u001b[31m\u001b[1m___________________ test_broadcast_to[cuda-shape0-shape_to0] ___________________\u001b[0m\n","\n","shape = (1, 1, 1), shape_to = (3, 3, 3), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[-0.08954055]]])\n","_A         = array([[[-0.08065071]]], dtype=float32)\n","device     = cuda()\n","shape      = (1, 1, 1)\n","shape_to   = (3, 3, 3)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:201: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919df740>, array([[[-0.08065071, -0.08065071, -0.08065071],\n","     ... ],\n","        [-0.2586081 , -0.42856216, -1.2840658 ],\n","        [ 1.0015417 ,  0.07410227,  1.3002404 ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 26 / 27 (96.3%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 2.238428\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 4.0574217\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.080651, -0.080651, -0.080651],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.080651, -0.080651, -0.080651],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.080651, -0.080651, -0.080651]],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.080651,  0.143418, -1.631331],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-1.903704, -0.122485, -0.566888],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 2.028167, -0.128491,  0.030507]],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919df740>, array([[[-0.08065071, -0.08065071, -0.08065071],\n","     ... ],\n","        [-0.2586081 , -0.42856216, -1.2840658 ],\n","        [ 1.0015417 ,  0.07410227,  1.3002404 ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m___________________ test_broadcast_to[cuda-shape1-shape_to1] ___________________\u001b[0m\n","\n","shape = (4, 1, 6), shape_to = (4, 3, 6), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, BROADCAST_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_broadcast_to\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.broadcast_to(_A, shape_to), ndl.broadcast_to(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[ 2.39531   , -2.5133643 , -0.5170488 ,  0.17539982,\n","          0.08753362,  0.45906746]],\n","\n","       [[ 0.0352267... ]],\n","\n","       [[ 1.3382661 ,  1.2198175 , -0.92588097,  0.43618894,\n","          0.40669367,  1.0810921 ]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 1, 6)\n","shape_to   = (4, 3, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:201: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919df920>, array([[[ 2.39531   , -2.5133643 , -0.5170488 ,  0.175...       [[0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 72 / 72 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 2.5133643\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[ 2.39531 , -2.513364, -0.517049,  0.1754  ,  0.087534,\u001b[0m\n","\u001b[1m\u001b[31mE                     0.459067],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 2.39531 , -2.513364, -0.517049,  0.1754  ,  0.087534,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0., 0., 0., 0., 0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0., 0., 0., 0., 0., 0.],\u001b[0m\n","\u001b[1m\u001b[31mE                   [0., 0., 0., 0., 0., 0.]],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919df920>, array([[[ 2.39531   , -2.5133643 , -0.5170488 ,  0.175...       [[0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.],\n","        [0., 0., 0., 0., 0., 0.]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_reshape[cuda-shape0-shape_to0] ______________________\u001b[0m\n","\n","shape = (1, 1, 1), shape_to = (1,), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[0.44332168]]], dtype=float32)\n","device     = cuda()\n","shape      = (1, 1, 1)\n","shape_to   = (1,)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:211: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84e482c0>, array([0.44332168], dtype=float32), array([0.], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.44332168\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([0.443322], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([0.], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84e482c0>, array([0.44332168], dtype=float32), array([0.], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_____________________ test_reshape[cuda-shape1-shape_to1] ______________________\u001b[0m\n","\n","shape = (4, 1, 6), shape_to = (6, 4, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape,shape_to\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, RESHAPE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_reshape\u001b[39;49;00m(shape, shape_to, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.reshape(_A, shape_to), ndl.reshape(A, shape_to).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[ 0.86880785 -0.70741963 -0.14541002  2.4232674  -1.0968686\n","    0.73122996]]\n","\n"," [[-1.9972199  -0.177335...18063016  1.9181329\n","    1.3844903 ]]\n","\n"," [[ 0.24238469 -0.1497871  -1.2934906  -0.41482008 -0.23039535\n","   -0.41272727]]])\n","_A         = array([[[ 0.7413115 ,  0.65963185, -0.11051767,  1.1735438 ,\n","          0.27489   , -1.1435506 ]],\n","\n","       [[-1.2952224...8]],\n","\n","       [[-0.6703581 ,  1.1364204 , -1.3640325 , -0.79039156,\n","          0.4250712 ,  1.8339677 ]]], dtype=float32)\n","device     = cuda()\n","shape      = (4, 1, 6)\n","shape_to   = (6, 4, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:211: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84e48680>, array([[[ 0.7413115 ],\n","        [ 0.65963185],\n","        ...7871 ]],\n","\n","       [[-1.2934906 ],\n","        [-0.41482008],\n","        [-0.23039535],\n","        [-0.41272727]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 24 / 24 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 3.3346152\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 19.862822\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[ 0.741311],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.659632],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.110518],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[ 0.868808],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.70742 ],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.14541 ],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84e48680>, array([[[ 0.7413115 ],\n","        [ 0.65963185],\n","        ...7871 ]],\n","\n","       [[-1.2934906 ],\n","        [-0.41482008],\n","        [-0.23039535],\n","        [-0.41272727]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape0] _______________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = (0, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.7413115]]])\n","_A         = array([[[0.92298496]]], dtype=float32)\n","axes       = (0, 1)\n","device     = cuda()\n","np_axes    = (0, 1)\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84e49260>, array([[[0.92298496]]], dtype=float32), array([[[0.7413115]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.18167347\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 0.24507035\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[0.922985]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.741311]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84e49260>, array([[[0.92298496]]], dtype=float32), array([[[0.7413115]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes0-shape1] _______________________\u001b[0m\n","\n","shape = (4, 5, 6), axes = (0, 1), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[ 0.7154689   0.12812123 -0.4501938  -0.15363154 -0.7921522\n","   -0.9739061 ]\n","  [ 1.2552259   1.1897821 ...1.1375118  -0.92311984\n","    0.6506672 ]\n","  [-0.9265111  -2.464132    0.80426985  1.7439381  -0.3475364\n","   -0.5985401 ]]])\n","_A         = array([[[-0.52088904,  0.03809374,  0.9778864 ,  0.07981496,\n","         -1.6533154 , -0.28263307],\n","        [-0.51364565,...9  ],\n","        [-0.3764521 ,  2.638757  ,  0.11819558,  0.84113216,\n","         -0.40530366, -0.4401012 ]]], dtype=float32)\n","axes       = (0, 1)\n","device     = cuda()\n","np_axes    = (0, 1)\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84e48c20>, array([[[-0.52088904,  0.03809374,  0.9778864 ,  0.079...  ],\n","        [-0.3764521 ,  2.638757  ,  0.11819558,  0.84113216,\n","         -0.40530366, -0.4401012 ]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 108 / 120 (90%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.222373\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 60.677162\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.520889,  0.038094,  0.977886,  0.079815, -1.653315,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.282633],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 2.28944 ,  1.452158, -1.427183,  0.227228, -1.534433,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.520889,  0.038094,  0.977886,  0.079815, -1.653315,\u001b[0m\n","\u001b[1m\u001b[31mE                    -0.282633],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-0.513646,  0.0839  , -0.575817, -0.055655, -0.728204,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84e48c20>, array([[[-0.52088904,  0.03809374,  0.9778864 ,  0.079...  ],\n","        [-0.3764521 ,  2.638757  ,  0.11819558,  0.84113216,\n","         -0.40530366, -0.4401012 ]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape0] _______________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = (0, 2), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.24750309]]])\n","_A         = array([[[-0.16526793]]], dtype=float32)\n","axes       = (0, 2)\n","device     = cuda()\n","np_axes    = (0, 2)\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84e49580>, array([[[-0.16526793]]], dtype=float32), array([[[0.24750309]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.41277102\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 1.667741\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.165268]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.247503]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84e49580>, array([[[-0.16526793]]], dtype=float32), array([[[0.24750309]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m______________________ test_transpose[cuda-axes1-shape1] _______________________\u001b[0m\n","\n","shape = (4, 5, 6), axes = (0, 2), device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[-0.31185058,  0.26994687,  2.0156775 ,  0.29399768,\n","          1.3641229 ,  1.522246  ],\n","        [ 0.7832116 ,...37 ],\n","        [ 0.5095096 , -0.13911654, -0.48825276, -1.4051776 ,\n","          0.22937068, -0.49968913]]], dtype=float32)\n","axes       = (0, 2)\n","device     = cuda()\n","np_axes    = (0, 2)\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d10540>, array([[[-0.31185058,  0.828859  , -0.41570628,  0.071...0337 ,  0.5095096 , -0.13911654],\n","        [-0.48825276, -1.4051776 ,  0.22937068, -0.49968913]]],\n","      dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 118 / 120 (98.3%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.1370173\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 225.87653\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-0.311851,  0.828859, -0.415706,  0.071792],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.783212, -0.686224, -0.144246,  0.865608],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 0.460025,  0.108708,  1.198089, -0.238654],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-0.311851,  0.269947,  2.015677,  0.293998],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 1.364123,  1.522246,  0.783212, -0.152081],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 1.69321 ,  1.256303,  0.14213 ,  1.739326],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d10540>, array([[[-0.31185058,  0.828859  , -0.41570628,  0.071...0337 ,  0.5095096 , -0.13911654],\n","        [-0.48825276, -1.4051776 ,  0.22937068, -0.49968913]]],\n","      dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape0] _______________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = None, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.]]])\n","_A         = array([[[0.042238]]], dtype=float32)\n","axes       = None\n","device     = cuda()\n","np_axes    = (1, 2)\n","shape      = (1, 1, 1)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d11b20>, array([[[0.042238]]], dtype=float32), array([[[0.]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.042238\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: inf\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[0.042238]]], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[0.]]], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d11b20>, array([[[0.042238]]], dtype=float32), array([[[0.]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_______________________ test_transpose[cuda-None-shape1] _______________________\u001b[0m\n","\n","shape = (4, 5, 6), axes = None, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_SHAPES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, TRANSPOSE_AXES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_transpose\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = (_A.ndim - \u001b[94m2\u001b[39;49;00m, _A.ndim - \u001b[94m1\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            np_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(np.swapaxes(_A, np_axes[\u001b[94m0\u001b[39;49;00m], np_axes[\u001b[94m1\u001b[39;49;00m]), ndl.transpose(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0... 0.]]\n","\n"," [[0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]\n","  [0. 0. 0. 0. 0. 0.]]])\n","_A         = array([[[-1.18659759e+00, -4.49779689e-01, -4.32244986e-01,\n","          6.26861870e-01, -1.80563629e+00,  6.71942651e-01...,  7.66648531e-01, -1.49845636e+00,\n","          1.16017960e-01, -4.71417069e-01,  3.20715189e-01]]],\n","      dtype=float32)\n","axes       = None\n","device     = cuda()\n","np_axes    = (1, 2)\n","shape      = (4, 5, 6)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:226: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d12520>, array([[[-1.18659759e+00, -8.02810416e-02,  2.37964749...      [ 7.66648531e-01, -1.49845636e+00,  1.16017960e-01,\n","         -4.71417069e-01,  3.20715189e-01]]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 112 / 120 (93.3%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 4.213198\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 291.0464\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[[-1.186598e+00, -8.028104e-02,  2.379647e-01, -1.803691e+00,\u001b[0m\n","\u001b[1m\u001b[31mE                    -1.857446e-01],\u001b[0m\n","\u001b[1m\u001b[31mE                   [-4.497797e-01,  1.661905e+00,  3.032223e-01, -2.335102e+00,...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[[-1.186598e+00, -4.497797e-01, -4.322450e-01,  6.268619e-01,\u001b[0m\n","\u001b[1m\u001b[31mE                    -1.805636e+00],\u001b[0m\n","\u001b[1m\u001b[31mE                   [ 6.719427e-01, -8.028104e-02,  1.661905e+00, -9.070182e-01,...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d12520>, array([[[-1.18659759e+00, -8.02810416e-02,  2.37964749...      [ 7.66648531e-01, -1.49845636e+00,  1.16017960e-01,\n","         -4.71417069e-01,  3.20715189e-01]]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m_______________________ test_logsumexp[cuda-shape0-None] _______________________\u001b[0m\n","\n","shape = (1, 1, 1), axes = None, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0.7413115]]])\n","A_t        = tensor([[[0.5326]]])\n","_A         = array([[[0.53261983]]], dtype=float32)\n","axes       = None\n","device     = cuda()\n","shape      = (1, 1, 1)\n","t_axes     = (0, 1, 2)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c9196be20>, array(0.53261983, dtype=float32), array(-0.16526793, dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 1 / 1 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 0.6978878\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 4.222766\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array(0.53262, dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array(-0.165268, dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c9196be20>, array(0.53261983, dtype=float32), array(-0.16526793, dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape1-0] _________________________\u001b[0m\n","\n","shape = (5, 3), axes = 0, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]\n"," [0. 0. 0.]])\n","A_t        = tensor([[ 3.0259e-02, -4.5872e-01,  2.1940e-01],\n","        [ 2.1156e-01,  1.1877e+00,  8.6819e-01],\n","        [ 8.1104e-04...7e-01,  6.8488e-01],\n","        [ 1.0915e+00, -2.7299e+00, -8.9583e-01],\n","        [-7.6962e-01,  1.6294e-01,  8.8078e-01]])\n","_A         = array([[ 3.0258557e-02, -4.5871726e-01,  2.1940093e-01],\n","       [ 2.1155696e-01,  1.1876984e+00,  8.6819392e-01],\n","    ....0915481e+00, -2.7298880e+00, -8.9583379e-01],\n","       [-7.6962298e-01,  1.6293789e-01,  8.8077790e-01]], dtype=float32)\n","axes       = 0\n","device     = cuda()\n","shape      = (5, 3)\n","t_axes     = 0\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919dfe20>, array([1.90348  , 1.8574247, 2.1320958], dtype=float32), array([-0.3843144 , -0.75144523, -1.770937  ], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 3 / 3 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 3.9030328\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 5.9529243\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([1.90348 , 1.857425, 2.132096], dtype=float32)\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([-0.384314, -0.751445, -1.770937], dtype=float32)\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919dfe20>, array([1.90348  , 1.8574247, 2.1320958], dtype=float32), array([-0.3843144 , -0.75144523, -1.770937  ], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape2-1] _________________________\u001b[0m\n","\n","shape = (8, 3, 2), axes = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0....\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]])\n","A_t        = tensor([[[-0.3271, -0.8377],\n","         [-0.9397,  1.6033],\n","         [ 1.4388, -0.8560]],\n","\n","        [[ 0.4211, -1.9551],\n","...         [ 1.7539, -0.0813]],\n","\n","        [[-0.0692,  0.8882],\n","         [-0.3923,  0.6581],\n","         [-1.7375, -0.1404]]])\n","_A         = array([[[-0.32713804, -0.8376945 ],\n","        [-0.9397055 ,  1.6032794 ],\n","        [ 1.4387522 , -0.85600376]],\n","\n","       [...  [[-0.06918286,  0.88816154],\n","        [-0.3923111 ,  0.65810174],\n","        [-1.7375134 , -0.14040655]]], dtype=float32)\n","axes       = 1\n","device     = cuda()\n","shape      = (8, 3, 2)\n","t_axes     = 1\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c919dff60>, array([[1.6728182 , 1.7624792 ],\n","       [2.1192634 , 2...     [ 0.53297   , -0.03951903],\n","       [ 1.1929668 ,  0.671377  ],\n","       [ 1.2281172 , -1.258055  ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 16 / 16 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 3.9279754\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 9.127601\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[1.672818, 1.762479],\u001b[0m\n","\u001b[1m\u001b[31mE                  [2.119263, 2.256093],\u001b[0m\n","\u001b[1m\u001b[31mE                  [2.411325, 1.338994],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[-0.485757, -0.341735],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.428068, -1.422844],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-1.516651, -1.647232],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c919dff60>, array([[1.6728182 , 1.7624792 ],\n","       [2.1192634 , 2...     [ 0.53297   , -0.03951903],\n","       [ 1.1929668 ,  0.671377  ],\n","       [ 1.2281172 , -1.258055  ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[31m\u001b[1m________________________ test_logsumexp[cuda-shape3-2] _________________________\u001b[0m\n","\n","shape = (8, 3, 2), axes = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mshape, axes\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, SUMMATION_PARAMETERS)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES, ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mcpu\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mcuda\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_logsumexp\u001b[39;49;00m(shape, axes, device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(*shape).astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(nd.array(_A), device=device)\u001b[90m\u001b[39;49;00m\n","        A_t = torch.Tensor(_A)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m axes \u001b[95mis\u001b[39;49;00m \u001b[94mNone\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = \u001b[96mtuple\u001b[39;49;00m(\u001b[96mlist\u001b[39;49;00m(\u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(shape))))\u001b[90m\u001b[39;49;00m\n","        \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","            t_axes = axes\u001b[90m\u001b[39;49;00m\n",">       np.testing.assert_allclose(torch.logsumexp(A_t, dim=t_axes).numpy(), ndl.logsumexp(A, axes=axes).numpy(), atol=\u001b[94m1e-5\u001b[39;49;00m, rtol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0....\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]\n","\n"," [[0. 0.]\n","  [0. 0.]\n","  [0. 0.]]])\n","A_t        = tensor([[[-0.0906,  0.9108],\n","         [ 0.4265,  0.5579],\n","         [ 0.3293,  0.4249]],\n","\n","        [[-0.4909, -0.1292],\n","...         [-0.5577, -0.1780]],\n","\n","        [[ 1.1120,  0.9884],\n","         [-0.0534,  0.0805],\n","         [ 0.5399,  0.2518]]])\n","_A         = array([[[-0.09061285,  0.91081566],\n","        [ 0.42649978,  0.55792385],\n","        [ 0.32932186,  0.4249462 ]],\n","\n","       [...  [[ 1.1119847 ,  0.9884341 ],\n","        [-0.05336624,  0.08052945],\n","        [ 0.53987354,  0.25176743]]], dtype=float32)\n","axes       = 2\n","device     = cuda()\n","shape      = (8, 3, 2)\n","t_axes     = 2\n","\n","\u001b[1m\u001b[31mtests/hw4/test_nd_backend.py\u001b[0m:239: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","args = (<function assert_allclose.<locals>.compare at 0x137c84d12ac0>, array([[ 1.2236934 ,  1.1875165 ,  1.0714238 ],\n","      ...1738],\n","       [-0.6703581 ,  1.1364204 , -1.3640325 ],\n","       [-0.79039156,  0.4250712 ,  1.8339677 ]], dtype=float32))\n","kwds = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","\n","    \u001b[0m\u001b[37m@wraps\u001b[39;49;00m(func)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92minner\u001b[39;49;00m(*args, **kwds):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mwith\u001b[39;49;00m \u001b[96mself\u001b[39;49;00m._recreate_cm():\u001b[90m\u001b[39;49;00m\n",">           \u001b[94mreturn\u001b[39;49;00m func(*args, **kwds)\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE           AssertionError: \u001b[0m\n","\u001b[1m\u001b[31mE           Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n","\u001b[1m\u001b[31mE           \u001b[0m\n","\u001b[1m\u001b[31mE           Mismatched elements: 24 / 24 (100%)\u001b[0m\n","\u001b[1m\u001b[31mE           Max absolute difference among violations: 3.1405475\u001b[0m\n","\u001b[1m\u001b[31mE           Max relative difference among violations: 10.6945915\u001b[0m\n","\u001b[1m\u001b[31mE            ACTUAL: array([[ 1.223693,  1.187516,  1.071424],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.399349,  0.497715,  1.737064],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 0.958101, -0.641827,  0.88784 ],...\u001b[0m\n","\u001b[1m\u001b[31mE            DESIRED: array([[ 0.741311,  0.659632, -0.110518],\u001b[0m\n","\u001b[1m\u001b[31mE                  [ 1.173544,  0.27489 , -1.143551],\u001b[0m\n","\u001b[1m\u001b[31mE                  [-1.295222, -0.833974, -0.467622],...\u001b[0m\n","\n","args       = (<function assert_allclose.<locals>.compare at 0x137c84d12ac0>, array([[ 1.2236934 ,  1.1875165 ,  1.0714238 ],\n","      ...1738],\n","       [-0.6703581 ,  1.1364204 , -1.3640325 ],\n","       [-0.79039156,  0.4250712 ,  1.8339677 ]], dtype=float32))\n","func       = <function assert_array_compare at 0x137c91eeb060>\n","kwds       = {'equal_nan': True, 'err_msg': '', 'header': 'Not equal to tolerance rtol=1e-05, atol=1e-05', 'strict': False, ...}\n","self       = <contextlib._GeneratorContextManager object at 0x137c91959e50>\n","\n","\u001b[1m\u001b[31m/usr/lib/python3.12/contextlib.py\u001b[0m:81: AssertionError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cuda-shape0-divide]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cuda-shape0-subtract]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cuda-shape1-divide]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_ewise_fn[cuda-shape1-subtract]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape0-divide]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape0-subtract]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape1-divide]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_scalar_fn[cuda-shape1-subtract]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-16-16-16]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-8-8-8]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-1-2-3]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-3-4-5]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-5-4-3]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-16-16-32]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-64-64-64]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-72-72-72]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-72-73-74]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-74-73-72]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_matmul[cuda-128-128-128]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cuda-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_power[cuda-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_log[cuda-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_log[cuda-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_exp[cuda-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_exp[cuda-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_relu[cuda-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh[cuda-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_tanh_backward[cuda-shape1]\u001b[0m - assert np.float64(10.769484291355438) < 0.42\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape1-0-2]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack[cuda-shape2-2-5]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape0-0-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape1-0-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cpu-shape2-2-5]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape0-0-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape1-0-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_stack_backward[cuda-shape2-2-5]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cpu-shape0-None]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape0-None]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape1-0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape2-1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation[cuda-shape3-2]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cpu-shape0-None]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape0-None]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape1-0]\u001b[0m - assert np.float64(3.5555653551622064) < 0.42\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape2-1]\u001b[0m - assert np.float64(6.742708198354612) < 0.42\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_summation_backward[cuda-shape3-2]\u001b[0m - assert np.float64(7.508318165991799) < 0.42\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_broadcast_to[cuda-shape0-shape_to0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_broadcast_to[cuda-shape1-shape_to1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_reshape[cuda-shape0-shape_to0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_reshape[cuda-shape1-shape_to1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes0-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-axes1-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_transpose[cuda-None-shape1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape0-None]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape1-0]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape2-1]\u001b[0m - AssertionError: \n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1mtest_logsumexp[cuda-shape3-2]\u001b[0m - AssertionError: \n","\u001b[31m=============== \u001b[31m\u001b[1m61 failed\u001b[0m, \u001b[32m57 passed\u001b[0m, \u001b[33m1685 deselected\u001b[0m\u001b[31m in 16.48s\u001b[0m\u001b[31m ================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"nd_backend\""]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXZWQ4XSucAP","executionInfo":{"status":"ok","timestamp":1763015311802,"user_tz":300,"elapsed":9687,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"6680178e-612a-497f-f958-3540c9a9314b"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\n","\n","tests/hw4/test_nd_backend.py \n","Submitting new_nd_backend...\n","Grader test 1 passed\n","Grader test 2 passed\n","Grader test 3 failed\n","Grader test 4 failed\n","Grader test 5 passed\n","Grader test 6 passed\n","Grader test 7 failed\n","Grader test 8 failed\n","Grader test 9 passed\n","Grader test 10 passed\n","Grader test 11 failed\n","Grader test 12 failed\n","Grader test 13 passed\n","Grader test 14 failed\n","Grader test 15 passed\n","Grader test 16 failed\n","Grader test 17 passed\n","Grader test 18 failed\n","Grader test 19 passed\n","Grader test 20 passed\n","Grader test 21 failed\n","Grader test 22 failed\n","Grader test 23 passed\n","Grader test 24 passed\n","Grader test 25 failed\n","Grader test 26 failed\n","Grader test 27 passed\n","Grader test 28 passed\n","Grader test 29 passed\n","Grader test 30 passed\n","Grader test 31 failed\n","Grader test 32 failed\n","Grader test 33 failed\n","Grader test 34 failed\n","Grader test 35 passed\n","Grader test 36 passed\n","Grader test 37 failed\n","Grader test 38 failed\n","Grader test 39 passed\n","Grader test 40 failed\n","Grader test 41 passed\n","Grader test 42 passed\n","Grader test 43 passed\n","Grader test 44 failed\n","Grader test 45 failed\n","Grader test 46 passed\n","Grader test 47 passed\n","Grader test 48 passed\n","Grader test 49 passed\n","Grader test 50 passed\n","Grader test 51 failed\n","Grader test 52 failed\n","Grader test 53 failed\n","Grader test 54 failed\n","\u001b[31mF\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m____________________________ submit_new_nd_backend _____________________________\u001b[0m\n","\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_nd_backend.py::\u001b[1msubmit_new_nd_backend\u001b[0m - Failed\n","\u001b[31m================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m9 deselected\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 7.88s\u001b[0m\u001b[31m ==================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"new_nd_backend\""]},{"cell_type":"markdown","metadata":{"id":"Y-rb_yOLucAP"},"source":["## Part 2: CIFAR-10 dataset [10 points]\n","\n","Next, you will write support for the [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) image classification dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50k training images and 10k test images.\n","\n","Start by implementing the `__init__` function in the `CIFAR10Dataset` class in `python/needle/data/datasets/cifar10_dataset.py`. You can read in the link above how to properly read the CIFAR-10 dataset files you downloaded at the beginning of the homework. Also fill in `__getitem__` and `__len__`. Note that the return shape of the data from `__getitem__` should be in order (3, 32, 32).\n","\n","Copy `python/needle/data/data_transforms.py` and `python/needle/data/data_basic.py` from previous homeworks."]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rzPlH_hKucAP","executionInfo":{"status":"ok","timestamp":1763015330480,"user_tz":300,"elapsed":18678,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"856972a4-4441-4a18-f884-d256c2cfd505"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1793 deselected / 10 selected                           \u001b[0m\n","\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[True] \u001b[32mPASSED\u001b[0m\u001b[32m      [ 10%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_dataset[False] \u001b[32mPASSED\u001b[0m\u001b[32m     [ 20%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cpu-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-True-15] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n","tests/hw4/test_cifar_ptb_data.py::test_cifar10_loader[cuda-False-15] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n","\n","\u001b[32m===================== \u001b[32m\u001b[1m10 passed\u001b[0m, \u001b[33m1793 deselected\u001b[0m\u001b[32m in 16.65s\u001b[0m\u001b[32m =====================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"test_cifar10\""]},{"cell_type":"code","execution_count":24,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hLcG1FL5ucAP","executionInfo":{"status":"ok","timestamp":1763015337526,"user_tz":300,"elapsed":7046,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"118d5bf7-4783-4cb1-e090-01623492559b"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\n","\n","tests/hw4/test_cifar_ptb_data.py \n","Submitting cifar10...\n","Grader test 1 passed\n","Grader test 2 passed\n","Grader test 3 passed\n","Grader test 4 passed\n","Grader test 5 passed\n","Grader test 6 passed\n","Grader test 7 passed\n","Grader test 8 passed\n","Grader test 9 passed\n","Grader test 10 passed\n","Grader test 11 passed\n","Grader test 12 passed\n","Grader test 13 passed\n","Grader test 14 passed\n","Grader test 15 passed\n","Grader test 16 passed\n","Grader test 17 passed\n","Grader test 18 passed\n","\u001b[32m.\u001b[0m\n","\n","\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 4.96s\u001b[0m\u001b[32m ========================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"cifar10\""]},{"cell_type":"markdown","metadata":{"tags":[],"id":"M_yrixkWucAP"},"source":["## Part 3: Convolutional neural network [40 points]\n","\n","Here's an outline of what you will do in this task.\n","\n","In `python/needle/backend_ndarray/ndarray.py`, implement:\n","- `flip`\n","- `pad`\n","\n","In `python/needle/ops/ops_mathematic.py`, implement (forward and backward):\n","- `Flip`\n","- `Dilate`\n","- `UnDilate`\n","- `Conv`\n","\n","In `python/needle/nn/nn_conv.py`, implement:\n","- `Conv`\n","\n","In `apps/models.py`, fill in the `ResNet9` class.  \n","\n","In `apps/simple_ml.py`, fill in:\n","- `epoch_general_cifar10`,\n","- `train_cifar10`\n","- `evaluate_cifar10`\n","\n","We have provided a `BatchNorm2d` implementation in `python/needle/nn/nn_basic.py` for you as a wrapper around your previous `BatchNorm1d` implementation.\n","\n","**Note**: Remember to copy the solution of `nn_basic.py` from previous homework, make sure to not overwrite the `BatchNorm2d` module."]},{"cell_type":"markdown","metadata":{"id":"HNR7svEIucAP"},"source":["### Padding ndarrays\n","\n","Convolution as typically implemented in deep learning libraries cuts down the size of inputs;\n","e.g., a (1, 32, 32, 3) image convolved with a 3x3 filter would give a (1, 30, 30, c) output.\n","A way around this is to pad the input ndarray before performing convolution, e.g., pad with zeros to get a (1, 34, 34, 3) ndarray so that the result is (1, 32, 32, 3).\n","\n","Padding is also required for the backward pass of convolution.\n","\n","You should implement `pad` in `ndarray.py` to closely reflect the behavior of `np.pad`.\n","That is, `pad` should take a tuple of 2-tuples with length equal to the number of dimensions of the array,\n","where each element in the 2-tuple corresponds to \"left padding\" and \"right padding\", respectively.\n","\n","For example, if `A` is a (10, 32, 32, 8) ndarray (think NHWC), then `A.pad( (0, 0), (2, 2), (2, 2), (0, 0) )` would be a (10, 36, 36, 8) ndarray where the \"spatial\" dimension has been padded by two zeros on all sides."]},{"cell_type":"code","execution_count":25,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JLBqmjkucAP","executionInfo":{"status":"ok","timestamp":1763015341355,"user_tz":300,"elapsed":3827,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"0305c18b-85d8-41c4-dbea-5b2f748ad3b1"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\n","\n","tests/hw4/test_conv.py::test_pad_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_pad_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n","\n","\u001b[32m====================== \u001b[32m\u001b[1m2 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[32m in 2.09s\u001b[0m\u001b[32m ======================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"pad_forward\""]},{"cell_type":"markdown","metadata":{"id":"njY2K1c7ucAQ"},"source":["-------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"mKJyb1jtucAQ"},"source":["### Flipping ndarrays & FlipOp"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"MqUNx4NsucAQ","executionInfo":{"status":"ok","timestamp":1763015341358,"user_tz":300,"elapsed":2,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["import numpy as np\n","import ctypes"]},{"cell_type":"markdown","metadata":{"id":"-8BtnMCVucAQ"},"source":["Some utility code for a demonstration below which you can probably ignore. It might be instructive to check out the `offset` function."]},{"cell_type":"code","execution_count":27,"metadata":{"id":"52ETGP7kucAQ","executionInfo":{"status":"ok","timestamp":1763015341372,"user_tz":300,"elapsed":6,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["# reads off the underlying data array in order (i.e., offset 0, offset 1, ..., offset n)\n","# i.e., ignoring strides\n","def raw_data(X):\n","    X = np.array(X) # copy, thus compact X\n","    return np.frombuffer(ctypes.string_at(X.ctypes.data, X.nbytes), dtype=X.dtype, count=X.size)\n","\n","# Xold and Xnew should reference the same underlying data\n","def offset(Xold, Xnew):\n","    assert Xold.itemsize == Xnew.itemsize\n","    # compare addresses to the beginning of the arrays\n","    return (Xnew.ctypes.data - Xold.ctypes.data)//Xnew.itemsize\n","\n","def strides(X):\n","    return ', '.join([str(x//X.itemsize) for x in X.strides])\n","\n","def format_array(X, shape):\n","    assert len(shape) == 3, \"I only made this formatting work for ndims = 3\"\n","    def chunks(l, n):\n","        n = max(1, n)\n","        return (l[i:i+n] for i in range(0, len(l), n))\n","    a = [str(x) if x >= 10 else ' ' + str(x) for x in X]\n","    a = ['(' + ' '.join(y) + ')' for y in [x for x in chunks(a, shape[-1])]]\n","    a = ['|' + ' '.join(y) + '|' for y in [x for x in chunks(a, shape[-2])]]\n","    return '  '.join(a)\n","\n","def inspect_array(X, *, is_a_copy_of):\n","    # compacts X, then reads it off in order\n","    print('Data: %s' % format_array(raw_data(X), X.shape))\n","    # compares address of X to copy_of, thus finding X's offset\n","    print('Offset: %s' % offset(is_a_copy_of, X))\n","    print('Strides: %s' % strides(X))"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"XEkn4jXSucAQ"},"source":["In order to implement the backwards pass of 2D convolution, we will (probably) need a function which _flips_\n","axes of ndarrays. We say \"probably\" because you could probably cleverly implement your convolution forward\n","function to avoid this. However, we think it is easiest to think about this if you have the ability to \"flip\" the kernel along its vertical and horizontal dimensions.\n","\n","We will try to build up your intuition for the \"flip\" operation below in order to help you figure out how to implement it in `ndarray.py`. To do that, we explore numpy's `np.flip` function below. One thing to note is that\n","`flip` is typically implemented by using negative strides and changing the _offset_ of the underlying array.\n","\n","For example, flipping an array on _all_ of its axes is equivalent to reversing the array. In this case, you can imagine that we would want all the strides to be negative, and the offset to be the length of the array (to start at the end of the array and \"stride\" backwards).\n","\n","Since we did not explicitly support negative strides in our implementation for the last homework, we will merely call `NDArray.make` with them to make our \"flipped\" array and then immediately call `.compact()`. Other than changing unsigned ints to signed ints in a few places, we suspect your existing `compact` function should not have to change at all to accomodate negative strides. In the .cc and .cu files we distributed, we have already changed the function signatures to reflect this.\n","\n","Alternatively, you could simply implement `flip` in the CPU backend by copying memory, which you _may_ find more intuitive. We suggest following our mini tutorial below to keep your implementation Python-focused, since we believe it is involves approximately the same amount of effort to implement it slightly more naively in C."]},{"cell_type":"markdown","metadata":{"id":"BlTNoPFBucAQ"},"source":["Use this array as reference for the other examples:"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rDRAUWSwucAQ","executionInfo":{"status":"ok","timestamp":1763015341386,"user_tz":300,"elapsed":13,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"7ca90b04-af76-41a2-908a-ffdc9db183e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: |( 1  2  3  4) ( 5  6  7  8)|  |( 9 10 11 12) (13 14 15 16)|  |(17 18 19 20) (21 22 23 24)|\n","Offset: 0\n","Strides: 8, 4, 1\n"]}],"source":["A = np.arange(1, 25).reshape(3, 2, 4)\n","inspect_array(A, is_a_copy_of=A)"]},{"cell_type":"markdown","metadata":{"id":"pl6w8uhducAQ"},"source":["We have put brackets around each axis of the array. Notice that for this array, the offset is 0 and the strides are all positive."]},{"cell_type":"markdown","metadata":{"id":"JJpNooGsucAQ"},"source":["----------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"lLTW1Z_6ucAQ"},"source":["See what happens when you flip the array along the last axis below.\n","Note that the `inspect_array` function compacts the array after flipping it so you can see the\n","\"logical\" order of the data, and the offset is calculated by comparing the address of the **non**-compacted\n","flipped array with that of `is_copy_of`, i.e., the array `A` we looked at above.\n","\n","That is, we are looking at how numpy calculates the strides and offset for flipped arrays in order\n","to copy this behavior in our own implementation."]},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5tox-qd3ucAQ","executionInfo":{"status":"ok","timestamp":1763015341388,"user_tz":300,"elapsed":3,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"4b0178b4-efea-419d-afa0-77d5a820d557"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: |( 4  3  2  1) ( 8  7  6  5)|  |(12 11 10  9) (16 15 14 13)|  |(20 19 18 17) (24 23 22 21)|\n","Offset: 3\n","Strides: 8, 4, -1\n"]}],"source":["inspect_array(np.flip(A, (2,)), is_a_copy_of=A)"]},{"cell_type":"markdown","metadata":{"id":"ZY5yItIJucAQ"},"source":["So flipping the last axis reverses the order of the elements within each 4-dimensional \"cell\", as you can see above. The stride corresponding to the axis we flipped has been negated. And the offset is 3 -- this makes sense, e.g., because we want the new \"first\" element of the array to be 4, which was at index 3 in `A`."]},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ejDdZha7ucAQ","executionInfo":{"status":"ok","timestamp":1763015341403,"user_tz":300,"elapsed":15,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"569265f9-bd1e-4827-cac3-ab7e504263e6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: |( 5  6  7  8) ( 1  2  3  4)|  |(13 14 15 16) ( 9 10 11 12)|  |(21 22 23 24) (17 18 19 20)|\n","Offset: 4\n","Strides: 8, -4, 1\n"]}],"source":["inspect_array(np.flip(A, (1,)), is_a_copy_of=A)"]},{"cell_type":"markdown","metadata":{"id":"bVEsFg5UucAQ"},"source":["Again for the middle axis: we negate the middle stride, and the offset is 4, which seems reasonable since we now want the first element to be 5, which was at index 4 in the original array `A`."]},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tz2k-i-0ucAQ","executionInfo":{"status":"ok","timestamp":1763015341408,"user_tz":300,"elapsed":5,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"ace43d7b-bcf1-4c36-f5d4-848ecc6151cc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: |(17 18 19 20) (21 22 23 24)|  |( 9 10 11 12) (13 14 15 16)|  |( 1  2  3  4) ( 5  6  7  8)|\n","Offset: 16\n","Strides: -8, 4, 1\n"]}],"source":["inspect_array(np.flip(A, (0,)), is_a_copy_of=A)"]},{"cell_type":"markdown","metadata":{"id":"PrEFY6m_ucAQ"},"source":["Try to infer the more general algorithm for computing the offset given the axis to flip."]},{"cell_type":"markdown","metadata":{"id":"idM7Pp7UucAQ"},"source":["----------------------------------------------------------------------------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"bHhuNOk7ucAQ"},"source":["Observe what happens when we flip _all_ axes."]},{"cell_type":"code","execution_count":32,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dyXeXBWUucAQ","executionInfo":{"status":"ok","timestamp":1763015341418,"user_tz":300,"elapsed":9,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"95191982-bd63-49bd-fab6-544ca8dca74d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: |(24 23 22 21) (20 19 18 17)|  |(16 15 14 13) (12 11 10  9)|  |( 8  7  6  5) ( 4  3  2  1)|\n","Offset: 23\n","Strides: -8, -4, -1\n"]}],"source":["inspect_array(np.flip(A, (0, 1, 2)), is_a_copy_of=A)"]},{"cell_type":"markdown","metadata":{"id":"06iYYjMjucAQ"},"source":["As mentioned earlier, the offset is then sufficient to point to the last element of the array, and this is just the \"reverse order\" version of `A`."]},{"cell_type":"markdown","metadata":{"id":"G2JNGol9ucAQ"},"source":["When we flip just axes 1 and 0..."]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_ZWzA05ucAT","executionInfo":{"status":"ok","timestamp":1763015341418,"user_tz":300,"elapsed":4,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"e0e556ed-b4e4-4d24-b0e2-6203a11a2b7e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Data: |(21 22 23 24) (17 18 19 20)|  |(13 14 15 16) ( 9 10 11 12)|  |( 5  6  7  8) ( 1  2  3  4)|\n","Offset: 20\n","Strides: -8, -4, 1\n"]}],"source":["inspect_array(np.flip(A, (0, 1)), is_a_copy_of=A)"]},{"cell_type":"markdown","metadata":{"id":"DOEtEKSrucAT"},"source":["The offset is 20. Looking back on our previous offset computations, do you notice something?"]},{"cell_type":"markdown","metadata":{"id":"H5KGn2cqucAT"},"source":["-------------------\n","\n","With this exploration of numpy's ndarray flipping functionality, which uses negative strides and a custom offset,\n","try to implement `flip` in `ndarray.py`. You also must implement \"flip\" forward and backward functions in `ops_mathematic.py`; note that these should be extremely short.\n","\n","**Important:** You should call NDArray.make with the new strides and offset, and then immediately `.compact()` this array. The resulting array is then copied and has positive strides. We want this (less-than-optimal) behavior because we did not account for negative strides in our previous implementation. _Aside:_ If you want, consider where/if negative strides break your implementation. `__getitem__` definitely doesn't work due to how we processed slices; is there anything else? (_Note_: this isn't graded.)\n","\n","Also, if you want to add a `flip` operator implementation on the CPU/CUDA backends instead, that's also okay.\n","\n"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-77WW6VucAT","executionInfo":{"status":"ok","timestamp":1763015346998,"user_tz":300,"elapsed":5581,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"5f67cf09-37e7-4ea8-cec5-dfc97f41a722"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1763 deselected / 40 selected                           \u001b[0m\n","\n","tests/hw4/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  2%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  5%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 12%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 15%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 17%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 22%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 32%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 35%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 37%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 42%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 47%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_forward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 52%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 55%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 57%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 62%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 65%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 67%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 70%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 77%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 82%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 85%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 87%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 92%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [ 95%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 97%]\u001b[0m\n","tests/hw4/test_conv.py::test_flip_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n","\n","\u001b[32m===================== \u001b[32m\u001b[1m40 passed\u001b[0m, \u001b[33m1763 deselected\u001b[0m\u001b[32m in 3.85s\u001b[0m\u001b[32m ======================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"flip\""]},{"cell_type":"markdown","metadata":{"id":"_RRx8YgDucAT"},"source":["-------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"LASdldLCucAT"},"source":["### Dilation\n"]},{"cell_type":"markdown","metadata":{"id":"VIEJ5GBnucAT"},"source":["The dilation operator puts zeros between elements of an ndarray. We will need it for computing the backward pass of convolution when the stride of the convolution is greater than 1. As an example, dilation should do the following to a 2x2 matrix when dilated by 1 on both axes:\n","\n","$$\n","\\begin{bmatrix}\n","1 & 2 \\\\\n","3 & 4\n","\\end{bmatrix}\n","\\Longrightarrow\n","\\begin{bmatrix}\n","1 & 0 & 2 & 0 \\\\\n","0 & 0 & 0 & 0 \\\\\n","3 & 0 & 4 & 0 \\\\\n","0 & 0 & 0 & 0\n","\\end{bmatrix}\n","$$\n","\n","To get some intuition for why we need dilation for the backward pass of strided convolution, consider a  `stride=2`, `padding=\"same\"`, `input_channels=output_channels=8` convolution applied to an input of size (10, 32, 32, 8). The resulting output will be of size (10, 16, 16, 8) due to the stride, and thus `out_grad` will have shape (10, 16, 16, 8). Yet, the gradient of the input needs to, of course, have shape (10, 32, 32, 8) -- so we must need to increase the size of `out_grad` in some way. Consider also that you could implement strided convolution as `Conv(x)[:, ::2, ::2, :]`, i.e., only keeping every other pixel in the spatial dimension.\n","\n","\n","Implement `Dilate` and `UnDilate` in `ops_mathematic.py`. Each operator takes two additional parameters (in attrs): the `dilation` amount and the `axes` to dilate. You must also implement the corresponding op `UnDilate`, whose forward pass will be used to implement the gradient of `Dilate`. (This is so we do not have to implement `GetItem` and `SetItem` ops, which can be highly inefficient to backprop through without additional optimizations.)\n","\n","**Note**: The dilation amount is additive, not multiplicative. In the example above, a dilation of `1` implies adding one row/column of zeros between each element along each dilated axis (and one removed row/column for each undilated axis). A dilation of `0` means no change."]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iZG3VUHMucAT","executionInfo":{"status":"ok","timestamp":1763015352644,"user_tz":300,"elapsed":5639,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"1c47857f-77db-4193-b5b9-a5ca29fb0a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1777 deselected / 26 selected                           \u001b[0m\n","\n","tests/hw4/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [  3%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_forward[needle.backend_ndarray.ndarray_backend_cuda] \u001b[32mPASSED\u001b[0m\u001b[32m [  7%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 11%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 15%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 19%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 30%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 34%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 42%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 46%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 53%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 65%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 69%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 84%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[31m [ 96%]\u001b[0m\n","tests/hw4/test_conv.py::test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m__ test_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (0,), 'd': 1, 'shape': (2, 5)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (0,)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (0,), 'd': 1, 'shape': (2, 5)}\n","shape      = (2, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","kwargs = {'axes': (0,), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]])\n","c = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],\n","       [ 0.33367433,  1.49407907, -0.2051582...86 ,  0.8644362 , -0.74216502,  2.26975462],\n","       [-1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877]])\n","is_stacked = False, num_args = 1, i = 0, j = 9, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(4.039984708126605) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","backward_grad = (needle.Tensor([[ 0.14404356  1.4542735   0.7610377   0.12167501  0.44386324]\n"," [-2.5529897   0.6536186   0.8644362  -0.742165    2.2697546 ]]),)\n","c          = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],\n","       [ 0.33367433,  1.49407907, -0.2051582...86 ,  0.8644362 , -0.74216502,  2.26975462],\n","       [-1.45436567,  0.04575852, -0.18718385,  1.53277921,  1.46935877]])\n","eps        = 0.001\n","error      = np.float64(4.039984708126605)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 9\n","kwargs     = {'axes': (0,), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0.]])]\n","out        = needle.Tensor([[0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (1,), 'd': 2, 'shape': (2, 5)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (1,)\n","d          = 2\n","device     = cuda()\n","params     = {'axes': (1,), 'd': 2, 'shape': (2, 5)}\n","shape      = (2, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","kwargs = {'axes': (1,), 'dilation': 2}, eps = 0.001\n","out = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","c = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n","         0.33367433,  1.49407907, -0.20515826...6252, -0.88778575, -1.98079647, -0.34791215,\n","         0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275]])\n","is_stacked = False, num_args = 1, i = 0, j = 9, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(3.1494307569284006) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","backward_grad = (needle.Tensor([[ 0.14404356  0.12167501  1.4940791  -0.85409576  0.8644362 ]\n"," [-1.4543657   1.5327792   0.37816253 -0.34791216  1.2023798 ]]),)\n","c          = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n","         0.33367433,  1.49407907, -0.20515826...6252, -0.88778575, -1.98079647, -0.34791215,\n","         0.15634897,  1.23029068,  1.20237985, -0.38732682, -0.30230275]])\n","eps        = 0.001\n","error      = np.float64(3.1494307569284006)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 9\n","kwargs     = {'axes': (1,), 'dilation': 2}\n","num_args   = 1\n","numerical_grad = [array([[0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0.]])]\n","out        = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (0, 1), 'd': 1, 'shape': (2, 5)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (0, 1)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (0, 1), 'd': 1, 'shape': (2, 5)}\n","shape      = (2, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","kwargs = {'axes': (0, 1), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","c = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n","         0.33367433,  1.49407907, -0.20515826...1794, -1.70627019,  1.9507754 , -0.50965218,\n","        -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n","is_stacked = False, num_args = 1, i = 0, j = 9, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(2.3925044473339963) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","backward_grad = (needle.Tensor([[ 0.14404356  0.7610377   0.44386324  1.4940791   0.3130677 ]\n"," [ 0.15494743 -0.88778573 -0.34791216  1.2302907  -0.3873268 ]]),)\n","c          = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323,\n","         0.33367433,  1.49407907, -0.20515826...1794, -1.70627019,  1.9507754 , -0.50965218,\n","        -0.4380743 , -1.25279536,  0.77749036, -1.61389785, -0.21274028]])\n","eps        = 0.001\n","error      = np.float64(2.3925044473339963)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 9\n","kwargs     = {'axes': (0, 1), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0.]])]\n","out        = needle.Tensor([[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (0, 1), 'd': 0, 'shape': (2, 5)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (0, 1)\n","d          = 0\n","device     = cuda()\n","params     = {'axes': (0, 1), 'd': 0, 'shape': (2, 5)}\n","shape      = (2, 5)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","kwargs = {'axes': (0, 1), 'dilation': 0}, eps = 0.001\n","out = needle.Tensor([[0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]])\n","c = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],\n","       [ 0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574]])\n","is_stacked = False, num_args = 1, i = 0, j = 9, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(2.477871961181703) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558  ]\n"," [-0.9772779   0.95008844 -0.1513572  -0.10321885  0.41059852]]),)\n","backward_grad = (needle.Tensor([[ 0.14404356  1.4542735   0.7610377   0.12167501  0.44386324]\n"," [ 0.33367434  1.4940791  -0.20515826  0.3130677  -0.85409576]]),)\n","c          = array([[ 0.14404357,  1.45427351,  0.76103773,  0.12167502,  0.44386323],\n","       [ 0.33367433,  1.49407907, -0.20515826,  0.3130677 , -0.85409574]])\n","eps        = 0.001\n","error      = np.float64(2.477871961181703)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 9\n","kwargs     = {'axes': (0, 1), 'dilation': 0}\n","num_args   = 1\n","numerical_grad = [array([[0., 0., 0., 0., 0.],\n","       [0., 0., 0., 0., 0.]])]\n","out        = needle.Tensor([[0. 0. 0. 0. 0.]\n"," [0. 0. 0. 0. 0.]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (0, 1), 'd': 2, 'shape': (2, 3, 3, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (0, 1)\n","d          = 2\n","device     = cuda()\n","params     = {'axes': (0, 1), 'd': 2, 'shape': (2, 3, 3, 4)}\n","shape      = (2, 3, 3, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","kwargs = {'axes': (0, 1), 'dilation': 2}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]...0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","c = array([[[[ 1.13940068e+00, -1.23482582e+00,  4.02341641e-01,\n","          -6.84810091e-01],\n","         [-8.70797149e-01, -5...          -1.01281486e-01],\n","         [-8.03141387e-01, -4.64337691e-01,  1.02179059e+00,\n","          -5.52540673e-01]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 71, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(8.732505177734161) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","backward_grad = (needle.Tensor([[[[ 1.1394007  -1.2348258   0.40234163 -0.6848101 ]\n","   [-0.87079716 -0.5788497  -0.31155252  0.0561653...945464]\n","   [-0.27567053 -0.70972794  1.7388726   0.99439436]\n","   [ 1.3191369  -0.8824188   1.128594    0.49600095]]]]),)\n","c          = array([[[[ 1.13940068e+00, -1.23482582e+00,  4.02341641e-01,\n","          -6.84810091e-01],\n","         [-8.70797149e-01, -5...          -1.01281486e-01],\n","         [-8.03141387e-01, -4.64337691e-01,  1.02179059e+00,\n","          -5.52540673e-01]]]])\n","eps        = 0.001\n","error      = np.float64(8.732505177734161)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 71\n","kwargs     = {'axes': (0, 1), 'dilation': 2}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","       ... 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]...0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (0, 1), 'd': 3, 'shape': (3, 3, 6, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (0, 1)\n","d          = 3\n","device     = cuda()\n","params     = {'axes': (0, 1), 'd': 3, 'shape': (3, 3, 6, 4)}\n","shape      = (3, 3, 6, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n","   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n","   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n","kwargs = {'axes': (0, 1), 'dilation': 3}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n",".... 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","c = array([[[[-1.70204139e-02,  3.79151736e-01,  2.25930895e+00,\n","          -4.22571517e-02],\n","         [-9.55945000e-01, -3...          -6.30225748e-01],\n","         [-1.34319254e+00,  7.58038051e-01, -5.83840844e-01,\n","          -1.02370145e+00]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 215, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(13.601822853307533) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n","   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n","   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n","backward_grad = (needle.Tensor([[[[-1.7020414e-02  3.7915173e-01  2.2593091e+00 -4.2257152e-02]\n","   [-9.5594501e-01 -3.4598178e-01 -4.6...01  2.2542837e+00  6.7263675e-01  2.5983250e-01]\n","   [-7.3718518e-01 -6.7832983e-01 -8.3288394e-02  1.6028637e+00]]]]),)\n","c          = array([[[[-1.70204139e-02,  3.79151736e-01,  2.25930895e+00,\n","          -4.22571517e-02],\n","         [-9.55945000e-01, -3...          -6.30225748e-01],\n","         [-1.34319254e+00,  7.58038051e-01, -5.83840844e-01,\n","          -1.02370145e+00]]]])\n","eps        = 0.001\n","error      = np.float64(13.601822853307533)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 215\n","kwargs     = {'axes': (0, 1), 'dilation': 3}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         ...., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n",".... 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (1, 2), 'd': 0, 'shape': (2, 3, 3, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (1, 2)\n","d          = 0\n","device     = cuda()\n","params     = {'axes': (1, 2), 'd': 0, 'shape': (2, 3, 3, 4)}\n","shape      = (2, 3, 3, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","kwargs = {'axes': (1, 2), 'dilation': 0}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]...0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","c = array([[[[ 1.13940068, -1.23482582,  0.40234164, -0.68481009],\n","         [-0.87079715, -0.57884966, -0.31155253,  0.056...[ 0.57659082, -0.20829876,  0.39600671, -1.09306151],\n","         [-1.49125759,  0.4393917 ,  0.1666735 ,  0.63503144]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 71, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(8.455226974329095) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","backward_grad = (needle.Tensor([[[[ 1.1394007  -1.2348258   0.40234163 -0.6848101 ]\n","   [-0.87079716 -0.5788497  -0.31155252  0.0561653...643327]\n","   [ 0.57659084 -0.20829876  0.3960067  -1.0930616 ]\n","   [-1.4912575   0.4393917   0.1666735   0.63503146]]]]),)\n","c          = array([[[[ 1.13940068, -1.23482582,  0.40234164, -0.68481009],\n","         [-0.87079715, -0.57884966, -0.31155253,  0.056...[ 0.57659082, -0.20829876,  0.39600671, -1.09306151],\n","         [-1.49125759,  0.4393917 ,  0.1666735 ,  0.63503144]]]])\n","eps        = 0.001\n","error      = np.float64(8.455226974329095)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 71\n","kwargs     = {'axes': (1, 2), 'dilation': 0}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","       ... 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]...0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (1, 2), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (1, 2)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (1, 2), 'd': 1, 'shape': (2, 3, 3, 4)}\n","shape      = (2, 3, 3, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","kwargs = {'axes': (1, 2), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n",".... 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","c = array([[[[ 1.13940068, -1.23482582,  0.40234164, -0.68481009],\n","         [-0.87079715, -0.57884966, -0.31155253,  0.056...[ 1.36453185, -0.68944918, -0.6522936 , -0.52118931],\n","         [-1.84306955, -0.477974  , -0.47965581,  0.6203583 ]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 71\n","f1 = np.float64(279.1831846346351), f2 = np.float64(279.1831846346351)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(7.517772685898782) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","backward_grad = (needle.Tensor([[[[ 1.1394007  -1.2348258   0.40234163 -0.6848101 ]\n","   [-1.1651498   0.9008265   0.46566245 -1.5362437...435159]\n","   [-0.7196044  -0.812993    0.27451634 -0.8909151 ]\n","   [-0.7047003   0.9432607   0.7471883  -1.1889449 ]]]]),)\n","c          = array([[[[ 1.13940068, -1.23482582,  0.40234164, -0.68481009],\n","         [-0.87079715, -0.57884966, -0.31155253,  0.056...[ 1.36453185, -0.68944918, -0.6522936 , -0.52118931],\n","         [-1.84306955, -0.477974  , -0.47965581,  0.6203583 ]]]])\n","eps        = 0.001\n","error      = np.float64(7.517772685898782)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(279.1831846346351)\n","f2         = np.float64(279.1831846346351)\n","i          = 0\n","is_stacked = False\n","j          = 71\n","kwargs     = {'axes': (1, 2), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","       ... 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n",".... 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (1, 2), 'd': 1, 'shape': (3, 3, 6, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (1, 2)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (1, 2), 'd': 1, 'shape': (3, 3, 6, 4)}\n","shape      = (3, 3, 6, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n","   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n","   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n","kwargs = {'axes': (1, 2), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","  ...[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","c = array([[[[-1.70204139e-02,  3.79151736e-01,  2.25930895e+00,\n","          -4.22571517e-02],\n","         [-9.55945000e-01, -3...           9.38746876e-01],\n","         [ 6.07111672e-01, -1.04817041e+00, -8.60262452e-01,\n","           3.28301295e-01]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 215, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(14.317636033534761) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n","   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n","   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n","backward_grad = (needle.Tensor([[[[-0.01702041  0.37915173  2.259309   -0.04225715]\n","   [-1.540797    0.06326199  0.15650654  0.2321810...39857 ]\n","   [ 0.22425222 -1.6786884   0.2149656   0.09721923]\n","   [ 1.7123052  -0.79211503 -1.0455246  -1.084856  ]]]]),)\n","c          = array([[[[-1.70204139e-02,  3.79151736e-01,  2.25930895e+00,\n","          -4.22571517e-02],\n","         [-9.55945000e-01, -3...           9.38746876e-01],\n","         [ 6.07111672e-01, -1.04817041e+00, -8.60262452e-01,\n","           3.28301295e-01]]]])\n","eps        = 0.001\n","error      = np.float64(14.317636033534761)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 215\n","kwargs     = {'axes': (1, 2), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         ...., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","  ...[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda] ___\u001b[0m\n","\n","params = {'axes': (2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (2, 3)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}\n","shape      = (2, 3, 3, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","kwargs = {'axes': (2, 3), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n","    -0.9772779   0.95008844 -0.1513572 ]\n"," ...0.7065732 ]\n","   [ 0.01050002  1.7858706   0.12691209  0.40198937  1.8831507\n","    -1.347759   -1.270485    0.9693967 ]]]])\n","c = array([[[[ 1.13940068, -1.23482582,  0.40234164, -0.68481009,\n","          -0.87079715, -0.57884966, -0.31155253,  0.0561... [ 1.36453185, -0.68944918, -0.6522936 , -0.52118931,\n","          -1.84306955, -0.477974  , -0.47965581,  0.6203583 ]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 71, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(7.740147385762427) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","backward_grad = (needle.Tensor([[[[ 1.1394007   0.40234163 -0.87079716 -0.31155252]\n","   [-1.0707526  -0.40317693  0.20827498  0.3563664...16264 ]\n","   [-0.7047003   0.7471883   0.77325296 -2.6591723 ]\n","   [-0.44092262 -0.36469355  0.5785215  -0.76414394]]]]),)\n","c          = array([[[[ 1.13940068, -1.23482582,  0.40234164, -0.68481009,\n","          -0.87079715, -0.57884966, -0.31155253,  0.0561... [ 1.36453185, -0.68944918, -0.6522936 , -0.52118931,\n","          -1.84306955, -0.477974  , -0.47965581,  0.6203583 ]]]])\n","eps        = 0.001\n","error      = np.float64(7.740147385762427)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 71\n","kwargs     = {'axes': (2, 3), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","       ... 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931   1.867558\n","    -0.9772779   0.95008844 -0.1513572 ]\n"," ...0.7065732 ]\n","   [ 0.01050002  1.7858706   0.12691209  0.40198937  1.8831507\n","    -1.347759   -1.270485    0.9693967 ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda] __\u001b[0m\n","\n","params = {'axes': (2, 3), 'd': 1, 'shape': (3, 3, 6, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (2, 3)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (2, 3), 'd': 1, 'shape': (3, 3, 6, 4)}\n","shape      = (3, 3, 6, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n","   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n","   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n","kwargs = {'axes': (2, 3), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0...   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]]]])\n","c = array([[[[-1.70204139e-02,  3.79151736e-01,  2.25930895e+00,\n","          -4.22571517e-02, -9.55945000e-01, -3.45981776e-...14709e-02,\n","           9.38746876e-01,  6.07111672e-01, -1.04817041e+00,\n","          -8.60262452e-01,  3.28301295e-01]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 215\n","f1 = np.float64(826.8091528310688), f2 = np.float64(826.8091528310688)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(13.852510747528259) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...80309 ]\n","   [ 0.2799246  -0.09815039  0.9101789   0.3172182 ]\n","   [ 0.78632796 -0.4664191  -0.94444627 -0.4100497 ]]]]),)\n","backward_grad = (needle.Tensor([[[[-0.01702041  2.259309   -0.955945   -0.463596  ]\n","   [-0.54286146 -1.1561824   1.4944845   0.4262587...262553]\n","   [ 0.03308975 -0.71994054 -0.15602389  3.1709747 ]\n","   [ 0.09837791  0.06749225  0.2843145  -1.0314825 ]]]]),)\n","c          = array([[[[-1.70204139e-02,  3.79151736e-01,  2.25930895e+00,\n","          -4.22571517e-02, -9.55945000e-01, -3.45981776e-...14709e-02,\n","           9.38746876e-01,  6.07111672e-01, -1.04817041e+00,\n","          -8.60262452e-01,  3.28301295e-01]]]])\n","eps        = 0.001\n","error      = np.float64(13.852510747528259)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(826.8091528310688)\n","f2         = np.float64(826.8091528310688)\n","i          = 0\n","is_stacked = False\n","j          = 215\n","kwargs     = {'axes': (2, 3), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         ...., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0...   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[31m\u001b[1m__ test_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda] __\u001b[0m\n","\n","params = {'axes': (0, 1, 2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mparams\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, dilate_backward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_dilate_backward\u001b[39;49;00m(params, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        shape, d, axes = params[\u001b[33m'\u001b[39;49;00m\u001b[33mshape\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33md\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m], params[\u001b[33m'\u001b[39;49;00m\u001b[33maxes\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n",">       backward_check(ndl.dilate, ndl.Tensor(np.random.randn(*shape), device=device), dilation=d, axes=axes)\u001b[90m\u001b[39;49;00m\n","\n","axes       = (0, 1, 2, 3)\n","d          = 1\n","device     = cuda()\n","params     = {'axes': (0, 1, 2, 3), 'd': 1, 'shape': (2, 3, 3, 4)}\n","shape      = (2, 3, 3, 4)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:296: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","f = <function dilate at 0x11e654d8a3e0>\n","args = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","kwargs = {'axes': (0, 1, 2, 3), 'dilation': 1}, eps = 0.001\n","out = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. ... 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","c = array([[[[ 1.13940068e+00, -1.23482582e+00,  4.02341641e-01, ...,\n","          -5.78849665e-01, -3.11552532e-01,  5.61653...7.51946588e-01,  5.62989719e-01, -1.19498681e+00, ...,\n","          -4.08014709e-01,  1.77465856e+00, -3.93153195e-01]]]])\n","is_stacked = False, num_args = 1, i = 0, j = 71, f1 = np.float64(0.0)\n","f2 = np.float64(0.0)\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mbackward_check\u001b[39;49;00m(f, *args, **kwargs):\u001b[90m\u001b[39;49;00m\n","        eps = \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        out = f(*args, **kwargs)\u001b[90m\u001b[39;49;00m\n","        c = np.random.randn(*out.shape)\u001b[90m\u001b[39;49;00m\n","        is_stacked = \u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(args[\u001b[94m0\u001b[39;49;00m], \u001b[96mlist\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            args = args[\u001b[94m0\u001b[39;49;00m]\u001b[90m\u001b[39;49;00m\n","            is_stacked = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        numerical_grad = [np.zeros(a.shape) \u001b[94mfor\u001b[39;49;00m a \u001b[95min\u001b[39;49;00m args]\u001b[90m\u001b[39;49;00m\n","        num_args = \u001b[96mlen\u001b[39;49;00m(args)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(num_args):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m j \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(args[i].realize_cached_data().size):\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f1 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] -= \u001b[94m2\u001b[39;49;00m * eps\u001b[90m\u001b[39;49;00m\n","                \u001b[94mif\u001b[39;49;00m is_stacked:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                    f2 = (f(*args, **kwargs).numpy() * c).sum()\u001b[90m\u001b[39;49;00m\n","                args[i].realize_cached_data().flat[j] += eps\u001b[90m\u001b[39;49;00m\n","                numerical_grad[i].flat[j] = (f1 - f2) / (\u001b[94m2\u001b[39;49;00m * eps)\u001b[90m\u001b[39;49;00m\n","        backward_grad = out.op.gradient_as_tuple(ndl.Tensor(c, device=args[\u001b[94m0\u001b[39;49;00m].device), out)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mif\u001b[39;49;00m \u001b[96misinstance\u001b[39;49;00m(backward_grad[\u001b[94m0\u001b[39;49;00m], ndl.TensorTuple): \u001b[90m# TODO keep this?\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            backward_grad = backward_grad[\u001b[94m0\u001b[39;49;00m].tuple()\u001b[90m\u001b[39;49;00m\n","        error = \u001b[96msum\u001b[39;49;00m(\u001b[90m\u001b[39;49;00m\n","            np.linalg.norm(backward_grad[i].numpy() - numerical_grad[i])\u001b[90m\u001b[39;49;00m\n","            \u001b[94mfor\u001b[39;49;00m i \u001b[95min\u001b[39;49;00m \u001b[96mrange\u001b[39;49;00m(\u001b[96mlen\u001b[39;49;00m(args))\u001b[90m\u001b[39;49;00m\n","        )\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m error < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       assert np.float64(7.657603879683142) < 0.01\u001b[0m\n","\n","args       = (needle.Tensor([[[[ 1.7640524   0.4001572   0.978738    2.2408931 ]\n","   [ 1.867558   -0.9772779   0.95008844 -0.1513572...62826 ]\n","   [ 0.17742614 -0.40178093 -1.6301984   0.46278226]\n","   [-0.9072984   0.0519454   0.7290906   0.12898292]]]]),)\n","backward_grad = (needle.Tensor([[[[ 1.1394007   0.40234163 -0.87079716 -0.31155252]\n","   [-1.0707526  -0.40317693  0.20827498  0.3563664...57403 ]\n","   [-1.6429653  -0.53527015  1.154184    0.02106202]\n","   [-0.24133779  0.69938046 -0.222477    0.05095428]]]]),)\n","c          = array([[[[ 1.13940068e+00, -1.23482582e+00,  4.02341641e-01, ...,\n","          -5.78849665e-01, -3.11552532e-01,  5.61653...7.51946588e-01,  5.62989719e-01, -1.19498681e+00, ...,\n","          -4.08014709e-01,  1.77465856e+00, -3.93153195e-01]]]])\n","eps        = 0.001\n","error      = np.float64(7.657603879683142)\n","f          = <function dilate at 0x11e654d8a3e0>\n","f1         = np.float64(0.0)\n","f2         = np.float64(0.0)\n","i          = 0\n","is_stacked = False\n","j          = 71\n","kwargs     = {'axes': (0, 1, 2, 3), 'dilation': 1}\n","num_args   = 1\n","numerical_grad = [array([[[[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","       ... 0.],\n","         [0., 0., 0., 0.]],\n","\n","        [[0., 0., 0., 0.],\n","         [0., 0., 0., 0.],\n","         [0., 0., 0., 0.]]]])]\n","out        = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. ... 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:45: AssertionError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params0-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(4.039984708126605) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params1-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(3.1494307569284006) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params2-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(2.3925044473339963) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params3-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(2.477871961181703) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params4-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(8.732505177734161) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params5-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(13.601822853307533) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params6-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(8.455226974329095) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params7-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(7.517772685898782) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params8-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(14.317636033534761) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params9-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(7.740147385762427) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params10-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(13.852510747528259) < 0.01\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_dilate_backward[params11-needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - assert np.float64(7.657603879683142) < 0.01\n","\u001b[31m================ \u001b[31m\u001b[1m12 failed\u001b[0m, \u001b[32m14 passed\u001b[0m, \u001b[33m1777 deselected\u001b[0m\u001b[31m in 3.82s\u001b[0m\u001b[31m ================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"dilate\""]},{"cell_type":"markdown","metadata":{"id":"8b97XFQRucAT"},"source":["---------------------------------------"]},{"cell_type":"markdown","metadata":{"id":"IBSo3LkeucAT"},"source":["### Submit new ops (flip/dilation) to mugrade [10 points]"]},{"cell_type":"code","execution_count":36,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nIbSIyVFucAT","executionInfo":{"status":"ok","timestamp":1763015357369,"user_tz":300,"elapsed":4719,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"57094946-f044-4ea9-b461-8621596d120a"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\n","\n","tests/hw4/test_conv.py \n","Submitting new_ops...\n","Grader test 1 passed\n","Grader test 2 passed\n","Grader test 3 passed\n","Grader test 4 passed\n","Grader test 5 passed\n","\u001b[31mF\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m________________________________ submit_new_ops ________________________________\u001b[0m\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92msubmit_new_ops\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n","        \u001b[90m# pad\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m1337\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randint(low=\u001b[94m1\u001b[39;49;00m, high=\u001b[94m10\u001b[39;49;00m, size=(\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        A  = nd.NDArray(_A, device=nd.cpu())\u001b[90m\u001b[39;49;00m\n","        MugradeSubmit(A.pad(( (\u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), (\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m))))\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mDoFlip\u001b[39;49;00m(shape, axes, backward=\u001b[94mFalse\u001b[39;49;00m, device=ndl.cpu()):\u001b[90m\u001b[39;49;00m\n","            X = Rand(*shape, device=device)\u001b[90m\u001b[39;49;00m\n","            X.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            Y = ndl.flip(X, axes=axes)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m backward:\u001b[90m\u001b[39;49;00m\n","                V = Rand(*shape, device=device, entropy=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","                Z = (V*Y).sum()\u001b[90m\u001b[39;49;00m\n","                Z.backward()\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m X.grad\u001b[90m\u001b[39;49;00m\n","            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m Y\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mDoDilate\u001b[39;49;00m(shape, axes, dilation, backward=\u001b[94mFalse\u001b[39;49;00m, device=ndl.cpu()):\u001b[90m\u001b[39;49;00m\n","            X = Rand(*shape, device=device)\u001b[90m\u001b[39;49;00m\n","            X.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            Y = ndl.dilate(X, dilation=dilation, axes=axes)\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m backward:\u001b[90m\u001b[39;49;00m\n","                V = Rand(*Y.shape, device=device, entropy=\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","                Z = (V*Y).sum()\u001b[90m\u001b[39;49;00m\n","                Z.backward()\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m X.grad\u001b[90m\u001b[39;49;00m\n","            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m Y\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[90m# flip\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        MugradeSubmit(DoFlip((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m)))\u001b[90m\u001b[39;49;00m\n","        MugradeSubmit(DoFlip((\u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m,\u001b[94m1\u001b[39;49;00m,\u001b[94m2\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m)))\u001b[90m\u001b[39;49;00m\n","        MugradeSubmit(DoFlip((\u001b[94m8\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m), (\u001b[94m1\u001b[39;49;00m,)))\u001b[90m\u001b[39;49;00m\n","        MugradeSubmit(DoFlip((\u001b[94m4\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m), (\u001b[94m0\u001b[39;49;00m,)))\u001b[90m\u001b[39;49;00m\n",">       MugradeSubmit(DoFlip((\u001b[94m2\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m), (\u001b[94m2\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m), backward=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:641: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:618: in DoFlip\n","    \u001b[0mZ = (V*Y).sum()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xee7413aed20>\n","a = NDArray([[[[10.]\n","   [28.]\n","   [ 5.]]\n","\n","  [[ 2.]\n","   [ 2.]\n","   [15.]]]\n","\n","\n"," [[[21.]\n","   [49.]\n","   [21.]]\n","\n","  [[42.]\n","   [14.]\n","   [ 3.]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1msubmit_new_ops\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[31m in 2.57s\u001b[0m\u001b[31m ========================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"new_ops\""]},{"cell_type":"markdown","metadata":{"id":"g0UFCOyWucAT"},"source":["-----------------"]},{"cell_type":"markdown","metadata":{"id":"Dor8KuRkucAT"},"source":["### Convolution forward\n","\n","Implement the forward pass of 2D multi-channel convolution in `ops_mathematic.py`. You should probably refer to [this notebook](https://github.com/dlsyscourse/public_notebooks/blob/main/convolution_implementation.ipynb) from lecture, which implements 2D multi-channel convolution using im2col in numpy.\n","\n","**Note:** Your convolution op should accept tensors in the NHWC format, as in the example above, and weights in the format (kernel_size, kernel_size, input_channels, output_channels).\n","\n","However, you will need to add two additional features. Your convolution function should accept arguments for `padding` (default 0) and `stride` (default 1). For `padding`, you should simply apply your padding function to the spatial dimensions (i.e., axes 1 and 2).\n","\n","Implementing strided convolution should consist of a relatively small set of changes to your plain convolution implementation.\n","\n","We recommend working your way up through the full feature set: Implement convolution without stride first, ensuring you pass some of the tests below, and then add in support for stride."]},{"cell_type":"code","execution_count":37,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i0RHLgiwucAT","executionInfo":{"status":"ok","timestamp":1763015366031,"user_tz":300,"elapsed":8660,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"8556cbe5-5af5-4379-a8cb-66793982d687"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\n","\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 29%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e...5e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e...5e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x12210a6d8200>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x12210a6d8200>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x12210a6d94f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x12210a6d94f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x12210a6d8200>\n","a = NDArray([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x12210a6d8200>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 1\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109....98     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109....98     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109...     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x12202263e3f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109...     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x12202263e3f0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc4230>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc4230>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x12202263e3f0>\n","a = NDArray([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109.12486 ...21   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109.12486 ...21   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x12202263e3f0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 2\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...1e+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...1e+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf3aa0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bf3aa0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf3b30>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf3b30>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bf3aa0>\n","a = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf3aa0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e...8e+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e...8e+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493...+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021a34350>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493...+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021a34350>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a35e50>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a35e50>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021a34350>\n","a = NDArray([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e+01]\n"," ...72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e+01]\n"," ...72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021a34350>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595... -8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595... -8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.1759...8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bb4cb0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.1759...8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bb4cb0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bb7d40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bb7d40>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bb4cb0>\n","a = NDArray([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595   -10...54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595   -10...54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bb4cb0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621  ...-120.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621  ...-120.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621 ...20.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba2c90>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621 ...20.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021ba2c90>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba1610>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba1610>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021ba2c90>\n","a = NDArray([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621     348... 75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621     348... 75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba2c90>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 1\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e...9e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e...9e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf2f60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bf2f60>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf2420>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf2420>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bf2f60>\n","a = NDArray([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf2f60>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 2\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...3e+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...3e+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf1be0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bf1be0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf0a40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf0a40>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bf1be0>\n","a = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf1be0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625 ... 143.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625 ... 143.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625...43.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021a36930>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625...43.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021a36930>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a37ec0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a37ec0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021a36930>\n","a = NDArray([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625  -322....172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625  -322....172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021a36930>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e...3e+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e...3e+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266...+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bc4d40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266...+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bc4d40>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc65d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc65d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bc4d40>\n","a = NDArray([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e+01]\n"," ...30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e+01]\n"," ...30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bc4d40>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n","     -0.36175704   4.6234684 ]\n","   [  5.279258... -4.82095      4.98517   ]\n","   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n","      8.537833     2.3818388 ]]]])\n","W_shape    = (3, 3, 24, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n","      4.322181    -3.7108252 ]\n","   [ 11.348773...4.207389     8.935435  ]\n","   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n","     -1.400853     0.07475674]]]])\n","Z_shape    = (3, 16, 16, 24)\n","_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n","           -0.36175704,   4.6234684 ],\n","        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n","            8.537833  ,   2.3818388 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n","            4.322181  ,  -3.7108252 ],\n","        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n","           -1.400853  ,   0.07475674]]]], dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e...4e+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e...4e+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181...+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf2480>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181...+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bf2480>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf11f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf11f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bf2480>\n","a = NDArray([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e+02]\n"," ...03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e+02]\n"," ...03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf2480>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n","     2.17041516e+00 -3.88039827e...7e+00]\n","   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n","     4.27775383e-01 -3.84437203e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n","           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n","          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e...4e+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e...4e+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549...+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d22a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549...+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1220218d22a0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d2270>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d2270>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1220218d22a0>\n","a = NDArray([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e+00]\n"," ...85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e+00]\n"," ...85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d22a0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n","     6.47611380e+00 -3.38525438e...7e+00]\n","   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n","     2.56555057e+00  1.63004756e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...8e+00]\n","   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n","     4.72636795e+00 -5.75466204e+00]]]])\n","Z_shape    = (3, 17, 17, 8)\n","_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n","           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n","           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n","          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e...2e+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e...2e+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637...+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d1940>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637...+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1220218d1940>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d05c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d05c0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1220218d1940>\n","a = NDArray([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e+02]\n"," ...95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e+02]\n"," ...95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d1940>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n","     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n","     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n","     -7.9124722 ]]]])\n","W_shape    = (5, 5, 1, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00]\n","   [ 2.00078607e+00]\n","   [ 4.89369011e+00]\n","   [ 1.12044659e+01]\n","   [ 9.33778954e+00]...22066e-01]\n","   [ 1.05310105e-01]\n","   [ 4.97272283e-01]\n","   [ 1.13696384e+00]\n","   [-5.08369303e+00]\n","   [-5.73876619e-01]]]])\n","Z_shape    = (3, 17, 17, 1)\n","_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n","           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n","           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00],\n","         [ 2.00078607e+00],\n","         [ 4.89369011e+00],\n","         [ 1.12044659e+01],\n","      ... 4.97272283e-01],\n","         [ 1.13696384e+00],\n","         [-5.08369303e+00],\n","         [-5.73876619e-01]]]], dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e...8e+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e...8e+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985...+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d0b60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985...+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1220218d0b60>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d0830>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d0830>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1220218d0b60>\n","a = NDArray([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e+01]\n"," ...41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e+01]\n"," ...41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d0b60>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 1.55579513e-02]\n","   [-3.04658723e+00]\n","   [ 4.09725952e+00]\n","   [ 1.29906273e+00]\n","   [-7.50546598e+00]...75421e+00]\n","   [ 3.02805209e+00]\n","   [-4.20992136e+00]\n","   [ 4.11707735e+00]\n","   [ 4.99041653e+00]\n","   [ 5.11040497e+00]]]])\n","W_shape    = (5, 5, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 1.55579513e-02],\n","         [-3.04658723e+00],\n","         [ 4.09725952e+00],\n","         [ 1.29906273e+00],\n","      ...-4.20992136e+00],\n","         [ 4.11707735e+00],\n","         [ 4.99041653e+00],\n","         [ 5.11040497e+00]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.9615...44982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.9615...44982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.961...982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021a36480>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.961...982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021a36480>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a37aa0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a37aa0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021a36480>\n","a = NDArray([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.96155 ]\n","  ...201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.96155 ]\n","  ...201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021a36480>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 0.01555795]\n","   [-3.0465872 ]\n","   [ 4.0972595 ]\n","   [ 1.2990627 ]\n","   [-7.505466  ]\n","   [-1.7335238 ]\n","  ...[ 5.6893935 ]\n","   [ 2.267663  ]\n","   [-1.0709513 ]\n","   [-5.0566583 ]\n","   [-0.23777105]\n","   [-9.931785  ]\n","   [-0.44905776]]]])\n","W_shape    = (1, 1, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 0.01555795],\n","         [-3.0465872 ],\n","         [ 4.0972595 ],\n","         [ 1.2990627 ],\n","         [-7.505466  ]...13 ],\n","         [-5.0566583 ],\n","         [-0.23777105],\n","         [-9.931785  ],\n","         [-0.44905776]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154....]\n","   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154....]\n","   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154...   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d2450>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154...   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1220218d2450>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d0e00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d0e00>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1220218d2450>\n","a = NDArray([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154.07153 ...153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154.07153 ...153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d2450>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] _\u001b[0m\n","\n","Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n","backward = False, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n","   [ -1.4591868   -3.807461  ]]\n","\n","  [[  4.2896194    5.705509  ]\n","   [  7.3...7526     2.911123  ]\n","   [-10.473016     0.61860955]]\n","\n","  [[ -0.65053475   0.46976614]\n","   [  4.7152305  -13.698386  ]]]])\n","W_shape    = (3, 3, 2, 2)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   [  4.750...19315276  -8.283575  ]\n","   [ -4.9275537   -7.359175  ]\n","   [  8.240675     0.8211388 ]\n","   [  2.8364513   -1.1133755 ]]]])\n","Z_shape    = (1, 14, 14, 2)\n","_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n","         [ -1.4591868 ,  -3.807461  ]],\n","\n","        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n","\n","        [[ -0.65053475,   0.46976614],\n","         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...275537 ,  -7.359175  ],\n","         [  8.240675  ,   0.8211388 ],\n","         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n","backward   = False\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ ...   228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ ...   228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [... 228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfec00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [... 228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bfec00>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfd8b0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfd8b0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bfec00>\n","a = NDArray([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ -41.20... ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ -41.20... ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfec00>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd70>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd70>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfc5f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfc5f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd70>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd70>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 1\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba0e90>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021ba0e90>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba0a10>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba0a10>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021ba0e90>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba0e90>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 2\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfdfd0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bfdfd0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfd7c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfd7c0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bfdfd0>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfdfd0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611 ....\n","      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611 ....\n","      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611...      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfe480>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611...      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bfe480>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfc290>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfc290>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bfe480>\n","a = NDArray([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611    -2....      0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611    -2....      0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfe480>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba2900>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021ba2900>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba3470>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba3470>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021ba2900>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba2900>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. ... 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. ... 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0..... 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d0e60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0..... 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1220218d0e60>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d28d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d28d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1220218d0e60>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","  ...0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","  ...0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d0e60>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 1\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d3230>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1220218d3230>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d11c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1220218d11c0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1220218d3230>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1220218d3230>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 2\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf33e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bf33e0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf2a50>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bf2a50>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bf33e0>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bf33e0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd10>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd10>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bff860>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bff860>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd10>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfcd10>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021a37d10>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021a37d10>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a36a80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a36a80>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021a37d10>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021a37d10>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n","     -0.36175704   4.6234684 ]\n","   [  5.279258... -4.82095      4.98517   ]\n","   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n","      8.537833     2.3818388 ]]]])\n","W_shape    = (3, 3, 24, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n","      4.322181    -3.7108252 ]\n","   [ 11.348773...4.207389     8.935435  ]\n","   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n","     -1.400853     0.07475674]]]])\n","Z_shape    = (3, 16, 16, 24)\n","_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n","           -0.36175704,   4.6234684 ],\n","        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n","            8.537833  ,   2.3818388 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n","            4.322181  ,  -3.7108252 ],\n","        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n","           -1.400853  ,   0.07475674]]]], dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba1010>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021ba1010>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba1a30>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021ba1a30>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021ba1010>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021ba1010>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n","     2.17041516e+00 -3.88039827e...7e+00]\n","   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n","     4.27775383e-01 -3.84437203e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n","           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n","          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021a36d80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021a36d80>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a35d90>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021a35d90>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021a36d80>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021a36d80>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n","     6.47611380e+00 -3.38525438e...7e+00]\n","   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n","     2.56555057e+00  1.63004756e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...8e+00]\n","   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n","     4.72636795e+00 -5.75466204e+00]]]])\n","Z_shape    = (3, 17, 17, 8)\n","_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n","           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n","           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n","          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x12202186d820>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x12202186d820>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x12202186da00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x12202186da00>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x12202186d820>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x12202186d820>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n","     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n","     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n","     -7.9124722 ]]]])\n","W_shape    = (5, 5, 1, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00]\n","   [ 2.00078607e+00]\n","   [ 4.89369011e+00]\n","   [ 1.12044659e+01]\n","   [ 9.33778954e+00]...22066e-01]\n","   [ 1.05310105e-01]\n","   [ 4.97272283e-01]\n","   [ 1.13696384e+00]\n","   [-5.08369303e+00]\n","   [-5.73876619e-01]]]])\n","Z_shape    = (3, 17, 17, 1)\n","_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n","           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n","           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00],\n","         [ 2.00078607e+00],\n","         [ 4.89369011e+00],\n","         [ 1.12044659e+01],\n","      ... 4.97272283e-01],\n","         [ 1.13696384e+00],\n","         [-5.08369303e+00],\n","         [-5.73876619e-01]]]], dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfd190>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bfd190>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfc380>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bfc380>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bfd190>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bfd190>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 1.55579513e-02]\n","   [-3.04658723e+00]\n","   [ 4.09725952e+00]\n","   [ 1.29906273e+00]\n","   [-7.50546598e+00]...75421e+00]\n","   [ 3.02805209e+00]\n","   [-4.20992136e+00]\n","   [ 4.11707735e+00]\n","   [ 4.99041653e+00]\n","   [ 5.11040497e+00]]]])\n","W_shape    = (5, 5, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 1.55579513e-02],\n","         [-3.04658723e+00],\n","         [ 4.09725952e+00],\n","         [ 1.29906273e+00],\n","      ...-4.20992136e+00],\n","         [ 4.11707735e+00],\n","         [ 4.99041653e+00],\n","         [ 5.11040497e+00]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]...]\n","   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]...]\n","   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bc5340>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bc5340>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc6b70>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc6b70>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bc5340>\n","a = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]\n","\n","  [[...[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]\n","\n","  [[...[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bc5340>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 0.01555795]\n","   [-3.0465872 ]\n","   [ 4.0972595 ]\n","   [ 1.2990627 ]\n","   [-7.505466  ]\n","   [-1.7335238 ]\n","  ...[ 5.6893935 ]\n","   [ 2.267663  ]\n","   [-1.0709513 ]\n","   [-5.0566583 ]\n","   [-0.23777105]\n","   [-9.931785  ]\n","   [-0.44905776]]]])\n","W_shape    = (1, 1, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 0.01555795],\n","         [-3.0465872 ],\n","         [ 4.0972595 ],\n","         [ 1.2990627 ],\n","         [-7.505466  ]...13 ],\n","         [-5.0566583 ],\n","         [-0.23777105],\n","         [-9.931785  ],\n","         [-0.44905776]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","...0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","...0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bb6450>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bb6450>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bb4ce0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bb4ce0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bb6450>\n","a = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0....[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0....[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bb6450>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] _\u001b[0m\n","\n","Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n","backward = False, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n","   [ -1.4591868   -3.807461  ]]\n","\n","  [[  4.2896194    5.705509  ]\n","   [  7.3...7526     2.911123  ]\n","   [-10.473016     0.61860955]]\n","\n","  [[ -0.65053475   0.46976614]\n","   [  4.7152305  -13.698386  ]]]])\n","W_shape    = (3, 3, 2, 2)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   [  4.750...19315276  -8.283575  ]\n","   [ -4.9275537   -7.359175  ]\n","   [  8.240675     0.8211388 ]\n","   [  2.8364513   -1.1133755 ]]]])\n","Z_shape    = (1, 14, 14, 2)\n","_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n","         [ -1.4591868 ,  -3.807461  ]],\n","\n","        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n","\n","        [[ -0.65053475,   0.46976614],\n","         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...275537 ,  -7.359175  ],\n","         [  8.240675  ,   0.8211388 ],\n","         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n","backward   = False\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.9164972...+00]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.9164972...+00]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.916497...0]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x122021bc5c40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.916497...0]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x122021bc5c40>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc7380>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x122021bc7380>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x122021bc5c40>\n","a = NDArray([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.91649723e+00]...12946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.91649723e+00]...12946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x122021bc5c40>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[forward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31m===================== \u001b[31m\u001b[1m34 failed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[31m in 6.41s\u001b[0m\u001b[31m ======================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"op_conv and forward\""]},{"cell_type":"markdown","metadata":{"id":"lutcrgEFucAT"},"source":["-----------------"]},{"cell_type":"markdown","metadata":{"id":"IGqAU_nVucAT"},"source":["### Convolution backward"]},{"cell_type":"markdown","metadata":{"id":"kaiZsMdzucAT"},"source":["Finding the gradients of 2D multi-channel convolution can be technically quite challenging (especially \"rigorously\"). We will try to provide some useful hints here. Basically, we encourage you to make use of the surprising fact that _whatever makes the dimensions work out is typically right_.\n","\n","Ultimately, the backward pass of convolution can be done in terms of the convolution operator itself, with some clever manipulations using `flip`, `dilate`, and multiple applications of `transpose` to both the arguments and the results.\n","\n","In the last section, we essentially implemented convolution as a matrix product: ignoring the various restride and reshape operations, we basically have something like `X @ W`, where `X` is the input and `W` is the weight. We also have `out_grad`, which is the same shape as `X @ W`. Now, you have already implemented the backward pass of matrix multiplication in a previous assignment, and we can use this knowledge to get some insight into the backward pass of convolution. In particular, referencing your matmul backward implementation, you may notice (heuristically speaking here):\n","\n","`X.grad = out_grad @ W.transpose` \\\n","`W.grad = X.transpose @ out_grad`\n","\n","Surprisingly enough, things work out if we just assume that these are also convolutions (and now assuming that `out_grad`, `W`, and `X` are tensors amenable to 2D multi-channel convolution instead of matrices):\n","\n","`X.grad = conv(out_grad, W)` \\\n","`W.grad = conv(X, out_grad)`\n","\n","In which the \"\" indicates that you need to apply some additional operators to these terms in order to get the dimensions to work out, such as permuting/transposing axes, dilating, changing the `padding=` argument to the convolution function, or permuting/transposing axes of the resulting convolution.\n","\n","As we saw on the [last few slides here](https://dlsyscourse.org/slides/conv_nets.pdf) in class, the transpose of a convolution can be found by simply flipping the kernel. Since we're working in 2D instead of 1D, this means flipping the kernel both vertically and horizontally (thus why we implemented `flip`).\n","\n","Summarizing some hints for both `X.grad` and `W.grad`:\n","\n","`X.grad`\n","- The convolution of `out_grad` and `W`, with some operations applied to those\n","- `W` should be flipped over both the kernel dimensions\n","- If the convolution is strided, increase the size of `out_grad` with a corresponding dilation\n","- Do an example to analyze dimensions: note the shape you want for `X.grad`, and think about how you must permute/transpose the arguments and add padding to the convolution to achieve this shape\n","    - This padding depends on both the kernel size and the `padding` argument to the convolution\n","\n","`W.grad`\n","- The convolution of `X` and `out_grad`, with some operations applied to those\n","- The gradients of `W` must be accumulated over the batches; how can you make the conv operator itself do this accumulation?\n","    - Consider turning batches into channels via transpose/permute\n","- Analyze dimensions: how can you modify `X` and `out_grad` so that the shape of their convolution matches the shape of `W`? You may need to transpose/permute the result.\n","    - Remember to account for the `padding` argument passed to convolution\n","\n","General tips\n","- Deal with strided convolutions last (you should be able to just drop in `dilate` when you've passed most of the tests)\n","- Start with the case where `padding=0`, then consider changing `padding` arguments\n","- You can \"permute\" axes with multiple calls to `transpose`\n","\n","It might also be useful to skip ahead to nn.Conv, pass the forward tests, and then use both the tests below and the nn.Conv backward tests to debug your implementation."]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"km9rtGwzucAT","executionInfo":{"status":"ok","timestamp":1763015375392,"user_tz":300,"elapsed":9338,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"7cb185e8-f56c-4ea1-bff4-21103eb4e385"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1769 deselected / 34 selected                           \u001b[0m\n","\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [  2%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  5%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [  8%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 11%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 17%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 20%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 23%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 26%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 29%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 32%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 38%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 41%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 44%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 47%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 52%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 55%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 58%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 61%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 67%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 73%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 76%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 79%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 82%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 88%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 91%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 94%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [ 97%]\u001b[0m\n","tests/hw4/test_conv.py::test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e...5e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e...5e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x1317a04bccb0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x1317a04bccb0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1317a0d0a780>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x1317a0d0a780>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x1317a04bccb0>\n","a = NDArray([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-2.18507862e+01  7.10567322e+01 -2.19253067e+02 ...  1.22643860e+02\n","     1.09581779e+02  5.29599533e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x1317a04bccb0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 1\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109....98     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109....98     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109...     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b0710>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109...     128.84021   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6b0710>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b39b0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b39b0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6b0710>\n","a = NDArray([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109.12486 ...21   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 107.25448      31.246029   -213.92354    ...  -29.267391\n","      90.55236      20.23182   ]\n","   [-109.12486 ...21   ]\n","   [ 243.11641     201.83783      41.88371    ...  121.594315\n","     127.64035      28.411123  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b0710>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 2\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...1e+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...1e+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f627dd0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-7.55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f627dd0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b3aa0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b3aa0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f627dd0>\n","a = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...55530777e+01 -3.63743286e+01  5.69509811e+01 ...  6.93700714e+01\n","    -3.12737598e+01 -2.82451649e+01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f627dd0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e...8e+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e...8e+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493...+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68ef00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493...+02]\n","   [ 3.72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f68ef00>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68d070>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68d070>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f68ef00>\n","a = NDArray([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e+01]\n"," ...72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 5.73456383e+00 -2.22225971e+01  2.64719360e+02 ... -6.97373962e+01\n","    -3.79622650e+02  9.98186493e+01]\n"," ...72021103e+01  8.33753281e+01 -4.17866564e+00 ... -1.67722427e+02\n","    -1.28576309e+02 -4.60030090e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68ef00>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595... -8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595... -8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.1759...8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff7d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.1759...8.201235    54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff7d0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6fd8e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6fd8e0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff7d0>\n","a = NDArray([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595   -10...54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-346.07162     -1.1081867  420.15115   ...  -58.583153\n","     -66.19938     33.158627 ]\n","   [ -28.17595   -10...54.58357  ]\n","   [ -17.069721  -119.112015    24.423635  ...   47.367493\n","     -38.16554     92.47348  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff7d0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621  ...-120.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621  ...-120.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621 ...20.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5100>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621 ...20.10991     75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5100>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c6bd0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c6bd0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5100>\n","a = NDArray([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621     348... 75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -21.850786    71.05673   -219.25307   ...  122.64386\n","     109.58178     52.959953 ]\n","   [-294.9621     348... 75.945595 ]\n","   [ 372.3654    -347.1187     253.12747   ...  220.83806\n","     113.929146  -171.54272  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5100>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 1\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e...9e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e...9e+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b2150>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192...+02]\n","   [ 1.20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6b2150>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b2f90>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b2f90>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6b2150>\n","a = NDArray([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.07254478e+02  3.12460289e+01 -2.13923538e+02 ... -2.92673912e+01\n","     9.05523605e+01  2.02318192e+01]\n"," ...20957283e+02 -4.53491516e+02  7.81254578e+01 ...  1.71220749e+02\n","    -1.71809418e+02  2.06600098e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b2150>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 2\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...3e+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e...3e+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c7560>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520...+02]\n","   [-8.42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6c7560>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c5b20>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c5b20>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6c7560>\n","a = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-6.38349190e+01  1.26404709e+02 -6.92413330e+01 ...  7.84971542e+01\n","     5.02614059e+01 -1.00136520e+02]\n"," ...42394543e+00 -1.02299728e+02 -4.70671501e+01 ...  7.48113098e+01\n","     7.02560806e+01  8.07628021e+01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c7560>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625 ... 143.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625 ... 143.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625...43.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6fc140>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625...43.49785    172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6fc140>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ff4a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ff4a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6fc140>\n","a = NDArray([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625  -322....172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[   5.734564   -22.222597   264.71936   ...  -69.7374\n","    -379.62265     99.81865  ]\n","   [ -55.910625  -322....172.42424  ]\n","   [ 140.06458    -33.92796    128.48518   ...  228.07355\n","     153.4408     163.71072  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6fc140>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e...3e+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e...3e+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266...+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68f500>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266...+01]\n","   [-9.30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f68f500>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68dd00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68dd00>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f68f500>\n","a = NDArray([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e+01]\n"," ...30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[-3.46071625e+02 -1.10818672e+00  4.20151154e+02 ... -5.85831528e+01\n","    -6.61993790e+01  3.31586266e+01]\n"," ...30875702e+01 -4.32016563e+01  1.87175808e+01 ... -3.38230171e+01\n","    -5.38563805e+01  3.20688462e+00]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68f500>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n","     -0.36175704   4.6234684 ]\n","   [  5.279258... -4.82095      4.98517   ]\n","   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n","      8.537833     2.3818388 ]]]])\n","W_shape    = (3, 3, 24, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n","      4.322181    -3.7108252 ]\n","   [ 11.348773...4.207389     8.935435  ]\n","   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n","     -1.400853     0.07475674]]]])\n","Z_shape    = (3, 16, 16, 24)\n","_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n","           -0.36175704,   4.6234684 ],\n","        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n","            8.537833  ,   2.3818388 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n","            4.322181  ,  -3.7108252 ],\n","        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n","           -1.400853  ,   0.07475674]]]], dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e...4e+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e...4e+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181...+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b18e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181...+01]\n","   [-5.03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6b18e0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b31a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b31a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6b18e0>\n","a = NDArray([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e+02]\n"," ...03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 5.96437805e+02 -2.17912796e+02 -2.09804783e+01 ... -4.44460907e+02\n","    -2.74169159e+02 -6.12073181e+02]\n"," ...03057983e+02 -1.26949539e+02 -1.28574600e+01 ...  4.08393738e+02\n","    -1.71167480e+02  1.53551315e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b18e0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n","     2.17041516e+00 -3.88039827e...7e+00]\n","   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n","     4.27775383e-01 -3.84437203e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n","           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n","          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e...4e+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e...4e+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549...+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6bb290>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549...+02]\n","   [-4.85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6bb290>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6bb170>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6bb170>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6bb290>\n","a = NDArray([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e+00]\n"," ...85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.75570267e+02  2.39688141e+02  5.25702209e+02 ...  4.85040955e+02\n","    -2.37886475e+02 -2.36008549e+00]\n"," ...85305603e+02 -1.46666489e+02 -3.62995392e+02 ...  1.50065781e+02\n","    -5.45707825e+02  2.44026398e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6bb290>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n","     6.47611380e+00 -3.38525438e...7e+00]\n","   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n","     2.56555057e+00  1.63004756e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...8e+00]\n","   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n","     4.72636795e+00 -5.75466204e+00]]]])\n","Z_shape    = (3, 17, 17, 8)\n","_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n","           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n","           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n","          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e...2e+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e...2e+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637...+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c72f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637...+02]\n","   [-6.95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6c72f0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c7410>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c7410>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6c72f0>\n","a = NDArray([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e+02]\n"," ...95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 5.86050903e+02 -5.11887695e+02  5.65467911e+01 ...  7.48446426e+01\n","    -3.56891510e+02 -3.76215637e+02]\n"," ...95190125e+02 -8.10500183e+01  2.38304077e+02 ... -1.27415886e+02\n","     8.60565063e+02 -6.53100510e+01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c72f0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n","     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n","     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n","     -7.9124722 ]]]])\n","W_shape    = (5, 5, 1, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00]\n","   [ 2.00078607e+00]\n","   [ 4.89369011e+00]\n","   [ 1.12044659e+01]\n","   [ 9.33778954e+00]...22066e-01]\n","   [ 1.05310105e-01]\n","   [ 4.97272283e-01]\n","   [ 1.13696384e+00]\n","   [-5.08369303e+00]\n","   [-5.73876619e-01]]]])\n","Z_shape    = (3, 17, 17, 1)\n","_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n","           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n","           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00],\n","         [ 2.00078607e+00],\n","         [ 4.89369011e+00],\n","         [ 1.12044659e+01],\n","      ... 4.97272283e-01],\n","         [ 1.13696384e+00],\n","         [-5.08369303e+00],\n","         [-5.73876619e-01]]]], dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e...8e+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e...8e+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985...+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13188629a810>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985...+01]\n","   [ 6.41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13188629a810>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13188629a540>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13188629a540>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13188629a810>\n","a = NDArray([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e+01]\n"," ...41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 4.14443283e+01  1.16950035e+01 -4.72139893e+01 ...  1.41469421e+02\n","    -2.21567459e+01 -3.79012985e+01]\n"," ...41885986e+01  7.46171875e+01  3.42209511e+01 ...  1.39405457e+02\n","    -1.01194525e+01 -1.25883507e+02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13188629a810>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 1.55579513e-02]\n","   [-3.04658723e+00]\n","   [ 4.09725952e+00]\n","   [ 1.29906273e+00]\n","   [-7.50546598e+00]...75421e+00]\n","   [ 3.02805209e+00]\n","   [-4.20992136e+00]\n","   [ 4.11707735e+00]\n","   [ 4.99041653e+00]\n","   [ 5.11040497e+00]]]])\n","W_shape    = (5, 5, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 1.55579513e-02],\n","         [-3.04658723e+00],\n","         [ 4.09725952e+00],\n","         [ 1.29906273e+00],\n","      ...-4.20992136e+00],\n","         [ 4.11707735e+00],\n","         [ 4.99041653e+00],\n","         [ 5.11040497e+00]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.9615...44982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.9615...44982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.961...982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68d220>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.961...982 ]\n","   [  201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f68d220>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68f290>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68f290>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f68d220>\n","a = NDArray([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.96155 ]\n","  ...201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[  173.23763 ]\n","   [ -598.2368  ]\n","   [-1057.3813  ]\n","   [  300.51642 ]\n","   [  -78.8975  ]\n","   [  272.96155 ]\n","  ...201.49226 ]\n","   [ -241.77127 ]\n","   [  538.54675 ]\n","   [  257.22565 ]\n","   [  163.92024 ]\n","   [ 1022.2947  ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68d220>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 0.01555795]\n","   [-3.0465872 ]\n","   [ 4.0972595 ]\n","   [ 1.2990627 ]\n","   [-7.505466  ]\n","   [-1.7335238 ]\n","  ...[ 5.6893935 ]\n","   [ 2.267663  ]\n","   [-1.0709513 ]\n","   [-5.0566583 ]\n","   [-0.23777105]\n","   [-9.931785  ]\n","   [-0.44905776]]]])\n","W_shape    = (1, 1, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 0.01555795],\n","         [-3.0465872 ],\n","         [ 4.0972595 ],\n","         [ 1.2990627 ],\n","         [-7.505466  ]...13 ],\n","         [-5.0566583 ],\n","         [-0.23777105],\n","         [-9.931785  ],\n","         [-0.44905776]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154....]\n","   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154....]\n","   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154...   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5250>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154...   [ -53.193153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5250>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c51f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c51f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5250>\n","a = NDArray([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154.07153 ...153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -89.18882   ]\n","   [  65.9584    ]\n","   [ -71.3126    ]\n","   [  94.12995   ]\n","   [  58.195744  ]\n","   [-154.07153 ...153  ]\n","   [  18.765165  ]\n","   [   0.41209608]\n","   [-161.62552   ]\n","   [  16.99903   ]\n","   [ 227.20952   ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c5250>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0] _\u001b[0m\n","\n","Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n","backward = True, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n","   [ -1.4591868   -3.807461  ]]\n","\n","  [[  4.2896194    5.705509  ]\n","   [  7.3...7526     2.911123  ]\n","   [-10.473016     0.61860955]]\n","\n","  [[ -0.65053475   0.46976614]\n","   [  4.7152305  -13.698386  ]]]])\n","W_shape    = (3, 3, 2, 2)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   [  4.750...19315276  -8.283575  ]\n","   [ -4.9275537   -7.359175  ]\n","   [  8.240675     0.8211388 ]\n","   [  2.8364513   -1.1133755 ]]]])\n","Z_shape    = (1, 14, 14, 2)\n","_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n","         [ -1.4591868 ,  -3.807461  ]],\n","\n","        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n","\n","        [[ -0.65053475,   0.46976614],\n","         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...275537 ,  -7.359175  ],\n","         [  8.240675  ,   0.8211388 ],\n","         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n","backward   = True\n","device     = cpu()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ ...   228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ ...   228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [... 228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc9e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [... 228.99751   ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc9e0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ced20>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ced20>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc9e0>\n","a = NDArray([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ -41.20... ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -83.14382     -46.58788   ]\n","   [  -6.3601565    -5.1046524 ]\n","   [  93.87601     -20.797268  ]\n","   [ -41.20... ]\n","   [  41.437016      8.441346  ]\n","   [ -26.093277   -121.97922   ]\n","   [  22.205511     26.16009   ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc9e0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cfc80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6cfc80>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ce6c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ce6c0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6cfc80>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cfc80>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 1\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 1\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5940>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5940>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e7f50>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e7f50>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5940>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5940>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 1, padding = 2\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 2\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5580>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5580>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e7950>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e7950>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5580>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5580>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611 ....\n","      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611 ....\n","      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611...      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cf320>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611...      0.           0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6cf320>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6cdd00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6cdd00>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6cf320>\n","a = NDArray([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611    -2....      0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ -0.425578    -2.821505     4.83384    ...  -4.311335\n","      0.8033059   -4.7632246 ]\n","   [  8.042611    -2....      0.        ]\n","   [  0.           0.           0.         ...   0.\n","      0.           0.        ]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cf320>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68e0f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f68e0f0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68d190>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68d190>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f68e0f0>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68e0f0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. ... 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. ... 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0..... 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5550>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0..... 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5550>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e6b70>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e6b70>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5550>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","  ...0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","  ...0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5550>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 1\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  4.282669     8.133604    -4.6223483  ...   2.0798197\n","      2.1704152   -0.38803983]\n","   [  2.245844...2.618842     0.71487164]\n","   [ -4.09374      3.7291534  -10.551835   ...   0.31741843\n","      0.03223436   2.0512383 ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[  4.282669  ,   8.133604  ,  -4.6223483 , ...,   2.0798197 ,\n","            2.1704152 ,  -0.38803983],\n","        ... [ -4.09374   ,   3.7291534 , -10.551835  , ...,   0.31741843,\n","            0.03223436,   2.0512383 ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 1\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e50a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6e50a0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e5940>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e5940>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6e50a0>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e50a0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 16), stride = 2, padding = 2\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  0.40910086  -0.64982927  11.320294   ...  -0.12077556\n","      1.428692    -5.5845156 ]\n","   [ -6.89178...-1.5437248  -10.136281  ]\n","   [  1.1511405    0.7441038   16.463472   ...  -6.8931007\n","     -0.5833115   -2.504636  ]]]])\n","W_shape    = (3, 3, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[  0.40910086,  -0.64982927,  11.320294  , ...,  -0.12077556,\n","            1.428692  ,  -5.5845156 ],\n","        ... [  1.1511405 ,   0.7441038 ,  16.463472  , ...,  -6.8931007 ,\n","           -0.5833115 ,  -2.504636  ]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 2\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5670>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5670>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e4d10>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6e4d10>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5670>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6e5670>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 8), W_shape = (3, 3, 8, 14), stride = 2, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.09100860e-01 -6.49829268e-01  1.13202944e+01 ... -5.56089449e+00\n","    -4.84385538e+00 -1.20775558e...5e+00]\n","   [-1.10714078e+00  2.54448557e+00 -7.73867989e+00 ... -1.13336074e+00\n","    -1.21658230e+00 -4.79330921e+00]]]])\n","W_shape    = (3, 3, 8, 14)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...5e+00]\n","   [ 6.10367346e+00 -2.05434513e+00 -4.41384506e+00 ... -6.15245628e+00\n","     3.33514667e+00 -1.35630560e+00]]]])\n","Z_shape    = (3, 16, 16, 8)\n","_W         = array([[[[ 4.09100860e-01, -6.49829268e-01,  1.13202944e+01, ...,\n","          -5.56089449e+00, -4.84385538e+00, -1.20775...448557e+00, -7.73867989e+00, ...,\n","          -1.13336074e+00, -1.21658230e+00, -4.79330921e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...434513e+00, -4.41384506e+00, ...,\n","          -6.15245628e+00,  3.33514667e+00, -1.35630560e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b1d30>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6b1d30>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b31a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6b31a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6b1d30>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6b1d30>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 2), W_shape = (3, 3, 2, 14), stride = 2, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -8.0968      -2.5552022    8.703147    -1.4674252    4.5861077\n","     -0.28521433   4.383634    -9.13...727333   0.22582927   9.296732    -8.13161     -0.67411226\n","     -2.9204676    1.675528   -12.187821     5.5746226 ]]]])\n","W_shape    = (3, 3, 2, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   ...\n","   [...   -5.112822  ]\n","   ...\n","   [ -7.922368     4.2222714 ]\n","   [ -6.064339     1.4188478 ]\n","   [ -1.4109794   -5.791016  ]]]])\n","Z_shape    = (3, 16, 16, 2)\n","_W         = array([[[[ -8.0968    ,  -2.5552022 ,   8.703147  ,  -1.4674252 ,\n","            4.5861077 ,  -0.28521433,   4.383634  , ...        -8.13161   ,  -0.67411226,  -2.9204676 ,   1.675528  ,\n","          -12.187821  ,   5.5746226 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...22368  ,   4.2222714 ],\n","         [ -6.064339  ,   1.4188478 ],\n","         [ -1.4109794 ,  -5.791016  ]]]], dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc830>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc830>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ced20>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ced20>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc830>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cc830>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0] _\u001b[0m\n","\n","Z_shape = (3, 16, 16, 24), W_shape = (3, 3, 24, 14), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.3245817    1.0111231   -1.9236585  ...  -2.5907016\n","     -0.36175704   4.6234684 ]\n","   [  5.279258... -4.82095      4.98517   ]\n","   [  3.2300415   -2.4161792   -3.420825   ...  -9.444658\n","      8.537833     2.3818388 ]]]])\n","W_shape    = (3, 3, 24, 14)\n","Z          = needle.Tensor([[[[  8.820262     2.000786     4.89369    ...   3.2680929\n","      4.322181    -3.7108252 ]\n","   [ 11.348773...4.207389     8.935435  ]\n","   [  2.5084004    3.1399443   -0.12958507 ...   0.13740699\n","     -1.400853     0.07475674]]]])\n","Z_shape    = (3, 16, 16, 24)\n","_W         = array([[[[ -1.3245817 ,   1.0111231 ,  -1.9236585 , ...,  -2.5907016 ,\n","           -0.36175704,   4.6234684 ],\n","        ... [  3.2300415 ,  -2.4161792 ,  -3.420825  , ...,  -9.444658  ,\n","            8.537833  ,   2.3818388 ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ,   4.89369   , ...,   3.2680929 ,\n","            4.322181  ,  -3.7108252 ],\n","        ... [  2.5084004 ,   3.1399443 ,  -0.12958507, ...,   0.13740699,\n","           -1.400853  ,   0.07475674]]]], dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68c680>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f68c680>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68c0b0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f68c0b0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f68c680>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f68c680>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0] _\u001b[0m\n","\n","Z_shape = (3, 14, 14, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 4.28266907e+00  8.13360405e+00 -4.62234831e+00 ...  2.07981968e+00\n","     2.17041516e+00 -3.88039827e...7e+00]\n","   [-7.86027253e-01  4.30947495e+00 -4.97804356e+00 ... -5.69430470e-01\n","     4.27775383e-01 -3.84437203e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...3e+00]\n","   [ 1.09905624e+01  2.23166728e+00  4.62055349e+00 ... -6.23090088e-01\n","    -8.48226726e-01 -2.29984760e+00]]]])\n","Z_shape    = (3, 14, 14, 8)\n","_W         = array([[[[ 4.28266907e+00,  8.13360405e+00, -4.62234831e+00, ...,\n","           2.07981968e+00,  2.17041516e+00, -3.88039...947495e+00, -4.97804356e+00, ...,\n","          -5.69430470e-01,  4.27775383e-01, -3.84437203e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...166728e+00,  4.62055349e+00, ...,\n","          -6.23090088e-01, -8.48226726e-01, -2.29984760e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6ced80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6ced80>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ccbf0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ccbf0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6ced80>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6ced80>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 8), W_shape = (5, 5, 8, 16), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 5.93117094e+00 -4.61600447e+00 -7.43411541e+00 ...  9.49053860e+00\n","     6.47611380e+00 -3.38525438e...7e+00]\n","   [-3.93575430e+00 -6.98609781e+00  1.63659811e+00 ...  8.22474575e+00\n","     2.56555057e+00  1.63004756e+00]]]])\n","W_shape    = (5, 5, 8, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ... -4.88638926e+00\n","     4.75044203e+00 -7.56786048e...8e+00]\n","   [ 2.80155873e+00  3.11298299e+00 -3.24756050e+00 ... -7.96233535e-01\n","     4.72636795e+00 -5.75466204e+00]]]])\n","Z_shape    = (3, 17, 17, 8)\n","_W         = array([[[[ 5.93117094e+00, -4.61600447e+00, -7.43411541e+00, ...,\n","           9.49053860e+00,  6.47611380e+00, -3.38525...609781e+00,  1.63659811e+00, ...,\n","           8.22474575e+00,  2.56555057e+00,  1.63004756e+00]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","          -4.88638926e+00,  4.75044203e+00, -7.56786...298299e+00, -3.24756050e+00, ...,\n","          -7.96233535e-01,  4.72636795e+00, -5.75466204e+00]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6da330>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6da330>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6d9490>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6d9490>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6da330>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6da330>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 1), W_shape = (5, 5, 1, 16), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[  1.5437562   -6.8538       4.3282647    5.4068804   -3.15688\n","     -1.206689    -4.3909516    3.4969...448   -2.242325     0.6578698\n","     -7.0278      -1.7489109   10.11736      2.5269346    1.7962458\n","     -7.9124722 ]]]])\n","W_shape    = (5, 5, 1, 16)\n","Z          = needle.Tensor([[[[ 8.82026196e+00]\n","   [ 2.00078607e+00]\n","   [ 4.89369011e+00]\n","   [ 1.12044659e+01]\n","   [ 9.33778954e+00]...22066e-01]\n","   [ 1.05310105e-01]\n","   [ 4.97272283e-01]\n","   [ 1.13696384e+00]\n","   [-5.08369303e+00]\n","   [-5.73876619e-01]]]])\n","Z_shape    = (3, 17, 17, 1)\n","_W         = array([[[[  1.5437562 ,  -6.8538    ,   4.3282647 ,   5.4068804 ,\n","           -3.15688   ,  -1.206689  ,  -4.3909516 , ...  -7.0278    ,  -1.7489109 ,\n","           10.11736   ,   2.5269346 ,   1.7962458 ,  -7.9124722 ]]]],\n","      dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00],\n","         [ 2.00078607e+00],\n","         [ 4.89369011e+00],\n","         [ 1.12044659e+01],\n","      ... 4.97272283e-01],\n","         [ 1.13696384e+00],\n","         [-5.08369303e+00],\n","         [-5.73876619e-01]]]], dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cef00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6cef00>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ce4b0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ce4b0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6cef00>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6cef00>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (5, 5, 16, 1), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 1.55579513e-02]\n","   [-3.04658723e+00]\n","   [ 4.09725952e+00]\n","   [ 1.29906273e+00]\n","   [-7.50546598e+00]...75421e+00]\n","   [ 3.02805209e+00]\n","   [-4.20992136e+00]\n","   [ 4.11707735e+00]\n","   [ 4.99041653e+00]\n","   [ 5.11040497e+00]]]])\n","W_shape    = (5, 5, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 1.55579513e-02],\n","         [-3.04658723e+00],\n","         [ 4.09725952e+00],\n","         [ 1.29906273e+00],\n","      ...-4.20992136e+00],\n","         [ 4.11707735e+00],\n","         [ 4.99041653e+00],\n","         [ 5.11040497e+00]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]...]\n","   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]...]\n","   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff2f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...   [0.]]\n","\n","  [[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff2f0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6fc890>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6fc890>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff2f0>\n","a = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]\n","\n","  [[...[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]\n","\n","  [[...[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6ff2f0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0] _\u001b[0m\n","\n","Z_shape = (3, 17, 17, 16), W_shape = (1, 1, 16, 1), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ 0.01555795]\n","   [-3.0465872 ]\n","   [ 4.0972595 ]\n","   [ 1.2990627 ]\n","   [-7.505466  ]\n","   [-1.7335238 ]\n","  ...[ 5.6893935 ]\n","   [ 2.267663  ]\n","   [-1.0709513 ]\n","   [-5.0566583 ]\n","   [-0.23777105]\n","   [-9.931785  ]\n","   [-0.44905776]]]])\n","W_shape    = (1, 1, 16, 1)\n","Z          = needle.Tensor([[[[ 8.82026196e+00  2.00078607e+00  4.89369011e+00 ...  6.08375072e-01\n","     2.21931624e+00  1.66837168e...3e+00]\n","   [-1.25365603e+00 -1.71452928e+00  1.35930243e+01 ...  3.13170171e+00\n","    -1.35708165e+00  1.29997072e+01]]]])\n","Z_shape    = (3, 17, 17, 16)\n","_W         = array([[[[ 0.01555795],\n","         [-3.0465872 ],\n","         [ 4.0972595 ],\n","         [ 1.2990627 ],\n","         [-7.505466  ]...13 ],\n","         [-5.0566583 ],\n","         [-0.23777105],\n","         [-9.931785  ],\n","         [-0.44905776]]]], dtype=float32)\n","_Z         = array([[[[ 8.82026196e+00,  2.00078607e+00,  4.89369011e+00, ...,\n","           6.08375072e-01,  2.21931624e+00,  1.66837...452928e+00,  1.35930243e+01, ...,\n","           3.13170171e+00, -1.35708165e+00,  1.29997072e+01]]]],\n","      dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","...0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","...0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c51f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]...]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6c51f0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c6120>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6c6120>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6c51f0>\n","a = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0....[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0....[0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]\n","   [0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6c51f0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0] _\u001b[0m\n","\n","Z_shape = (1, 14, 14, 2), W_shape = (3, 3, 2, 2), stride = 1, padding = 0\n","backward = True, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mZ_shape, W_shape, stride, padding\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, op_conv_shapes)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, [\u001b[94mTrue\u001b[39;49;00m, \u001b[94mFalse\u001b[39;49;00m], ids=[\u001b[33m\"\u001b[39;49;00m\u001b[33mbackward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[33m\"\u001b[39;49;00m\u001b[33mforward\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_op_conv\u001b[39;49;00m(Z_shape, W_shape, stride, padding, backward, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = np.random.randn(*Z_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _Z = _Z.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        _W = np.random.randn(*W_shape)*\u001b[94m5\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        _W = _W.astype(np.float32)\u001b[90m\u001b[39;49;00m\n","        Z = ndl.Tensor(_Z, device=device)\u001b[90m\u001b[39;49;00m\n","        W = ndl.Tensor(_W, device=device)\u001b[90m\u001b[39;49;00m\n","        y = ndl.conv(Z, W, padding=padding, stride=stride)\u001b[90m\u001b[39;49;00m\n",">       y2 = y.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","W          = needle.Tensor([[[[ -1.7671587   -8.082371  ]\n","   [ -1.4591868   -3.807461  ]]\n","\n","  [[  4.2896194    5.705509  ]\n","   [  7.3...7526     2.911123  ]\n","   [-10.473016     0.61860955]]\n","\n","  [[ -0.65053475   0.46976614]\n","   [  4.7152305  -13.698386  ]]]])\n","W_shape    = (3, 3, 2, 2)\n","Z          = needle.Tensor([[[[  8.820262     2.000786  ]\n","   [  4.89369     11.204466  ]\n","   [  9.33779     -4.8863893 ]\n","   [  4.750...19315276  -8.283575  ]\n","   [ -4.9275537   -7.359175  ]\n","   [  8.240675     0.8211388 ]\n","   [  2.8364513   -1.1133755 ]]]])\n","Z_shape    = (1, 14, 14, 2)\n","_W         = array([[[[ -1.7671587 ,  -8.082371  ],\n","         [ -1.4591868 ,  -3.807461  ]],\n","\n","        [[  4.2896194 ,   5.705509  ],...016  ,   0.61860955]],\n","\n","        [[ -0.65053475,   0.46976614],\n","         [  4.7152305 , -13.698386  ]]]], dtype=float32)\n","_Z         = array([[[[  8.820262  ,   2.000786  ],\n","         [  4.89369   ,  11.204466  ],\n","         [  9.33779   ,  -4.8863893 ],\n"," ...275537 ,  -7.359175  ],\n","         [  8.240675  ,   0.8211388 ],\n","         [  2.8364513 ,  -1.1133755 ]]]], dtype=float32)\n","backward   = True\n","device     = cuda()\n","padding    = 0\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","y          = needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.9164972...+00]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:430: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.9164972...+00]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.916497...0]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6fcec0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.916497...0]\n","   [ 5.68112946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0x13179f6fcec0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ffec0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0x13179f6ffec0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x13179f6fcec0>\n","a = NDArray([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.91649723e+00]...12946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.13085880e+01  3.24930787e+00]\n","   [-3.75750160e+00 -5.10143757e+00]\n","   [-1.76881945e+00  1.91649723e+00]...12946e+00 -3.51979017e+00]\n","   [ 4.05532885e+00  3.09993196e+00]\n","   [-6.08671856e+00  3.71654844e+00]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0x13179f6fcec0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape0-W_shape0-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape1-W_shape1-1-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape2-W_shape2-1-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape3-W_shape3-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape4-W_shape4-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape5-W_shape5-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape6-W_shape6-2-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape7-W_shape7-2-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape8-W_shape8-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape9-W_shape9-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape10-W_shape10-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape11-W_shape11-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape12-W_shape12-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape13-W_shape13-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape14-W_shape14-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape15-W_shape15-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cpu-Z_shape16-W_shape16-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape0-W_shape0-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape1-W_shape1-1-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape2-W_shape2-1-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape3-W_shape3-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape4-W_shape4-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape5-W_shape5-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape6-W_shape6-2-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape7-W_shape7-2-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape8-W_shape8-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape9-W_shape9-2-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape10-W_shape10-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape11-W_shape11-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape12-W_shape12-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape13-W_shape13-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape14-W_shape14-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape15-W_shape15-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_op_conv[backward-needle.backend_ndarray.ndarray_backend_cuda-Z_shape16-W_shape16-1-0]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31m===================== \u001b[31m\u001b[1m34 failed\u001b[0m, \u001b[33m1769 deselected\u001b[0m\u001b[31m in 7.45s\u001b[0m\u001b[31m ======================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"op_conv and backward\""]},{"cell_type":"markdown","metadata":{"id":"sQlUNK_ducAT"},"source":["-----------------"]},{"cell_type":"markdown","metadata":{"id":"cxqyhrVRucAU"},"source":["### nn.Conv"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"lAeCQKfKucAU"},"source":["#### Fixing init._calculate_fans for convolution\n","Previously, we have implemented Kaiming uniform/normal initializations, where we essentially assigned `fan_in = input_size` and `fan_out = output_size`.\n","For convolution, this becomes somewhat more detailed, in that you should multiply both of these by the \"receptive field size\", which is in this case just the product of the kernel sizes -- which in our case are always going to be the same, i.e., $k\\times k$ kernels.\n","\n","**You will need to edit your `kaiming_uniform` in `python/needle/init/init_initializers.py`, etc. init functions to support multidimensional arrays.** In particular, it should support a new `shape` argument which is then passed to, e.g., the underlying `rand` function. Specifically, if the argument `shape` is not `None`, then ignore `fan_in` and `fan_out`, and use the value of `shape` for initializations instead.\n","\n","You can test this below; though it is not _directly_ graded, it must match ours to pass the nn.Conv mugrade tests."]},{"cell_type":"code","execution_count":39,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qn_KUYR7ucAU","executionInfo":{"status":"ok","timestamp":1763015379520,"user_tz":300,"elapsed":4127,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"c6308091-7f14-458d-a196-7e09b322af79"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\n","\n","tests/hw4/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cpu] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m____ test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cpu] _____\u001b[0m\n","\n","device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_init_kaiming_uniform\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(\u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.init.kaiming_uniform(\u001b[94m16\u001b[39;49;00m*\u001b[94m9\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m*\u001b[94m9\u001b[39;49;00m, shape=A.shape)\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mabs\u001b[39;49;00m(A.sum().numpy() - -\u001b[94m2.5719218\u001b[39;49;00m) < \u001b[94m1e-4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0....2 -0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]])\n","_A         = array([[[[ 1.78862847e+00,  4.36509851e-01,  9.64974681e-02, ...,\n","          -3.54758979e-01, -8.27414815e-02, -6.27000...1.97471091e-01, -3.35760446e-01, -1.52273581e+00, ...,\n","           1.04328943e+00, -1.29098626e-01, -4.98050186e-01]]]])\n","device     = cpu()\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:170: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0....2 -0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0...-0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd55f4de3260>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:241: in make_from_op\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m tensor.detach()\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0...-0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd55f4de3260>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd55f4d749e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:273: in detach\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_const(\u001b[96mself\u001b[39;49;00m.realize_cached_data())\u001b[90m\u001b[39;49;00m\n","                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd55f4d749e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd55f4d749e0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd55f4de3260>\n","a = NDArray([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0.047584...     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0.047584...     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd55f4de3260>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m____ test_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cuda] ____\u001b[0m\n","\n","device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_init_kaiming_uniform\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(\u001b[94m3\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m16\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.init.kaiming_uniform(\u001b[94m16\u001b[39;49;00m*\u001b[94m9\u001b[39;49;00m, \u001b[94m8\u001b[39;49;00m*\u001b[94m9\u001b[39;49;00m, shape=A.shape)\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mabs\u001b[39;49;00m(A.sum().numpy() - -\u001b[94m2.5719218\u001b[39;49;00m) < \u001b[94m1e-4\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                   ^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","A          = needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0....2 -0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]])\n","_A         = array([[[[-7.62114512e-01, -8.87780137e-01,  9.36398544e-01, ...,\n","          -8.01496885e-01, -6.47181432e-01,  4.72247...2.62466179e-02, -1.14335160e-01,  7.43553516e-01, ...,\n","           1.36606007e+00,  1.55511403e+00,  6.13326226e-01]]]])\n","device     = cuda()\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:170: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0....2 -0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0...-0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd55f4de24e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:241: in make_from_op\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m tensor.detach()\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0...-0.11381223\n","     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd55f4de24e0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd55f4b0ef60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:273: in detach\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_const(\u001b[96mself\u001b[39;49;00m.realize_cached_data())\u001b[90m\u001b[39;49;00m\n","                             ^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd55f4b0ef60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd55f4b0ef60>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd55f4de24e0>\n","a = NDArray([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0.047584...     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 0.01992804  0.08785069  0.04195297 ...  0.05956101 -0.02547991\n","     0.15994066]\n","   [ 0.18928954 -0.047584...     0.07500601]\n","   [-0.16230063 -0.04203904 -0.09118237 ...  0.08426678 -0.19409062\n","     0.05469996]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd55f4de24e0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cpu]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_init_kaiming_uniform[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31m====================== \u001b[31m\u001b[1m2 failed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 2.43s\u001b[0m\u001b[31m ======================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"kaiming_uniform\""]},{"cell_type":"markdown","metadata":{"id":"7rbPSUwgucAU"},"source":["#### Implementing nn.Conv\n","\n","Essentially, nn.Conv is just a wrapper of the convolution operator we previously implemented\n","which adds a bias term, initializes the weight and bias, and ensures that the padding is set so that the input and output dimensions are the same (in the `stride=1` case, anyways).\n","\n","Importantly, nn.Conv should support NCHW format instead of NHWC format. In particular, we think this makes more sense given our current BatchNorm implementation. You can implement this by applying `transpose` twice to both the input and output.  \n","\n","- Ensure nn.Conv works for `(N, C, H, W)` tensors even though we implemented the conv op for `(N, H, W, C)` tensors\n","- Initialize the `(k, k, i, o)` weight tensor using Kaiming uniform initialization with default settings\n","- Initialize the `(o,)` bias tensor using uniform initialization on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|in_channels| \\times \\verb|kernel_size|^2}}$\n","- Calculate the appropriate padding to ensure input and output dimensions are the same\n","- Calculate the convolution, then add the properly-broadcasted bias term if present\n","\n","You can now test your nn.Conv against PyTorch's nn.Conv2d with the two PyTest calls below."]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hevzfoN8ucAU","executionInfo":{"status":"ok","timestamp":1763015385645,"user_tz":300,"elapsed":6123,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"2be3cab3-ba3c-4a5f-d73b-8d8a267ebe9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1793 deselected / 10 selected                           \u001b[0m\n","\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-4-8-16-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 10%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-16-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-8-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 30%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-1] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cpu-32-16-8-3-2] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 60%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 70%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 80%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 90%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1] _\u001b[0m\n","\n","s = 4, cin = 8, cout = 16, k = 3, stride = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m10\u001b[39;49;00m, cin, s, s, device=device)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(f(x).cached_data.numpy() - g(z).data.numpy()) < \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float32(28.731468) < 0.001\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float32(28.731468) = <function norm at 0xdd1175c3a30>((array([[[[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]]],\\n\\n\\n       [[[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., ...       [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]]],\\n\\n\\n       [[[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]]]], dtype=float32) - array([[[[0.66017354, 0.66017354, 0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, 0.66017354]],\\n\\n        [[0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ]],\\n\\n        [[0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ]],\\n\\n        ...,\\n\\n        [[0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188]],\\n\\n        [[0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.4358649...00776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ]],\\n\\n        [[0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ]],\\n\\n        ...,\\n\\n        [[0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188]],\\n\\n        [[0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493]],\\n\\n        [[0.89192337, 0.89192337, 0.89192337, 0.89192337],\\n         [0.89192337, 0.89192337, 0.89192337, 0.89192337],\\n         [0.89192337, 0.89192337, 0.89192337, 0.89192337],\\n         [0.89192337, 0.89192337, 0.89192337, 0.89192337]]]],\\n      dtype=float32)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0xdd1175c3a30> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]]],\\n\\n\\n       [[[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., ...       [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]]],\\n\\n\\n       [[[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]],\\n\\n        [[0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.],\\n         [0., 0., 0., 0.]]]], dtype=float32) = numpy()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where numpy = NDArray([[[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0...]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]], device=cuda()).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where NDArray([[[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0...]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]], device=cuda()) = needle.Tensor([[[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0...   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]]).cached_data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where needle.Tensor([[[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0...   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]]) = <needle.nn.nn_conv.Conv object at 0xdd034cab650>(needle.Tensor([[[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0...   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]\\n\\n  [[0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]\\n   [0. 0. 0. 0.]]]]))\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0.66017354, 0.66017354, 0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, 0.66017354]],\\n\\n        [[0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ]],\\n\\n        [[0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ]],\\n\\n        ...,\\n\\n        [[0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188]],\\n\\n        [[0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.4358649...00776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , 0.2900776 ]],\\n\\n        [[0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ],\\n         [0.6180154 , 0.6180154 , 0.6180154 , 0.6180154 ]],\\n\\n        ...,\\n\\n        [[0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188],\\n         [0.36756188, 0.36756188, 0.36756188, 0.36756188]],\\n\\n        [[0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493],\\n         [0.43586493, 0.43586493, 0.43586493, 0.43586493]],\\n\\n        [[0.89192337, 0.89192337, 0.89192337, 0.89192337],\\n         [0.89192337, 0.89192337, 0.89192337, 0.89192337],\\n         [0.89192337, 0.89192337, 0.89192337, 0.89192337],\\n         [0.89192337, 0.89192337, 0.89192337, 0.89192337]]]],\\n      dtype=float32) = <built-in method numpy of Tensor object at 0xdd0597a0fa0>()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in method numpy of Tensor object at 0xdd0597a0fa0> = tensor([[[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602....8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]]]).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where tensor([[[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602....8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]]]) = tensor([[[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602...  [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]]], grad_fn=<ConvolutionBackward0>).data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where tensor([[[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602...  [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]],\\n\\n\\n        [[[0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180, 0.6180]],\\n\\n         ...,\\n\\n         [[0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676],\\n          [0.3676, 0.3676, 0.3676, 0.3676]],\\n\\n         [[0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359],\\n          [0.4359, 0.4359, 0.4359, 0.4359]],\\n\\n         [[0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919],\\n          [0.8919, 0.8919, 0.8919, 0.8919]]]], grad_fn=<ConvolutionBackward0>) = Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(tensor([[[[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]]],\\n\\n\\n        [[[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.]...       [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]]],\\n\\n\\n        [[[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]],\\n\\n         [[0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.],\\n          [0., 0., 0., 0.]]]]))\u001b[0m\n","\n","cin        = 8\n","cout       = 16\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xdd034cab650>\n","g          = Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","s          = 4\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]\n","\n","  [[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.]],\n","\n","    ...]],\n","\n","         [[0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:357: AssertionError\n","\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2] _\u001b[0m\n","\n","s = 32, cin = 8, cout = 16, k = 3, stride = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m10\u001b[39;49;00m, cin, s, s, device=device)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(f(x).cached_data.numpy() - g(z).data.numpy()) < \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float32(124.36587) < 0.001\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float32(124.36587) = <function norm at 0xdd1175c3a30>((array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) - array([[[[0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         ...,\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245]],\\n\\n        [[0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         ...,\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.584...97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         ...,\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513]],\\n\\n        [[0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         ...,\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526]]]], dtype=float32)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0xdd1175c3a30> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) = numpy()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where numpy = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()) = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]).cached_data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]) = <needle.nn.nn_conv.Conv object at 0xdd11ad0b830>(needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]))\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         ...,\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245]],\\n\\n        [[0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         ...,\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.584...97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         ...,\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513],\\n         [0.97749513, 0.97749513, 0.97749513, ..., 0.97749513,\\n          0.97749513, 0.97749513]],\\n\\n        [[0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         ...,\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526],\\n         [0.87650526, 0.87650526, 0.87650526, ..., 0.87650526,\\n          0.87650526, 0.87650526]]]], dtype=float32) = <built-in method numpy of Tensor object at 0xdd034183ac0>()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in method numpy of Tensor object at 0xdd034183ac0> = tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ... 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          ...,\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856]],\\n\\n         [[0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          ...,\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775]],\\n\\n         [[0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          ...,\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765]]]]).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ... 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          ...,\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856]],\\n\\n         [[0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          ...,\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775]],\\n\\n         [[0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          ...,\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765]]]]) = tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ...4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          ...,\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856]],\\n\\n         [[0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          ...,\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775]],\\n\\n         [[0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          ...,\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765]]]],\\n       grad_fn=<ConvolutionBackward0>).data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ...4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          ...,\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856],\\n          [0.4856, 0.4856, 0.4856,  ..., 0.4856, 0.4856, 0.4856]],\\n\\n         [[0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          ...,\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775],\\n          [0.9775, 0.9775, 0.9775,  ..., 0.9775, 0.9775, 0.9775]],\\n\\n         [[0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          ...,\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765],\\n          [0.8765, 0.8765, 0.8765,  ..., 0.8765, 0.8765, 0.8765]]]],\\n       grad_fn=<ConvolutionBackward0>) = Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0., ....,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]]]]))\u001b[0m\n","\n","cin        = 8\n","cout       = 16\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xdd11ad0b830>\n","g          = Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","s          = 32\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:357: AssertionError\n","\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2] _\u001b[0m\n","\n","s = 32, cin = 8, cout = 8, k = 3, stride = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m10\u001b[39;49;00m, cin, s, s, device=device)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(f(x).cached_data.numpy() - g(z).data.numpy()) < \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float32(69.13553) < 0.001\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float32(69.13553) = <function norm at 0xdd1175c3a30>((array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) - array([[[[0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         ...,\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354]],\\n\\n        [[0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         ...,\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.290...5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         ...,\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ]],\\n\\n        [[0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         ...,\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276]]]], dtype=float32)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0xdd1175c3a30> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) = numpy()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where numpy = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()) = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]).cached_data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]) = <needle.nn.nn_conv.Conv object at 0xdd11cc083b0>(needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]))\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         ...,\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354],\\n         [0.66017354, 0.66017354, 0.66017354, ..., 0.66017354,\\n          0.66017354, 0.66017354]],\\n\\n        [[0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         ...,\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.2900776 ],\\n         [0.2900776 , 0.2900776 , 0.2900776 , ..., 0.2900776 ,\\n          0.2900776 , 0.290...5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         ...,\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ],\\n         [0.5699649 , 0.5699649 , 0.5699649 , ..., 0.5699649 ,\\n          0.5699649 , 0.5699649 ]],\\n\\n        [[0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         ...,\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276],\\n         [0.59087276, 0.59087276, 0.59087276, ..., 0.59087276,\\n          0.59087276, 0.59087276]]]], dtype=float32) = <built-in method numpy of Tensor object at 0xdd0341c00f0>()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in method numpy of Tensor object at 0xdd0341c00f0> = tensor([[[[0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          ...,\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          ...,\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          ...,\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, ... 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          ...,\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983]],\\n\\n         [[0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          ...,\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700]],\\n\\n         [[0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          ...,\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909]]]]).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where tensor([[[[0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          ...,\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          ...,\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          ...,\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, ... 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          ...,\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983]],\\n\\n         [[0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          ...,\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700]],\\n\\n         [[0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          ...,\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909]]]]) = tensor([[[[0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          ...,\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          ...,\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          ...,\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, ...2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          ...,\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983]],\\n\\n         [[0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          ...,\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700]],\\n\\n         [[0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          ...,\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909]]]],\\n       grad_fn=<ConvolutionBackward0>).data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where tensor([[[[0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          ...,\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602],\\n          [0.6602, 0.6602, 0.6602,  ..., 0.6602, 0.6602, 0.6602]],\\n\\n         [[0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          ...,\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901],\\n          [0.2901, 0.2901, 0.2901,  ..., 0.2901, 0.2901, 0.2901]],\\n\\n         [[0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          ...,\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, 0.6180,  ..., 0.6180, 0.6180, 0.6180],\\n          [0.6180, 0.6180, ...2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          ...,\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983],\\n          [0.2983, 0.2983, 0.2983,  ..., 0.2983, 0.2983, 0.2983]],\\n\\n         [[0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          ...,\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700],\\n          [0.5700, 0.5700, 0.5700,  ..., 0.5700, 0.5700, 0.5700]],\\n\\n         [[0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          ...,\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909],\\n          [0.5909, 0.5909, 0.5909,  ..., 0.5909, 0.5909, 0.5909]]]],\\n       grad_fn=<ConvolutionBackward0>) = Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0., ....,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]]]]))\u001b[0m\n","\n","cin        = 8\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xdd11cc083b0>\n","g          = Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","s          = 32\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:357: AssertionError\n","\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1] _\u001b[0m\n","\n","s = 32, cin = 16, cout = 8, k = 3, stride = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m10\u001b[39;49;00m, cin, s, s, device=device)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(f(x).cached_data.numpy() - g(z).data.numpy()) < \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float32(162.83974) < 0.001\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float32(162.83974) = <function norm at 0xdd1175c3a30>((array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) - array([[[[0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         ...,\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245]],\\n\\n        [[0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         ...,\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.584...01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         ...,\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963]],\\n\\n        [[0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         ...,\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ]]]], dtype=float32)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0xdd1175c3a30> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) = numpy()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where numpy = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()) = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]).cached_data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]) = <needle.nn.nn_conv.Conv object at 0xdd034193e90>(needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]))\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         ...,\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245],\\n         [0.55219245, 0.55219245, 0.55219245, ..., 0.55219245,\\n          0.55219245, 0.55219245]],\\n\\n        [[0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         ...,\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.58447605],\\n         [0.58447605, 0.58447605, 0.58447605, ..., 0.58447605,\\n          0.58447605, 0.584...01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         ...,\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963],\\n         [0.01642963, 0.01642963, 0.01642963, ..., 0.01642963,\\n          0.01642963, 0.01642963]],\\n\\n        [[0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         ...,\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ],\\n         [0.9295293 , 0.9295293 , 0.9295293 , ..., 0.9295293 ,\\n          0.9295293 , 0.9295293 ]]]], dtype=float32) = <built-in method numpy of Tensor object at 0xdd0341c2b70>()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in method numpy of Tensor object at 0xdd0341c2b70> = tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ... 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          ...,\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003]],\\n\\n         [[0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          ...,\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164]],\\n\\n         [[0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          ...,\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295]]]]).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ... 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          ...,\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003]],\\n\\n         [[0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          ...,\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164]],\\n\\n         [[0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          ...,\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295]]]]) = tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ...1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          ...,\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003]],\\n\\n         [[0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          ...,\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164]],\\n\\n         [[0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          ...,\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295]]]],\\n       grad_fn=<ConvolutionBackward0>).data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where tensor([[[[0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          ...,\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522],\\n          [0.5522, 0.5522, 0.5522,  ..., 0.5522, 0.5522, 0.5522]],\\n\\n         [[0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          ...,\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845],\\n          [0.5845, 0.5845, 0.5845,  ..., 0.5845, 0.5845, 0.5845]],\\n\\n         [[0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          ...,\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, 0.9619,  ..., 0.9619, 0.9619, 0.9619],\\n          [0.9619, 0.9619, ...1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          ...,\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003],\\n          [0.1003, 0.1003, 0.1003,  ..., 0.1003, 0.1003, 0.1003]],\\n\\n         [[0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          ...,\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164],\\n          [0.0164, 0.0164, 0.0164,  ..., 0.0164, 0.0164, 0.0164]],\\n\\n         [[0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          ...,\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295],\\n          [0.9295, 0.9295, 0.9295,  ..., 0.9295, 0.9295, 0.9295]]]],\\n       grad_fn=<ConvolutionBackward0>) = Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0., ....,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]]]]))\u001b[0m\n","\n","cin        = 16\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xdd034193e90>\n","g          = Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","s          = 32\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:357: AssertionError\n","\u001b[31m\u001b[1m_ test_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2] _\u001b[0m\n","\n","s = 32, cin = 16, cout = 8, k = 3, stride = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_forward_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_forward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m10\u001b[39;49;00m, cin, s, s, device=device)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(f(x).cached_data.numpy() - g(z).data.numpy()) < \u001b[94m1e-3\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float32(88.45428) < 0.001\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float32(88.45428) = <function norm at 0xdd1175c3a30>((array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) - array([[[[0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         ...,\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ]],\\n\\n        [[0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         ...,\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.715...4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         ...,\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ]],\\n\\n        [[0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         ...,\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ]]]], dtype=float32)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0xdd1175c3a30> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ...,... 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        ...,\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]],\\n\\n        [[0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         ...,\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.],\\n         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32) = numpy()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where numpy = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where NDArray([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ...0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda()) = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]).cached_data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]) = <needle.nn.nn_conv.Conv object at 0xdd034190e90>(needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. .... 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]\\n\\n\\n [[[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  ...\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]\\n\\n  [[0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   ...\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]\\n   [0. 0. 0. ... 0. 0. 0.]]]]))\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[[[0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         ...,\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ],\\n         [0.5488135 , 0.5488135 , 0.5488135 , ..., 0.5488135 ,\\n          0.5488135 , 0.5488135 ]],\\n\\n        [[0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         ...,\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.71518934],\\n         [0.71518934, 0.71518934, 0.71518934, ..., 0.71518934,\\n          0.71518934, 0.715...4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         ...,\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ],\\n         [0.4375872 , 0.4375872 , 0.4375872 , ..., 0.4375872 ,\\n          0.4375872 , 0.4375872 ]],\\n\\n        [[0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         ...,\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ],\\n         [0.891773  , 0.891773  , 0.891773  , ..., 0.891773  ,\\n          0.891773  , 0.891773  ]]]], dtype=float32) = <built-in method numpy of Tensor object at 0xdd0341c3ac0>()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in method numpy of Tensor object at 0xdd0341c3ac0> = tensor([[[[0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          ...,\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488]],\\n\\n         [[0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          ...,\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152]],\\n\\n         [[0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          ...,\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, ... 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          ...,\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459]],\\n\\n         [[0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          ...,\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376]],\\n\\n         [[0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          ...,\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918]]]]).numpy\u001b[0m\n","\u001b[1m\u001b[31mE        +        where tensor([[[[0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          ...,\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488]],\\n\\n         [[0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          ...,\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152]],\\n\\n         [[0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          ...,\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, ... 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          ...,\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459]],\\n\\n         [[0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          ...,\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376]],\\n\\n         [[0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          ...,\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918]]]]) = tensor([[[[0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          ...,\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488]],\\n\\n         [[0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          ...,\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152]],\\n\\n         [[0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          ...,\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, ...6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          ...,\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459]],\\n\\n         [[0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          ...,\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376]],\\n\\n         [[0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          ...,\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918]]]],\\n       grad_fn=<ConvolutionBackward0>).data\u001b[0m\n","\u001b[1m\u001b[31mE        +          where tensor([[[[0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          ...,\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488],\\n          [0.5488, 0.5488, 0.5488,  ..., 0.5488, 0.5488, 0.5488]],\\n\\n         [[0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          ...,\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152],\\n          [0.7152, 0.7152, 0.7152,  ..., 0.7152, 0.7152, 0.7152]],\\n\\n         [[0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          ...,\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, 0.6028,  ..., 0.6028, 0.6028, 0.6028],\\n          [0.6028, 0.6028, ...6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          ...,\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459],\\n          [0.6459, 0.6459, 0.6459,  ..., 0.6459, 0.6459, 0.6459]],\\n\\n         [[0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          ...,\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376],\\n          [0.4376, 0.4376, 0.4376,  ..., 0.4376, 0.4376, 0.4376]],\\n\\n         [[0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          ...,\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918],\\n          [0.8918, 0.8918, 0.8918,  ..., 0.8918, 0.8918, 0.8918]]]],\\n       grad_fn=<ConvolutionBackward0>) = Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))(tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0., ....,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         ...,\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]],\\n\\n         [[0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          ...,\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.],\\n          [0., 0., 0.,  ..., 0., 0., 0.]]]]))\u001b[0m\n","\n","cin        = 16\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xdd034190e90>\n","g          = Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","s          = 32\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0... [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:357: AssertionError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-4-8-16-3-1]\u001b[0m - AssertionError: assert np.float32(28.731468) < 0.001\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-16-3-2]\u001b[0m - AssertionError: assert np.float32(124.36587) < 0.001\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-8-8-3-2]\u001b[0m - AssertionError: assert np.float32(69.13553) < 0.001\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-1]\u001b[0m - AssertionError: assert np.float32(162.83974) < 0.001\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_forward[needle.backend_ndarray.ndarray_backend_cuda-32-16-8-3-2]\u001b[0m - AssertionError: assert np.float32(88.45428) < 0.001\n","\u001b[31m================= \u001b[31m\u001b[1m5 failed\u001b[0m, \u001b[32m5 passed\u001b[0m, \u001b[33m1793 deselected\u001b[0m\u001b[31m in 4.05s\u001b[0m\u001b[31m =================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"nn_conv_forward\""]},{"cell_type":"code","execution_count":41,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H-RETyy-ucAU","executionInfo":{"status":"ok","timestamp":1763015390926,"user_tz":300,"elapsed":5287,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"40318fb2-0818-4846-d998-1b07bd2896b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1789 deselected / 14 selected                           \u001b[0m\n","\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [  7%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 14%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 21%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 28%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 35%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 42%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 57%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 64%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 71%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 78%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [ 85%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1] \u001b[31mFAILED\u001b[0m\u001b[31m [ 92%]\u001b[0m\n","tests/hw4/test_conv.py::test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1] __\u001b[0m\n","\n","s = 4, cin = 1, cout = 1, k = 3, stride = 1, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 1\n","cout       = 1\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87ca8f0740>\n","g          = Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0.06101197 0.11352888 0.74523795 0.3791517 ]\n","   [1.4577379  1.4840058  1.6641259  0.6020411 ]\n","   [0.9583608  0.68555784 0.6888615  0.5859287 ]\n","   [0.47016236 0.4189636  0.66456234 0.21001771]]]])\n","s          = 4\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.79172504 0.5288949  0.56804454 0.92559665]\n","   [0.07103606 0.0871293  0.0202184  0.83261985]\n","   [0.77815676 0.87001216 0.9786183  0.7991586 ]\n","   [0.46147937 0.7805292  0.11827443 0.639921  ]]]])\n","z          = tensor([[[[0.7917, 0.5289, 0.5680, 0.9256],\n","          [0.0710, 0.0871, 0.0202, 0.8326],\n","          [0.7782, 0.8700, 0.9786, 0.7992],\n","          [0.4615, 0.7805, 0.1183, 0.6399]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0.06101197 0.11352888 0.74523795 0.3791517 ]\n","   [1.4577379  1.4840058  1.6641259  0.6020411 ]\n","   [0.9583608  0.68555784 0.6888615  0.5859287 ]\n","   [0.47016236 0.4189636  0.66456234 0.21001771]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0.06101197 0.11352888 0.74523795 0.3791517 ]\n","   [1.4577379  1.4840058  1.6641259  0.6020411 ]\n","   [0.9583608  0.68555784 0.6888615  0.5859287 ]\n","   [0.47016236 0.4189636  0.66456234 0.21001771]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2ba40>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0.06101197 0.11352888 0.74523795 0.3791517 ]\n","   [1.4577379  1.4840058  1.6641259  0.6020411 ]\n","   [0.9583608  0.68555784 0.6888615  0.5859287 ]\n","   [0.47016236 0.4189636  0.66456234 0.21001771]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2ba40>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c2b500>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c2b500>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2ba40>\n","a = NDArray([[[[0.06101197 0.11352888 0.74523795 0.3791517 ]\n","   [1.4577379  1.4840058  1.6641259  0.6020411 ]\n","   [0.9583608  0.68555784 0.6888615  0.5859287 ]\n","   [0.47016236 0.4189636  0.66456234 0.21001771]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0.06101197 0.11352888 0.74523795 0.3791517 ]\n","   [1.4577379  1.4840058  1.6641259  0.6020411 ]\n","   [0.9583608  0.68555784 0.6888615  0.5859287 ]\n","   [0.47016236 0.4189636  0.66456234 0.21001771]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2ba40>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 16, k = 3, stride = 1, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 16\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87ca8f1430>\n","g          = Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[ 1.2306503   0.6323117   0.3390208  ...  0.41770732  0.45129475\n","     0.15171665]\n","   [ 0.8320013   0....   0.23705792\n","    -0.4429681 ]\n","   [ 0.01150343 -0.6674815  -0.34074932 ... -0.23327562 -0.16204375\n","    -0.54578465]]]])\n","s          = 14\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.79868925 0.92345554 0.29915366 ... 0.13554843 0.72026587\n","    0.925395  ]\n","   [0.6646656  0.42305443...0.4892255  0.48206186\n","    0.45311075]\n","   [0.8035453  0.47169307 0.7583655  ... 0.07899625 0.9464155\n","    0.4682215 ]]]])\n","z          = tensor([[[[0.7987, 0.9235, 0.2992,  ..., 0.1355, 0.7203, 0.9254],\n","          [0.6647, 0.4231, 0.1990,  ..., 0.3695, 0.3....4892, 0.4821, 0.4531],\n","          [0.8035, 0.4717, 0.7584,  ..., 0.0790, 0.9464, 0.4682]]]],\n","       requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.2306503   0.6323117   0.3390208  ...  0.41770732  0.45129475\n","     0.15171665]\n","   [ 0.8320013   0....   0.23705792\n","    -0.4429681 ]\n","   [ 0.01150343 -0.6674815  -0.34074932 ... -0.23327562 -0.16204375\n","    -0.54578465]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.2306503   0.6323117   0.3390208  ...  0.41770732  0.45129475\n","     0.15171665]\n","   [ 0.8320013   0... 0.23705792\n","    -0.4429681 ]\n","   [ 0.01150343 -0.6674815  -0.34074932 ... -0.23327562 -0.16204375\n","    -0.54578465]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c59250>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.2306503   0.6323117   0.3390208  ...  0.41770732  0.45129475\n","     0.15171665]\n","   [ 0.8320013   0... 0.23705792\n","    -0.4429681 ]\n","   [ 0.01150343 -0.6674815  -0.34074932 ... -0.23327562 -0.16204375\n","    -0.54578465]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9c59250>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c593d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c593d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9c59250>\n","a = NDArray([[[[ 1.2306503   0.6323117   0.3390208  ...  0.41770732  0.45129475\n","     0.15171665]\n","   [ 0.8320013   0.006327...    -0.4429681 ]\n","   [ 0.01150343 -0.6674815  -0.34074932 ... -0.23327562 -0.16204375\n","    -0.54578465]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.2306503   0.6323117   0.3390208  ...  0.41770732  0.45129475\n","     0.15171665]\n","   [ 0.8320013   0.006327...    -0.4429681 ]\n","   [ 0.01150343 -0.6674815  -0.34074932 ... -0.23327562 -0.16204375\n","    -0.54578465]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c59250>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 16, k = 3, stride = 2, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 16\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87ca75c410>\n","g          = Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[ 1.23065031e+00  3.39020789e-01  4.11729902e-01  9.00930405e-01\n","     7.02045619e-01  8.68556321e-01 ...2.12055802e-01 -3.23740244e-01 -1.16070533e+00 -7.61978865e-01\n","    -8.50212395e-01  1.16794623e-01  2.37057924e-01]]]])\n","s          = 14\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.79868925 0.92345554 0.29915366 ... 0.13554843 0.72026587\n","    0.925395  ]\n","   [0.6646656  0.42305443...0.4892255  0.48206186\n","    0.45311075]\n","   [0.8035453  0.47169307 0.7583655  ... 0.07899625 0.9464155\n","    0.4682215 ]]]])\n","z          = tensor([[[[0.7987, 0.9235, 0.2992,  ..., 0.1355, 0.7203, 0.9254],\n","          [0.6647, 0.4231, 0.1990,  ..., 0.3695, 0.3....4892, 0.4821, 0.4531],\n","          [0.8035, 0.4717, 0.7584,  ..., 0.0790, 0.9464, 0.4682]]]],\n","       requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 1.23065031e+00  3.39020789e-01  4.11729902e-01  9.00930405e-01\n","     7.02045619e-01  8.68556321e-01 ...2.12055802e-01 -3.23740244e-01 -1.16070533e+00 -7.61978865e-01\n","    -8.50212395e-01  1.16794623e-01  2.37057924e-01]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 1.23065031e+00  3.39020789e-01  4.11729902e-01  9.00930405e-01\n","     7.02045619e-01  8.68556321e-01...12055802e-01 -3.23740244e-01 -1.16070533e+00 -7.61978865e-01\n","    -8.50212395e-01  1.16794623e-01  2.37057924e-01]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2be00>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 1.23065031e+00  3.39020789e-01  4.11729902e-01  9.00930405e-01\n","     7.02045619e-01  8.68556321e-01...12055802e-01 -3.23740244e-01 -1.16070533e+00 -7.61978865e-01\n","    -8.50212395e-01  1.16794623e-01  2.37057924e-01]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2be00>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c2b7a0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c2b7a0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2be00>\n","a = NDArray([[[[ 1.23065031e+00  3.39020789e-01  4.11729902e-01  9.00930405e-01\n","     7.02045619e-01  8.68556321e-01  4.512... -3.23740244e-01 -1.16070533e+00 -7.61978865e-01\n","    -8.50212395e-01  1.16794623e-01  2.37057924e-01]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 1.23065031e+00  3.39020789e-01  4.11729902e-01  9.00930405e-01\n","     7.02045619e-01  8.68556321e-01  4.512... -3.23740244e-01 -1.16070533e+00 -7.61978865e-01\n","    -8.50212395e-01  1.16794623e-01  2.37057924e-01]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c2be00>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 8, k = 3, stride = 1, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 8\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9c5a6c0>\n","g          = Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[ 0.13541745 -0.39964718 -0.17145535 ... -0.23007657 -0.63488907\n","    -1.0780072 ]\n","   [-0.0466461  -0....9  -0.5053438\n","    -0.45708266]\n","   [ 0.07413982 -0.7652476  -0.12649626 ... -0.8777528  -0.01689595\n","    -0.32612592]]]])\n","s          = 14\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[6.96463168e-01 2.47398749e-01 3.96155231e-02 ... 9.97962236e-01\n","    3.62189054e-01 4.70648944e-01]\n"," ...3395844e-01]\n","   [1.76103845e-01 3.19807321e-01 8.16825151e-01 ... 5.61398745e-01\n","    7.13245988e-01 9.81864214e-01]]]])\n","z          = tensor([[[[6.9646e-01, 2.4740e-01, 3.9616e-02,  ..., 9.9796e-01,\n","           3.6219e-01, 4.7065e-01],\n","          [3.7825...      [1.7610e-01, 3.1981e-01, 8.1683e-01,  ..., 5.6140e-01,\n","           7.1325e-01, 9.8186e-01]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 0.13541745 -0.39964718 -0.17145535 ... -0.23007657 -0.63488907\n","    -1.0780072 ]\n","   [-0.0466461  -0....9  -0.5053438\n","    -0.45708266]\n","   [ 0.07413982 -0.7652476  -0.12649626 ... -0.8777528  -0.01689595\n","    -0.32612592]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 0.13541745 -0.39964718 -0.17145535 ... -0.23007657 -0.63488907\n","    -1.0780072 ]\n","   [-0.0466461  -0... -0.5053438\n","    -0.45708266]\n","   [ 0.07413982 -0.7652476  -0.12649626 ... -0.8777528  -0.01689595\n","    -0.32612592]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c5b3e0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 0.13541745 -0.39964718 -0.17145535 ... -0.23007657 -0.63488907\n","    -1.0780072 ]\n","   [-0.0466461  -0... -0.5053438\n","    -0.45708266]\n","   [ 0.07413982 -0.7652476  -0.12649626 ... -0.8777528  -0.01689595\n","    -0.32612592]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9c5b3e0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c5a060>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c5a060>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9c5b3e0>\n","a = NDArray([[[[ 0.13541745 -0.39964718 -0.17145535 ... -0.23007657 -0.63488907\n","    -1.0780072 ]\n","   [-0.0466461  -0.723997...    -0.45708266]\n","   [ 0.07413982 -0.7652476  -0.12649626 ... -0.8777528  -0.01689595\n","    -0.32612592]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 0.13541745 -0.39964718 -0.17145535 ... -0.23007657 -0.63488907\n","    -1.0780072 ]\n","   [-0.0466461  -0.723997...    -0.45708266]\n","   [ 0.07413982 -0.7652476  -0.12649626 ... -0.8777528  -0.01689595\n","    -0.32612592]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c5b3e0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 8, k = 3, stride = 2, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 8\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce7c50>\n","g          = Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[ 0.13541745 -0.17145535 -0.01068119 -0.6273463  -0.43287224\n","     0.01402761 -0.63488907]\n","   [ 0.6618....24053305 -0.6554097 ]\n","   [ 0.03133741 -0.9030416  -1.0101695  -1.2016605  -0.20600146\n","     0.01237326 -0.5053438 ]]]])\n","s          = 14\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[6.96463168e-01 2.47398749e-01 3.96155231e-02 ... 9.97962236e-01\n","    3.62189054e-01 4.70648944e-01]\n"," ...3395844e-01]\n","   [1.76103845e-01 3.19807321e-01 8.16825151e-01 ... 5.61398745e-01\n","    7.13245988e-01 9.81864214e-01]]]])\n","z          = tensor([[[[6.9646e-01, 2.4740e-01, 3.9616e-02,  ..., 9.9796e-01,\n","           3.6219e-01, 4.7065e-01],\n","          [3.7825...      [1.7610e-01, 3.1981e-01, 8.1683e-01,  ..., 5.6140e-01,\n","           7.1325e-01, 9.8186e-01]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 0.13541745 -0.17145535 -0.01068119 -0.6273463  -0.43287224\n","     0.01402761 -0.63488907]\n","   [ 0.6618....24053305 -0.6554097 ]\n","   [ 0.03133741 -0.9030416  -1.0101695  -1.2016605  -0.20600146\n","     0.01237326 -0.5053438 ]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 0.13541745 -0.17145535 -0.01068119 -0.6273463  -0.43287224\n","     0.01402761 -0.63488907]\n","   [ 0.661...4053305 -0.6554097 ]\n","   [ 0.03133741 -0.9030416  -1.0101695  -1.2016605  -0.20600146\n","     0.01237326 -0.5053438 ]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7ce0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 0.13541745 -0.17145535 -0.01068119 -0.6273463  -0.43287224\n","     0.01402761 -0.63488907]\n","   [ 0.661...4053305 -0.6554097 ]\n","   [ 0.03133741 -0.9030416  -1.0101695  -1.2016605  -0.20600146\n","     0.01237326 -0.5053438 ]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7ce0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce50d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce50d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7ce0>\n","a = NDArray([[[[ 0.13541745 -0.17145535 -0.01068119 -0.6273463  -0.43287224\n","     0.01402761 -0.63488907]\n","   [ 0.6618111  -...554097 ]\n","   [ 0.03133741 -0.9030416  -1.0101695  -1.2016605  -0.20600146\n","     0.01237326 -0.5053438 ]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 0.13541745 -0.17145535 -0.01068119 -0.6273463  -0.43287224\n","     0.01402761 -0.63488907]\n","   [ 0.6618111  -...554097 ]\n","   [ 0.03133741 -0.9030416  -1.0101695  -1.2016605  -0.20600146\n","     0.01237326 -0.5053438 ]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7ce0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1] _\u001b[0m\n","\n","s = 14, cin = 16, cout = 8, k = 3, stride = 1, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 16\n","cout       = 8\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9c5aea0>\n","g          = Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[ 0.6068339   0.4264753   0.48195186 ...  0.2931519   0.54818267\n","     0.05751162]\n","   [ 0.52536005  0....25  0.04865149\n","    -0.49439794]\n","   [ 0.64650583 -0.11903492 -0.55669105 ... -0.3260765  -0.5033075\n","     0.26678377]]]])\n","s          = 14\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.7510323  0.15597793 0.42600238 ... 0.38840413 0.4862721\n","    0.58815145]\n","   [0.9838538  0.69733024 ...0.35930127 0.41500303\n","    0.3384509 ]\n","   [0.4826699  0.37319595 0.14424832 ... 0.36331356 0.6944434\n","    0.39911222]]]])\n","z          = tensor([[[[0.7510, 0.1560, 0.4260,  ..., 0.3884, 0.4863, 0.5882],\n","          [0.9839, 0.6973, 0.3895,  ..., 0.3675, 0.7....3593, 0.4150, 0.3385],\n","          [0.4827, 0.3732, 0.1442,  ..., 0.3633, 0.6944, 0.3991]]]],\n","       requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 0.6068339   0.4264753   0.48195186 ...  0.2931519   0.54818267\n","     0.05751162]\n","   [ 0.52536005  0....25  0.04865149\n","    -0.49439794]\n","   [ 0.64650583 -0.11903492 -0.55669105 ... -0.3260765  -0.5033075\n","     0.26678377]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 0.6068339   0.4264753   0.48195186 ...  0.2931519   0.54818267\n","     0.05751162]\n","   [ 0.52536005  0...  0.04865149\n","    -0.49439794]\n","   [ 0.64650583 -0.11903492 -0.55669105 ... -0.3260765  -0.5033075\n","     0.26678377]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8a70>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 0.6068339   0.4264753   0.48195186 ...  0.2931519   0.54818267\n","     0.05751162]\n","   [ 0.52536005  0...  0.04865149\n","    -0.49439794]\n","   [ 0.64650583 -0.11903492 -0.55669105 ... -0.3260765  -0.5033075\n","     0.26678377]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8a70>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9cea6c0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9cea6c0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8a70>\n","a = NDArray([[[[ 0.6068339   0.4264753   0.48195186 ...  0.2931519   0.54818267\n","     0.05751162]\n","   [ 0.52536005  0.982958...\n","    -0.49439794]\n","   [ 0.64650583 -0.11903492 -0.55669105 ... -0.3260765  -0.5033075\n","     0.26678377]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 0.6068339   0.4264753   0.48195186 ...  0.2931519   0.54818267\n","     0.05751162]\n","   [ 0.52536005  0.982958...\n","    -0.49439794]\n","   [ 0.64650583 -0.11903492 -0.55669105 ... -0.3260765  -0.5033075\n","     0.26678377]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8a70>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2] _\u001b[0m\n","\n","s = 14, cin = 16, cout = 8, k = 3, stride = 2, device = cpu()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 16\n","cout       = 8\n","device     = cpu()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9cead20>\n","g          = Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[ 6.06833875e-01  4.81951863e-01 -3.48422751e-02  2.03439862e-01\n","     5.55159509e-01  1.60604656e-01 ...2.66289949e-01 -7.41105378e-01 -3.57464701e-01 -2.87837237e-01\n","    -2.30419636e-01  7.82464385e-01  4.86514941e-02]]]])\n","s          = 14\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.7510323  0.15597793 0.42600238 ... 0.38840413 0.4862721\n","    0.58815145]\n","   [0.9838538  0.69733024 ...0.35930127 0.41500303\n","    0.3384509 ]\n","   [0.4826699  0.37319595 0.14424832 ... 0.36331356 0.6944434\n","    0.39911222]]]])\n","z          = tensor([[[[0.7510, 0.1560, 0.4260,  ..., 0.3884, 0.4863, 0.5882],\n","          [0.9839, 0.6973, 0.3895,  ..., 0.3675, 0.7....3593, 0.4150, 0.3385],\n","          [0.4827, 0.3732, 0.1442,  ..., 0.3633, 0.6944, 0.3991]]]],\n","       requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[ 6.06833875e-01  4.81951863e-01 -3.48422751e-02  2.03439862e-01\n","     5.55159509e-01  1.60604656e-01 ...2.66289949e-01 -7.41105378e-01 -3.57464701e-01 -2.87837237e-01\n","    -2.30419636e-01  7.82464385e-01  4.86514941e-02]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[ 6.06833875e-01  4.81951863e-01 -3.48422751e-02  2.03439862e-01\n","     5.55159509e-01  1.60604656e-01...66289949e-01 -7.41105378e-01 -3.57464701e-01 -2.87837237e-01\n","    -2.30419636e-01  7.82464385e-01  4.86514941e-02]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9940>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[ 6.06833875e-01  4.81951863e-01 -3.48422751e-02  2.03439862e-01\n","     5.55159509e-01  1.60604656e-01...66289949e-01 -7.41105378e-01 -3.57464701e-01 -2.87837237e-01\n","    -2.30419636e-01  7.82464385e-01  4.86514941e-02]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9940>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9cebb60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9cebb60>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9940>\n","a = NDArray([[[[ 6.06833875e-01  4.81951863e-01 -3.48422751e-02  2.03439862e-01\n","     5.55159509e-01  1.60604656e-01  5.481... -7.41105378e-01 -3.57464701e-01 -2.87837237e-01\n","    -2.30419636e-01  7.82464385e-01  4.86514941e-02]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[ 6.06833875e-01  4.81951863e-01 -3.48422751e-02  2.03439862e-01\n","     5.55159509e-01  1.60604656e-01  5.481... -7.41105378e-01 -3.57464701e-01 -2.87837237e-01\n","    -2.30419636e-01  7.82464385e-01  4.86514941e-02]]]], device=cpu())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9940>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1] _\u001b[0m\n","\n","s = 4, cin = 1, cout = 1, k = 3, stride = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 1\n","cout       = 1\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce6de0>\n","g          = Conv2d(1, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","s          = 4\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.],\n","          [0., 0., 0., 0.]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7bf0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7bf0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce54f0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce54f0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7bf0>\n","a = NDArray([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]\n","   [0. 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce7bf0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 16, k = 3, stride = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 16\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce6ff0>\n","g          = Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","s          = 14\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0...0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce6600>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce6600>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce4950>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce4950>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce6600>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce6600>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 16, k = 3, stride = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 16\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce6fc0>\n","g          = Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]])\n","s          = 14\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0...0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]...0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9d60>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]...0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9d60>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce8200>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce8200>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9d60>\n","a = NDArray([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0....[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0....[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce9d60>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 8, k = 3, stride = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce93a0>\n","g          = Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","s          = 14\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.8207671  0.9088437  0.8155238  ... 0.9589827  0.35536885\n","    0.3567069 ]\n","   [0.0163285  0.18523233...0.         ... 0.         0.\n","    0.        ]\n","   [0.         0.         0.         ... 0.         0.\n","    0.        ]]]])\n","z          = tensor([[[[0.8208, 0.9088, 0.8155,  ..., 0.9590, 0.3554, 0.3567],\n","          [0.0163, 0.1852, 0.4013,  ..., 0.0331, 0.0....0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n","       requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8890>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8890>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ceb980>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ceb980>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8890>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8890>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2] _\u001b[0m\n","\n","s = 14, cin = 8, cout = 8, k = 3, stride = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 8\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce9a90>\n","g          = Conv2d(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]])\n","s          = 14\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0...0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]...0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c89b80>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]...0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9c89b80>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c890d0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c890d0>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9c89b80>\n","a = NDArray([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0....[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0....[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c89b80>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1] _\u001b[0m\n","\n","s = 14, cin = 16, cout = 8, k = 3, stride = 1, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 16\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9ce9e20>\n","g          = Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","s          = 14\n","stride     = 1\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0.5846446  0.09127072 0.600471   ... 0.74865574 0.4128966\n","    0.58909833]\n","   [0.02638933 0.9521098  ...0.         ... 0.         0.\n","    0.        ]\n","   [0.         0.         0.         ... 0.         0.\n","    0.        ]]]])\n","z          = tensor([[[[0.5846, 0.0913, 0.6005,  ..., 0.7487, 0.4129, 0.5891],\n","          [0.0264, 0.9521, 0.8776,  ..., 0.5347, 0.0....0000, 0.0000, 0.0000],\n","          [0.0000, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 0.0000]]]],\n","       requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8dd0>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. .....  [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8dd0>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce8890>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9ce8890>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8dd0>\n","a = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0...... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9ce8dd0>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[31m\u001b[1m_ test_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2] _\u001b[0m\n","\n","s = 14, cin = 16, cout = 8, k = 3, stride = 2, device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33ms,cin,cout,k,stride\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, conv_back_params)\u001b[90m\u001b[39;49;00m\n","    \u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_nn_conv_backward\u001b[39;49;00m(s, cin, cout, k, stride, device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        \u001b[94mimport\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mtorch\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","        f = ndl.nn.Conv(cin, cout, k, stride=stride, device=device)\u001b[90m\u001b[39;49;00m\n","        x = ndl.init.rand(\u001b[94m1\u001b[39;49;00m, cin, s, s, device=device, requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        g = torch.nn.Conv2d(cin, cout, k, stride=stride, padding=k//\u001b[94m2\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        g.weight.data = torch.tensor(f.weight.cached_data.numpy().transpose(\u001b[94m3\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m0\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","        g.bias.data = torch.tensor(f.bias.cached_data.numpy())\u001b[90m\u001b[39;49;00m\n","        z = torch.tensor(x.cached_data.numpy(), requires_grad=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        z.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        res1 = f(x)\u001b[90m\u001b[39;49;00m\n",">       y1 = res1.sum()\u001b[90m\u001b[39;49;00m\n","             ^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","cin        = 16\n","cout       = 8\n","device     = cuda()\n","f          = <needle.nn.nn_conv.Conv object at 0xd87c9c88aa0>\n","g          = Conv2d(16, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","k          = 3\n","res1       = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]])\n","s          = 14\n","stride     = 2\n","torch      = <module 'torch' from '/usr/local/lib/python3.12/dist-packages/torch/__init__.py'>\n","x          = needle.Tensor([[[[0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ......\n","   [0. 0. 0. ... 0. 0. 0.]\n","   ...\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]\n","   [0. 0. 0. ... 0. 0. 0.]]]])\n","z          = tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0...0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.],\n","          [0., 0., 0.,  ..., 0., 0., 0.]]]], requires_grad=True)\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:384: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        axes       = None\n","        self       = needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n",".... 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]])\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","        args       = (needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]...0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]]),)\n","        self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c8bf50>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","        inputs     = (needle.Tensor([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]...0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]]),)\n","        op         = <needle.ops.ops_mathematic.Summation object at 0xd87c9c8bf50>\n","        tensor     = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c8a180>\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","        self       = <[TypeError(\"object of type 'NoneType' has no len()\") raised in repr()] Tensor object at 0xd87c9c8a180>\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0xd87c9c8bf50>\n","a = NDArray([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0....[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","a          = NDArray([[[[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0....[0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]\n","   [0. 0. 0. 0. 0. 0. 0.]]]], device=cuda())\n","self       = <needle.ops.ops_mathematic.Summation object at 0xd87c9c8bf50>\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-4-1-1-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-16-3-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-8-8-3-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cpu-14-16-8-3-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-4-1-1-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-16-3-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-8-8-3-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-1]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_nn_conv_backward[needle.backend_ndarray.ndarray_backend_cuda-14-16-8-3-2]\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31m===================== \u001b[31m\u001b[1m14 failed\u001b[0m, \u001b[33m1789 deselected\u001b[0m\u001b[31m in 3.53s\u001b[0m\u001b[31m ======================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"nn_conv_backward\""]},{"cell_type":"markdown","metadata":{"id":"Q0rZll-rucAU"},"source":["-----------------"]},{"cell_type":"markdown","metadata":{"id":"cHY3eYh5ucAU"},"source":["### Submit nn.Conv to mugrade [20 points]"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"G7S4jAHmucAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763015396568,"user_tz":300,"elapsed":5642,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"fab3c9b9-8fd4-49b2-b37e-084de3a7fd40"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\n","\n","tests/hw4/test_conv.py \n","Submitting conv_forward...\n","Grader test 1 passed\n","Grader test 2 passed\n","Grader test 3 passed\n","Grader test 4 passed\n","Grader test 5 passed\n","Grader test 6 passed\n","Grader test 7 passed\n","Grader test 8 passed\n","Grader test 9 passed\n","Grader test 10 passed\n","Grader test 11 passed\n","Grader test 12 passed\n","Grader test 13 failed\n","Grader test 14 failed\n","\u001b[31mF\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_____________________________ submit_conv_forward ______________________________\u001b[0m\n","\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1msubmit_conv_forward\u001b[0m - Failed\n","\u001b[31m================== \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m9 deselected\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 3.54s\u001b[0m\u001b[31m ==================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"conv_forward\""]},{"cell_type":"code","execution_count":43,"metadata":{"id":"IbyD3w9jucAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763015400889,"user_tz":300,"elapsed":4320,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"ac6a9f2e-4080-4aad-d7e2-a0d015412bb3"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\n","\n","tests/hw4/test_conv.py \n","Submitting conv_backward...\n","\u001b[31mF\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_____________________________ submit_conv_backward _____________________________\u001b[0m\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92msubmit_conv_backward\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mDoConvOpBackward\u001b[39;49;00m(batches, cin, cout, n, k=\u001b[94m3\u001b[39;49;00m, stride=\u001b[94m1\u001b[39;49;00m, padding=\u001b[94m0\u001b[39;49;00m, device=ndl.cpu(), wrtX=\u001b[94mTrue\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            X = Rand(batches, n, n, cin, device=device)\u001b[90m\u001b[39;49;00m\n","            X.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            W = Rand(k, k, cin, cout, device=device)\u001b[90m\u001b[39;49;00m\n","            W.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            y = ndl.conv(X, W, stride=stride, padding=padding).sum()\u001b[90m\u001b[39;49;00m\n","            y.backward()\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m wrtX:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m W.grad\u001b[90m\u001b[39;49;00m\n","            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m X.grad\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mDoConvLayerBackward\u001b[39;49;00m(batches, cin, cout, n, k=\u001b[94m3\u001b[39;49;00m, stride=\u001b[94m1\u001b[39;49;00m, bias=\u001b[94mTrue\u001b[39;49;00m, device=ndl.cpu(), wrtX=\u001b[94mTrue\u001b[39;49;00m):\u001b[90m\u001b[39;49;00m\n","            X = Rand(batches, cin, n, n, device=device)\u001b[90m\u001b[39;49;00m\n","            X.requires_grad = \u001b[94mTrue\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","            f = ndl.nn.Conv(cin, cout, k, stride=stride, bias=bias, device=device)\u001b[90m\u001b[39;49;00m\n","            y = f(X).sum()\u001b[90m\u001b[39;49;00m\n","            y.backward()\u001b[90m\u001b[39;49;00m\n","            \u001b[94mif\u001b[39;49;00m wrtX:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m f.weight.grad\u001b[90m\u001b[39;49;00m\n","            \u001b[94melse\u001b[39;49;00m:\u001b[90m\u001b[39;49;00m\n","                \u001b[94mreturn\u001b[39;49;00m X.grad\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       MugradeSubmit(DoConvOpBackward(\u001b[94m2\u001b[39;49;00m, \u001b[94m1\u001b[39;49;00m, \u001b[94m2\u001b[39;49;00m, \u001b[94m4\u001b[39;49;00m, k=\u001b[94m1\u001b[39;49;00m, stride=\u001b[94m1\u001b[39;49;00m, padding=\u001b[94m0\u001b[39;49;00m, wrtX=\u001b[94mTrue\u001b[39;49;00m))\u001b[90m\u001b[39;49;00m\n","                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:579: \n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:561: in DoConvOpBackward\n","    \u001b[0my = ndl.conv(X, W, stride=stride, padding=padding).sum()\u001b[90m\u001b[39;49;00m\n","        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:348: in sum\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m needle.ops.Summation(axes)(\u001b[96mself\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:80: in __call__\n","    \u001b[0m\u001b[94mreturn\u001b[39;49;00m Tensor.make_from_op(\u001b[96mself\u001b[39;49;00m, args)\u001b[90m\u001b[39;49;00m\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:242: in make_from_op\n","    \u001b[0mtensor.realize_cached_data()\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mpython/needle/autograd.py\u001b[0m:107: in realize_cached_data\n","    \u001b[0m\u001b[96mself\u001b[39;49;00m.cached_data = \u001b[96mself\u001b[39;49;00m.op.compute(\u001b[90m\u001b[39;49;00m\n","_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n","\n","self = <needle.ops.ops_mathematic.Summation object at 0x105f60b227e0>\n","a = NDArray([[[[12. 15.]\n","   [16. 20.]\n","   [24. 30.]\n","   [12. 15.]]\n","\n","  [[32. 40.]\n","   [12. 15.]\n","   [20. 25.]\n","   [24. 30.]]\n","\n","  ...32. 40.]\n","   [36. 45.]\n","   [36. 45.]\n","   [28. 35.]]\n","\n","  [[32. 40.]\n","   [ 4.  5.]\n","   [20. 25.]\n","   [16. 20.]]]], device=cpu())\n","\n","    \u001b[0m\u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mcompute\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m, a):\u001b[90m\u001b[39;49;00m\n","        \u001b[90m### BEGIN YOUR SOLUTION\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m \u001b[96mlen\u001b[39;49;00m(\u001b[96mself\u001b[39;49;00m.axes) > \u001b[94m0\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","               ^^^^^^^^^^^^^^\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       TypeError: object of type 'NoneType' has no len()\u001b[0m\n","\n","\u001b[1m\u001b[31mpython/needle/ops/ops_mathematic.py\u001b[0m:255: TypeError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1msubmit_conv_backward\u001b[0m - TypeError: object of type 'NoneType' has no len()\n","\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[31m in 2.07s\u001b[0m\u001b[31m ========================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"conv_backward\""]},{"cell_type":"markdown","metadata":{"id":"ZcLdoJEUucAU"},"source":["------------------------------------------------"]},{"cell_type":"markdown","metadata":{"tags":[],"id":"8oBR-9WEucAU"},"source":["### Implementing \"ResNet9\""]},{"cell_type":"markdown","metadata":{"id":"aCoj-l85ucAU"},"source":["You will now use your convolutional layer to implement a model similar to _ResNet9_, which is known to be a reasonable model for getting good accuracy on CIFAR-10 quickly (see [here](https://github.com/davidcpage/cifar10-fast)). Our main change is that we used striding instead of pooling and divided all of the channels by 4 for the sake of performance (as our framework is not as well-optimized as industry-grade frameworks).\n","\n","In the figure below, before the first linear layer, you should \"flatten\" the tensor. You can use the module `Flatten` in `nn_basic.py`, or you can simply use `.reshape` in the `forward()` method of your ResNet9.\n","\n","Make sure that you pass the device to all modules in your model; otherwise, you will get errors about mismatched devices when trying to run with CUDA.\n","\n","<center><img src=\"https://github.com/dlsyscourse/hw4/blob/main/ResNet9.png?raw=true\" alt=\"ResNet9\" style=\"width: 400px;\" /></center>\n","\n","We have tried to make it easier to pass the tests here than for previous assignments where you have implemented models. In particular, we are just going to make sure it has the right number of parameters and similar accuracy and loss after 1 or 2 batches of CIFAR-10."]},{"cell_type":"code","execution_count":44,"metadata":{"tags":[],"id":"GumASW8eucAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763015405010,"user_tz":300,"elapsed":4120,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"0e59ba2e-3c7c-43d6-b0d1-65c34d634a83"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\n","\n","tests/hw4/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_resnet9[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m__________ test_resnet9[needle.backend_ndarray.ndarray_backend_cuda] ___________\u001b[0m\n","\n","device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_resnet9\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n","        \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mnum_params\u001b[39;49;00m(model):\u001b[90m\u001b[39;49;00m\n","            \u001b[94mreturn\u001b[39;49;00m np.sum([np.prod(x.shape) \u001b[94mfor\u001b[39;49;00m x \u001b[95min\u001b[39;49;00m model.parameters()])\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m ResNet9\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        model = ResNet9(device=device)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        \u001b[94massert\u001b[39;49;00m num_params(model) == \u001b[94m431946\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n","        _A = np.random.randn(\u001b[94m2\u001b[39;49;00m, \u001b[94m3\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m, \u001b[94m32\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        A = ndl.Tensor(_A, device=device)\u001b[90m\u001b[39;49;00m\n","        y = model(A)\u001b[90m\u001b[39;49;00m\n","    \u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(np.array([[-\u001b[94m1.8912625\u001b[39;49;00m ,  \u001b[94m0.64833605\u001b[39;49;00m,  \u001b[94m1.9400386\u001b[39;49;00m ,  \u001b[94m1.1435282\u001b[39;49;00m ,  \u001b[94m1.89777\u001b[39;49;00m   ,\u001b[90m\u001b[39;49;00m\n","             \u001b[94m2.9039745\u001b[39;49;00m , -\u001b[94m0.10433993\u001b[39;49;00m,  \u001b[94m0.35458302\u001b[39;49;00m, -\u001b[94m0.5684191\u001b[39;49;00m ,  \u001b[94m2.6178317\u001b[39;49;00m ],\u001b[90m\u001b[39;49;00m\n","           [-\u001b[94m0.2905612\u001b[39;49;00m , -\u001b[94m0.4147861\u001b[39;49;00m ,  \u001b[94m0.90268034\u001b[39;49;00m,  \u001b[94m0.46530387\u001b[39;49;00m,  \u001b[94m1.3335679\u001b[39;49;00m ,\u001b[90m\u001b[39;49;00m\n","             \u001b[94m1.8534894\u001b[39;49;00m , -\u001b[94m0.1867125\u001b[39;49;00m , -\u001b[94m2.4298222\u001b[39;49;00m , -\u001b[94m0.5344223\u001b[39;49;00m ,  \u001b[94m4.362149\u001b[39;49;00m  ]]) - y.numpy()) < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float64(7.498542457184476) < 0.01\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float64(7.498542457184476) = <function norm at 0x14b9ef203670>((array([[-1.8912625 ,  0.64833605,  1.9400386 ,  1.1435282 ,  1.89777   ,\\n         2.9039745 , -0.10433993,  0.35458302, -0.5684191 ,  2.6178317 ],\\n       [-0.2905612 , -0.4147861 ,  0.90268034,  0.46530387,  1.3335679 ,\\n         1.8534894 , -0.1867125 , -2.4298222 , -0.5344223 ,  4.362149  ]]) - array([[0.39641726, 0.21159936, 0.07144465, 0.48555785, 0.08255634,\\n        0.4641346 , 0.03601498, 0.9516535 , 0.18090308, 0.27106223],\\n       [0.36929584, 0.5011014 , 0.09979127, 0.35509056, 0.29871982,\\n        0.71534824, 0.6811138 , 0.9703304 , 0.28985542, 0.6344659 ]],\\n      dtype=float32)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0x14b9ef203670> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[-1.8912625 ,  0.64833605,  1.9400386 ,  1.1435282 ,  1.89777   ,\\n         2.9039745 , -0.10433993,  0.35458302, -0.5684191 ,  2.6178317 ],\\n       [-0.2905612 , -0.4147861 ,  0.90268034,  0.46530387,  1.3335679 ,\\n         1.8534894 , -0.1867125 , -2.4298222 , -0.5344223 ,  4.362149  ]]) = <built-in function array>([[-1.8912625, 0.64833605, 1.9400386, 1.1435282, 1.89777, 2.9039745, ...], [-0.2905612, -0.4147861, 0.90268034, 0.46530387, 1.3335679, 1.8534894, ...]])\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in function array> = np.array\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([[0.39641726, 0.21159936, 0.07144465, 0.48555785, 0.08255634,\\n        0.4641346 , 0.03601498, 0.9516535 , 0.18090308, 0.27106223],\\n       [0.36929584, 0.5011014 , 0.09979127, 0.35509056, 0.29871982,\\n        0.71534824, 0.6811138 , 0.9703304 , 0.28985542, 0.6344659 ]],\\n      dtype=float32) = numpy()\u001b[0m\n","\u001b[1m\u001b[31mE        +      where numpy = needle.Tensor([[0.39641726 0.21159936 0.07144465 0.48555785 0.08255634 0.4641346\\n  0.03601498 0.9516535  0.18090308 0.27106223]\\n [0.36929584 0.5011014  0.09979127 0.35509056 0.29871982 0.71534824\\n  0.6811138  0.9703304  0.28985542 0.6344659 ]]).numpy\u001b[0m\n","\n","A          = needle.Tensor([[[[-4.73285615e-01 -7.25738525e-01  2.37614036e-01 ... -7.80708790e-01\n","     5.42091370e-01 -4.57965702e...5e+00]\n","   [-6.61311224e-02 -5.09674728e-01  1.42986202e+00 ... -3.52020800e-01\n","    -6.78952754e-01 -1.48065418e-01]]]])\n","ResNet9    = <class 'apps.models.ResNet9'>\n","_A         = array([[[[-4.73285609e-01, -7.25738502e-01,  2.37614036e-01, ...,\n","          -7.80708792e-01,  5.42091361e-01, -4.57965...6.61311208e-02, -5.09674701e-01,  1.42986197e+00, ...,\n","          -3.52020797e-01, -6.78952743e-01, -1.48065416e-01]]]])\n","device     = cuda()\n","model      = <apps.models.ResNet9 object at 0x14b90c6a9880>\n","num_params = <function test_resnet9.<locals>.num_params at 0x14b90bb12160>\n","y          = needle.Tensor([[0.39641726 0.21159936 0.07144465 0.48555785 0.08255634 0.4641346\n","  0.03601498 0.9516535  0.18090308 0....3]\n"," [0.36929584 0.5011014  0.09979127 0.35509056 0.29871982 0.71534824\n","  0.6811138  0.9703304  0.28985542 0.6344659 ]])\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:188: AssertionError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_resnet9[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - AssertionError: assert np.float64(7.498542457184476) < 0.01\n","\u001b[31m================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 2.44s\u001b[0m\u001b[31m =================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"resnet9\""]},{"cell_type":"markdown","metadata":{"id":"DnSYv59aucAU"},"source":["Now we can train a ResNet on CIFAR10: (remember to copy the solutions in `python/needle/optim.py` from previous homeworks)"]},{"cell_type":"code","execution_count":45,"metadata":{"id":"fU3WCBuiucAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763015416044,"user_tz":300,"elapsed":11027,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"2c10f108-20af-4140-eb4b-10801c198a8f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0 -- /usr/bin/python3\n","cachedir: .pytest_cache\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","collected 1803 items / 1801 deselected / 2 selected                            \u001b[0m\n","\n","tests/hw4/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cpu] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n","tests/hw4/test_conv.py::test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] \u001b[31mFAILED\u001b[0m\u001b[31m [100%]\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m_______ test_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda] ________\u001b[0m\n","\n","device = cuda()\n","\n","    \u001b[0m\u001b[37m@pytest\u001b[39;49;00m.mark.parametrize(\u001b[33m\"\u001b[39;49;00m\u001b[33mdevice\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, _DEVICES)\u001b[90m\u001b[39;49;00m\n","    \u001b[94mdef\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[92mtest_train_cifar10\u001b[39;49;00m(device):\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dataset = ndl.data.CIFAR10Dataset(\u001b[33m\"\u001b[39;49;00m\u001b[33m./data/cifar-10-batches-py\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, train=\u001b[94mTrue\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        dataloader = ndl.data.DataLoader(\\\n","                 dataset=dataset,\u001b[90m\u001b[39;49;00m\n","                 batch_size=\u001b[94m128\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n","                 shuffle=\u001b[94mFalse\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                 \u001b[90m# collate_fn=ndl.data.collate_ndarray,\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                 \u001b[90m# drop_last=False,\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                 \u001b[90m# device=device,\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                 \u001b[90m# dtype=\"float32\"\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","                 )\u001b[90m\u001b[39;49;00m\n","        \u001b[94mfrom\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[04m\u001b[96mapps\u001b[39;49;00m\u001b[04m\u001b[96m.\u001b[39;49;00m\u001b[04m\u001b[96mmodels\u001b[39;49;00m\u001b[90m \u001b[39;49;00m\u001b[94mimport\u001b[39;49;00m ResNet9\u001b[90m\u001b[39;49;00m\n","        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        model = ResNet9(device=device, dtype=\u001b[33m\"\u001b[39;49;00m\u001b[33mfloat32\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n","        out = one_iter_of_cifar10_training(dataloader, model, opt=ndl.optim.Adam(model.parameters(), lr=\u001b[94m0.001\u001b[39;49;00m, weight_decay=\u001b[94m0.001\u001b[39;49;00m), device=device)\u001b[90m\u001b[39;49;00m\n",">       \u001b[94massert\u001b[39;49;00m np.linalg.norm(np.array(\u001b[96mlist\u001b[39;49;00m(out), dtype=\u001b[96mobject\u001b[39;49;00m) - np.array([\u001b[94m0.09375\u001b[39;49;00m, \u001b[94m3.5892258\u001b[39;49;00m])) < \u001b[94m1e-2\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n","\u001b[1m\u001b[31mE       AssertionError: assert np.float64(3.0607607826990813) < 0.01\u001b[0m\n","\u001b[1m\u001b[31mE        +  where np.float64(3.0607607826990813) = <function norm at 0xf2e7d7c0570>((array([np.float64(0.09375), np.float32(0.528465)], dtype=object) - array([0.09375  , 3.5892258])))\u001b[0m\n","\u001b[1m\u001b[31mE        +    where <function norm at 0xf2e7d7c0570> = <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'>.norm\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <module 'numpy.linalg' from '/usr/local/lib/python3.12/dist-packages/numpy/linalg/__init__.py'> = np.linalg\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([np.float64(0.09375), np.float32(0.528465)], dtype=object) = <built-in function array>([np.float64(0.09375), np.float32(0.528465)], dtype=object)\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in function array> = np.array\u001b[0m\n","\u001b[1m\u001b[31mE        +      and   [np.float64(0.09375), np.float32(0.528465)] = list((np.float64(0.09375), np.float32(0.528465)))\u001b[0m\n","\u001b[1m\u001b[31mE        +    and   array([0.09375  , 3.5892258]) = <built-in function array>([0.09375, 3.5892258])\u001b[0m\n","\u001b[1m\u001b[31mE        +      where <built-in function array> = np.array\u001b[0m\n","\n","ResNet9    = <class 'apps.models.ResNet9'>\n","dataloader = <needle.data.data_basic.DataLoader object at 0xf2d9a3949e0>\n","dataset    = <needle.data.datasets.cifar10_dataset.CIFAR10Dataset object at 0xf2d9aea44d0>\n","device     = cuda()\n","model      = <apps.models.ResNet9 object at 0xf2d9a811880>\n","out        = (np.float64(0.09375), np.float32(0.528465))\n","\n","\u001b[1m\u001b[31mtests/hw4/test_conv.py\u001b[0m:468: AssertionError\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1mtest_train_cifar10[needle.backend_ndarray.ndarray_backend_cuda]\u001b[0m - AssertionError: assert np.float64(3.0607607826990813) < 0.01\n","\u001b[31m================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[32m1 passed\u001b[0m, \u001b[33m1801 deselected\u001b[0m\u001b[31m in 9.12s\u001b[0m\u001b[31m =================\u001b[0m\n"]}],"source":["!python3 -m pytest -l -v -k \"train_cifar10\""]},{"cell_type":"markdown","metadata":{"id":"Pg4WxkdVucAU"},"source":["### Submit ResNet9 to mugrade [10 points]"]},{"cell_type":"code","execution_count":46,"metadata":{"id":"f0ZHV6IJucAU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763015429969,"user_tz":300,"elapsed":13925,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"cc4c139c-1e7e-4d87-c34c-f23958de56a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["submit\n","\u001b[1m============================= test session starts ==============================\u001b[0m\n","platform linux -- Python 3.12.12, pytest-8.4.2, pluggy-1.6.0\n","rootdir: /content/drive/MyDrive/dl-test/hw4\n","plugins: langsmith-0.4.42, typeguard-4.4.4, anyio-4.11.0\n","\u001b[1mcollecting ... \u001b[0mUsing needle backend\n","collected 10 items / 9 deselected / 1 selected                                 \u001b[0m\n","\n","tests/hw4/test_conv.py \n","Submitting resnet9...\n","Grader test 1 passed\n","Grader test 2 failed\n","\u001b[31mF\u001b[0m\n","\n","=================================== FAILURES ===================================\n","\u001b[31m\u001b[1m________________________________ submit_resnet9 ________________________________\u001b[0m\n","\n","\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n","\u001b[31mFAILED\u001b[0m tests/hw4/test_conv.py::\u001b[1msubmit_resnet9\u001b[0m - Failed\n","\u001b[31m================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m9 deselected\u001b[0m, \u001b[33m1 warning\u001b[0m\u001b[31m in 12.18s\u001b[0m\u001b[31m ==================\u001b[0m\n"]}],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"resnet9\""]},{"cell_type":"markdown","metadata":{"id":"c4c7Jq00ucAU"},"source":["-----------------"]},{"cell_type":"markdown","metadata":{"id":"8YRE5vXsucAU"},"source":["Now, you can train your model on CIFAR-10 using the following code. Note that this is likely going to be quite slow, and also  not all that accurate due to the lack of data augmentation. You should expect it to take around 500s per epoch."]},{"cell_type":"code","execution_count":47,"metadata":{"id":"i6RL4WF-ucAU","colab":{"base_uri":"https://localhost:8080/","height":373},"executionInfo":{"status":"error","timestamp":1763015431096,"user_tz":300,"elapsed":1126,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}},"outputId":"fde29d80-6e2a-474a-9e9a-6d21b4d9ec26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using needle backend\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'list' object has no attribute 'shape'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1616907413.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m          shuffle=True,)\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet9\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n\u001b[0m\u001b[1;32m     16\u001b[0m       lr=0.001, weight_decay=0.001)\n\u001b[1;32m     17\u001b[0m \u001b[0mevaluate_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dl-test/hw4/apps/simple_ml.py\u001b[0m in \u001b[0;36mtrain_cifar10\u001b[0;34m(model, dataloader, n_epochs, optimizer, lr, weight_decay, loss_fn)\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mavg_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_general_cifar10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch: {epoch}, Acc: {avg_acc}, Loss: {avg_loss}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;31m### END YOUR SOLUTION\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/drive/MyDrive/dl-test/hw4/apps/simple_ml.py\u001b[0m in \u001b[0;36mepoch_general_cifar10\u001b[0;34m(dataloader, model, epoch, loss_fn, opt)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m             \u001b[0mbs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m             \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"]}],"source":["import sys\n","sys.path.append('./python')\n","sys.path.append('./apps')\n","import needle as ndl\n","from models import ResNet9\n","from simple_ml import train_cifar10, evaluate_cifar10\n","\n","device = ndl.cpu()\n","dataset = ndl.data.CIFAR10Dataset(\"data/cifar-10-batches-py\", train=True)\n","dataloader = ndl.data.DataLoader(\\\n","         dataset=dataset,\n","         batch_size=128,\n","         shuffle=True,)\n","model = ResNet9(device=device, dtype=\"float32\")\n","train_cifar10(model, dataloader, n_epochs=10, optimizer=ndl.optim.Adam,\n","      lr=0.001, weight_decay=0.001)\n","evaluate_cifar10(model, dataloader)"]},{"cell_type":"markdown","metadata":{"id":"LBD9tKanucAU"},"source":["## Part 4: Recurrent neural network [10 points]\n","\n","**Note:** In the following sections, you may find yourself wanting to index into tensors, i.e., to use getitem or setitem. However, we have not implemented these for tensors in our library; instead, you should use `stack` and `split` operations.\n","\n","In `python/needle/nn/nn_sequence.py`, implement `RNNCell`.\n","\n","$h^\\prime = \\text{tanh}(xW_{ih} + b_{ih} + hW_{hh} + b_{hh})$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n","\n","All weights and biases should be uniformly initialized on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|hidden_size|}}$.\n","\n","In `python/needle/nn/nn_sequence.py`, implement `RNN`.\n","\n","For each element in the input sequence, each layer computes the following function:\n","\n","$h_t = \\text{tanh}(x_tW_{ih} + b_{ih} + h_{(t-1)}W_{hh} + b_{hh})$\n","\n","where $h_t$ is the hidden state at time $t$, $x_t$ is the input at time $t$, and $h_{(t-1)}$ is the hidden state of the previous layer at time $t-1$ or the initial hidden state at time $0$. If nonlinearity is 'relu', then ReLU is used in place of tanh.\n","\n","In a multi-layer RNN, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dTBHG7zMucAU","executionInfo":{"status":"aborted","timestamp":1763015431117,"user_tz":300,"elapsed":1,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m pytest -l -v -k \"test_rnn\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"43QqRTqjucAU","executionInfo":{"status":"aborted","timestamp":1763015431117,"user_tz":300,"elapsed":187049,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"rnn\""]},{"cell_type":"markdown","metadata":{"id":"5i2x_FF-ucAU"},"source":["## Part 5: Long short-term memory network [10 points]\n","In `python/needle/nn/nn_sequence.py`, implement `Sigmoid`.\n","\n","$$\\sigma(x) = \\frac{1}{1 + \\text{exp}(-x)}$$\n","\n","In `python/needle/nn/nn_sequence.py`, implement `LSTMCell`.\n","\n","\\begin{align*}\n","i &= \\sigma(xW_{ii} + b_{ii} + hW_{hi} + b_{hi}) \\\\\n","f &= \\sigma(xW_{if} + b_{if} + hW_{hf} + b_{hf}) \\\\\n","g &= \\text{tanh}(xW_{ig} + b_{ig} + hW_{hg} + b_{hg}) \\\\\n","o &= \\sigma(xW_{io} + b_{io} + hW_{ho} + b_{ho}) \\\\\n","c^\\prime &= f * c + i * g \\\\\n","h^\\prime &= o * \\text{tanh}(c^\\prime)\n","\\end{align*}\n","\n","where $\\sigma$ is the sigmoid function, and $i$, $f$, $g$, $o$ are the input, forget, cell, and output gates, respectively.\n","\n","All weights and biases should be uniformly initialized on the interval $\\displaystyle\\pm\\frac{1}{\\sqrt{\\verb|hidden_size|}}$.\n","\n","Now implement `LSTM` in `python/needle/nn/nn_sequence.py`, which applies a multi-layer LSTM RNN to an input sequence. For each element in the input sequence, each layer computes the following function:\n","\n","\\begin{align*}\n","i_t &= \\sigma(x_tW_{ii} + b_{ii} + h_{(t-1)}W_{hi} + b_{hi}) \\\\\n","f_t &= \\sigma(x_tW_{if} + b_{if} + h_{(t-1)}W_{hf} + b_{hf}) \\\\\n","g_t &= \\text{tanh}(x_tW_{ig} + b_{ig} + h_{(t-1)}W_{hg} + b_{hg}) \\\\\n","o_t &= \\sigma(x_tW_{io} + b_{io} + h_{(t-1)}W_{ho} + b_{ho}) \\\\\n","c_t &= f * c_{(t-1)} + i * g \\\\\n","h_t &= o * \\text{tanh}(c_t)\n","\\end{align*}\n","\n","where $h_t$ is the hidden state at time $t$, $c_t$ is the cell state at time $t$, $x_t$ is the input at time $t$, $h_{(t-1)}$ is the hidden state of the layer at time $t-1$ or the initial hidden state at time $0$, and $i_t$, $f_t$, $g_t$, $o_t$ are the input, forget, cell, and output gates at time $t$ respectively.\n","\n","In a multi-layer LSTM, the input $x_t^{(l)}$ of the $l$-th layer ($l \\ge 2$) is the hidden state $h_t^{(l-1)}$ of the previous layer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BS-1C0n9ucAU","executionInfo":{"status":"aborted","timestamp":1763015431117,"user_tz":300,"elapsed":187048,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m pytest -l -v -k \"test_lstm\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIyL8x17ucAU","executionInfo":{"status":"aborted","timestamp":1763015431118,"user_tz":300,"elapsed":187049,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"lstm\""]},{"cell_type":"markdown","metadata":{"id":"iw7SdLA1ucAU"},"source":["## Part 6: Penn Treebank dataset [10 points]\n","\n","In word-level language modeling tasks, the model predicts the probability of the next word in the sequence, based on the words already observed in the sequence. You will write support for the Penn Treebank dataset, which consists of stories from the Wall Street Journal, to train and evaluate a language model on word-level prediction.\n","\n","In `python/needle/data/datasets/ptb_dataset.py`, start by implementing the `Dictionary` class, which creates a dictionary from a list of words, mapping each word to a unique integer.\n","\n","Next, we will use this `Dictionary` class to create a corpus from the train and test txt files in the Penn Treebank dataset that you downloaded at the beginning of the notebook. Implement the `tokenize` function in the `Corpus` class to do this.\n","\n","In order to prepare the data for training and evaluation, you will next implement the `batchify` function. Starting from sequential data, batchify arranges the dataset into columns. For instance, with the alphabet as the sequence and batch size 4, we'd get\n","\n","```\n"," a g m s \n"," b h n t \n"," c i o u \n"," d j p v \n"," e k q w \n"," f l r x \n","```\n","\n","These columns are treated as independent by the model, which means that the dependence of e. g. 'g' on 'f' cannot be learned, but allows more efficient batch processing.\n","\n","Next, implement the `get_batch` function. `get_batch` subdivides the source data into chunks of length `bptt`. If source is equal to the example output of the batchify function, with a bptt-limit of 2, we'd get the following two `Tensor`s for i = 0:\n","```\n"," a g m s   b h n t \n"," b h n t   c i o u \n","```\n","Note that despite the name of the function, the subdivison of data is not done along the batch dimension (i.e. dimension 1), since that was handled by the batchify function. The chunks are along dimension 0, corresponding to the seq_len dimension in the LSTM or RNN. Also, as per the function docs, the second returned `Tensor` (the targets) should be reshaped to be 1-dimensional."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"miEQ0RuIucAV","executionInfo":{"status":"aborted","timestamp":1763015431118,"user_tz":300,"elapsed":187048,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m pytest -l -v -k \"ptb\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WCsZ_gAGucAV","executionInfo":{"status":"aborted","timestamp":1763015431118,"user_tz":300,"elapsed":187048,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"ptb\""]},{"cell_type":"markdown","metadata":{"id":"32IlnME-ucAV"},"source":["## Part 7: Training a word-level language model [10 points]\n","\n","Finally, you will use the `RNN` and `LSTM` components you have written to construct a language model that we will train on the Penn Treebank dataset.\n","\n","First, in `python/needle/nn/nn_sequence.py` implement `Embedding`. Consider we have a dictionary with 1000 words. Then for a word which indexes into this dictionary, we can represent this word as a one-hot vector of size 1000, and then use a linear layer to project this to a vector of some embedding size.\n","\n","In `apps/models.py`, you can now implement `LanguageModel`. Your language model should consist of\n","\n","- An embedding layer (which maps word IDs to embeddings)\n","- A sequence model (either RNN or LSTM)\n","- A linear layer (which outputs probabilities of the next word)\n","\n","In `apps/simple_ml.py` implement `epoch_general_ptb`, `train_ptb`, and `evaluate_ptb`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Mi_SJEuQucAV","executionInfo":{"status":"aborted","timestamp":1763015431118,"user_tz":300,"elapsed":187048,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m pytest -l -v -k \"language_model_implementation\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Fb1SUr9kucAV","executionInfo":{"status":"aborted","timestamp":1763015431119,"user_tz":300,"elapsed":187048,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m pytest -l -v -k \"language_model_training\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1zaBsFPuucAV","executionInfo":{"status":"aborted","timestamp":1763015431119,"user_tz":300,"elapsed":187048,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["!python3 -m mugrade submit \"$MY_API_KEY\" \"$HW4_NAME\" -k \"language_model\""]},{"cell_type":"markdown","metadata":{"id":"XNI8a80VucAV"},"source":["Now, you can train your language model on the Penn Treebank dataset:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_sQ9iJwgucAV","executionInfo":{"status":"aborted","timestamp":1763015431119,"user_tz":300,"elapsed":187047,"user":{"displayName":"Changrui Mu","userId":"03226529930170103580"}}},"outputs":[],"source":["import needle as ndl\n","sys.path.append('./apps')\n","from models import LanguageModel\n","from simple_ml import train_ptb, evaluate_ptb\n","\n","device = ndl.cpu()\n","corpus = ndl.data.Corpus(\"data/ptb\")\n","train_data = ndl.data.batchify(corpus.train, batch_size=16, device=ndl.cpu(), dtype=\"float32\")\n","model = LanguageModel(30, len(corpus.dictionary), hidden_size=10, num_layers=2, seq_model='rnn', device=ndl.cpu())\n","train_ptb(model, train_data, seq_len=1, n_epochs=1, device=device)\n","evaluate_ptb(model, train_data, seq_len=40, device=device)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.17"},"colab":{"provenance":[],"toc_visible":true,"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}